---
title: "Flexdashboard"
output:
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
runtime: shiny
resource_files:
- SP25_602_omnibus_V1_May8.csv
---

```{r setup, include=FALSE}
library(flexdashboard)
library(ggplot2)
library(tidyverse) # Includes dplyr (for mutate, summarise) and readr (for read_csv)
library(knitr)     # For kable (to create tables)
library(stringr)   # For str_detect
library(plotly) # ADD THIS LINE
library(readr)
library(tidyr)
library(nnet)
```

```{r data processing, include=FALSE}
# Load data (using simple file path)
data <- read_csv("SP25_602_omnibus_V1_May8.csv", col_types = cols(.default = "c"))

# --- STATIC DATA SUMMARIZATION ---
# This runs once on the full dataset, as there is no interactive filter.
df_summary <- data %>%
  # 1. Create G13_RiskGroup by extracting the scenario name
  mutate(
    G13_RiskGroup = case_when(
      str_detect(Group13_AITrust_DO, "HighRisk_1") ~ "HighRisk_1",
      str_detect(Group13_AITrust_DO, "HighRisk_2") ~ "HighRisk_2",
      str_detect(Group13_AITrust_DO, "HighRisk_3") ~ "HighRisk_3",
      str_detect(Group13_AITrust_DO, "LowRisk_SongColl") ~ "LowRisk_SongColl",
      str_detect(Group13_AITrust_DO, "LowRisk_Collab") ~ "LowRisk_Collab",
      str_detect(Group13_AITrust_DO, "LowRisk_BlogWrit") ~ "LowRisk_BlogWrit",
      TRUE ~ "Unknown"
    ),
    # 2. Convert G13_DV to a numeric Likert scale (1 to 5)
    G13_DV_Likert = case_when(
      G13_DV == "Human alone" ~ 1,
      G13_DV == "Human assisted by AI" ~ 2,
      G13_DV == "Collaboration between AI and Human (50-50)" ~ 3,
      G13_DV == "AI assisted with Human" ~ 4,
      G13_DV == "AI alone" ~ 5,
      TRUE ~ NA_real_
    )
  ) %>%
  # 3. Filter out any unknown risk groups
  filter(G13_RiskGroup != "Unknown") %>%
  # 4. Summarize the mean, SD, N, and Confidence Intervals (CIs)
  group_by(G13_RiskGroup) %>%
  summarise(
    mean_trust = mean(G13_DV_Likert, na.rm = TRUE),
    sd = sd(G13_DV_Likert, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n),
    ci_low = mean_trust - 1.96 * se,
    ci_high = mean_trust + 1.96 * se,
    .groups = 'drop'
  )
```

```{r second data}
# Data Processing for Chart B (Regression/HOP) AND Model 4 Setup
# Assumes AI_Chatbots_Students_Attitude_Dataset_EN.csv
data_b <- read_csv("AI_Chatbots_Students_Attitude_Dataset_EN.csv", col_types = cols(.default = "c"))

# Define Likert scales
Q1_likert_scale <- c("Bachelor" = 0, "Master" = 1)
Q3_likert_scale <- c("Male" = 0, "Female" = 1)
Q4_likert_scale <- c("Never"=1, "Rarely"=2, "Sometimes"=3, "Often"=4, "Very often"=5)
likert_scale <- c("Strongly disagree"=1, "Disagree"=2, "Neutral"=3, "Agree"=4, "Strongly agree"=5)
likert_cols <- grep("^Q[5-9]\\.", names(data_b), value = TRUE)

# Prepare Data
data_b$Q1 <- factor(data_b$Q1, levels = c('Bachelor', 'Master'), ordered = TRUE)
data_b$Q2 <- factor(data_b$Q2, levels = c("Marketing", "Finance", "Accounting", "International Economic Relations", "Economics and Business", "Economics"), ordered = FALSE)
data_b$Q2 <- relevel(data_b$Q2, ref = "Finance")
data_b$Q3 <- factor(data_b$Q3, levels = c("Male", "Female"), ordered = TRUE)
data_b$Q4 <- factor(data_b$Q4, levels = c("Never", "Rarely", "Sometimes", "Often", "Very often"), ordered = TRUE)

for (col in likert_cols) {
  data_b[[col]] <- ordered(data_b[[col]], levels = names(likert_scale))
}

# Create Indices and Final Modeling Dataframe
model_data <- data_b %>%
  mutate(across(all_of(likert_cols), ~ likert_scale[.])) %>%
  mutate(
    ethics_index = rowMeans(select(., matches("^Q7\\.|^Q9\\.")), na.rm = TRUE),
    benefit_index = rowMeans(select(., starts_with("Q8.")), na.rm = TRUE),
    Q1_likert = Q1_likert_scale[as.character(Q1)],
    Q3_likert = Q3_likert_scale[as.character(Q3)]
  ) %>%
  select(ethics_index, benefit_index, Q2, Q1_likert, Q3_likert) %>%
  na.omit()


# Fit Model 4 (used for Chart C)
model4 <- lm(ethics_index ~ Q2 + Q1_likert + Q3_likert + benefit_index, data = model_data)


# Prepare data for Chart B (HOP Graph)
model_b <- lm(ethics_index ~ benefit_index, data = model_data) # Reduced model for the scatter plot line
new_data_b <- data.frame(
  benefit_index = seq(min(model_data$benefit_index, na.rm = TRUE), max(model_data$benefit_index, na.rm = TRUE), length.out = 100)
)
pred_b <- predict(model_b, newdata = new_data_b, interval = "prediction", level = 0.95)
hop_data_b <- new_data_b %>%
  mutate(predicted = pred_b[, "fit"], lower = pred_b[, "lwr"], upper = pred_b[, "upr"])
```

Column {data-width=500}
-----------------------------------------------------------------------

### Mean Trust in AI-Human Collaboration


```{r interactive_chart}
# Create the ggplot object using the static df_summary
p <- ggplot(df_summary, # NOTE: Changed from reactive_df_summary()
            aes(x = G13_RiskGroup, y = mean_trust, 
                text = paste("Scenario:", G13_RiskGroup, 
                             "<br>Mean Trust:", round(mean_trust, 3),
                             "<br>95% CI: [", round(ci_low, 3), ", ", round(ci_high, 3), "]",
                             "<br>N:", n))) +
  
  geom_point(size = 3, aes(color = G13_RiskGroup)) +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high, color = G13_RiskGroup), 
                width = 0.2) +
  geom_hline(yintercept = 3, linetype = "dashed", color = "gray") +
  
  labs(
    title = "Mean Trust in AI-Human Decision-Making by Scenario Condition",
    x = "Scenario Condition",
    y = "Mean Trust Score (1 = Human alone to 5 = AI alone)"
  ) +
  scale_color_brewer(palette = "Set3") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10), 
        legend.position = "none") 

# Convert to interactive plotly object
plotly::ggplotly(p, tooltip = "text") %>%
  config(displayModeBar = FALSE)
```

Column {data-width=500}
-----------------------------------------------------------------------

### Ethical Concerns vs. Perceived Benefits

```{r}
# Create the ggplot object for the HOP graph (Chart B)
p_hop_b <- ggplot(model_data, aes(x = benefit_index, y = ethics_index)) +
  geom_ribbon(data = hop_data_b, 
              aes(ymin = lower, ymax = upper, x = benefit_index, y = predicted), 
              fill = "lightblue", alpha = 0.4, inherit.aes = FALSE) +
  geom_point(alpha = 0.6, color = "darkgray", size = 2) +
  geom_line(data = hop_data_b, 
            aes(y = predicted, x = benefit_index), 
            color = "steelblue", size = 1, inherit.aes = FALSE) +
  labs(
    title = "Ethical Concerns by Perceived AI Benefits",
    x = "Perceived Benefit Index",
    y = "Ethical Concern Index"
  ) +
  theme_minimal() +
  scale_y_continuous(limits = c(2, 4.5), breaks = seq(2, 4.5, by = 0.5)) +
  scale_x_continuous(limits = c(2, 4.5), breaks = seq(2, 4.5, by = 0.5))

plotly::ggplotly(p_hop_b) %>%
  config(displayModeBar = FALSE)
```



### Model 4 Diagnostic Plot (Residuals vs. Fitted)

```{r}
# which=1 specifies the "Residuals vs. Fitted" plot
plot(model4, which = 1)
```


Column {data-width=250}
-----------------------------------------------------------------------

### Home returning



-   [**Home**](https://klee201.github.io/MyWebsite/)

