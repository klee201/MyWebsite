[
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "I hold a Bachelor of Science degree in Computer Science from Arizona State University, and I am currently preparing to pursue a Masterâ€™s degree in Data Analysis and Computational Social Science at the University of Massachusetts Amherst. My primary academic interest lies in data analysis, which is driven by my enthusiasm for collecting information from various platforms and employing analytical methods to uncover intriguing patterns and phenomena, particularly when working with text as data. I enjoy the process of transforming raw text into actionable insights and exploring the subtle nuances hidden within large textual datasets.\n\n\n\n\nView Full Resume\nThanks for checking out my web site!"
  },
  {
    "objectID": "604_Final_check-in_2.html",
    "href": "604_Final_check-in_2.html",
    "title": "DACSS604_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "604_Final_check-in_2.html#research-question-and-hypothesis",
    "href": "604_Final_check-in_2.html#research-question-and-hypothesis",
    "title": "DACSS604_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "604_Final_check-in_2.html#data-collection",
    "href": "604_Final_check-in_2.html#data-collection",
    "title": "DACSS604_Final_Project",
    "section": "Data Collection",
    "text": "Data Collection\nTo explore the concept of â€œsimping,â€ I collected YouTube comments from six relevant videos for textual analysis. I utilized an R scraping script to extract approximately 8,000 comments in total. Following a cleaning and filtering process, a dataset of around 7,000 practical comments was retained for analysis.\nI specifically focused on the case study known as the â€œèƒ–è²“è·³æ±Ÿäº‹ä»¶â€ (Suicide of Fat Cat). This event, which occurred in Mainland China, provides a particularly rich and relevant dataset because it was a well-documented news story officially reported by the Chinese court. This official documentation makes it a real and verifiable event, distinguishing it from mere rumors or social media anecdotes. Furthermore, the use of Chinese-language videos as the reference source is critical, as the Chinese-speaking audience possesses extensive background knowledge and cultural context directly related to the local details of this incident.\n\nSIMP001 - é™ªæ‰“éŠæˆ²è³ºç™¾è¬é¤Šå¥³å‹æ…˜é­åˆ†æ‰‹ï¼ã€Œèƒ–è²“äº‹ä»¶ã€å¼•çˆ†ä¸­åœ‹æ€§åˆ¥æˆ°çˆ­ï¼Ÿã€Œæ’ˆå¥³ã€æ»¿è¡—è·‘çš„èƒŒå¾ŒåŸå› ï¼Ÿã€TODAY çœ‹ä¸–ç•Œã€‘(https://www.youtube.com/watch?v=o5TfkwlthWU&t=13s) 1952 comments from 11/04/2025\nSIMP002 - å½“èƒ–çŒ«é‡åˆ°æå¥³ï¼Œä¸€ä¸ªå¹´è½»äººå¦‚ä½•èµ°ä¸Šä¸å½’è·¯ï¼Ÿï½œå¥³æƒï½œæå¥³ï½œèƒ–çŒ«ï½œç‹è€…è£è€€ï½œç”·å¥³å¹³æƒï½œæ—¥æœ¬ï½œæ¢…å¤§é«˜é€Ÿï½œèˆ†è®ºæ§åˆ¶ï½œç‹å±€æ‹æ¡ˆ20240507 (https://www.youtube.com/watch?v=39Gq_eOPuDY&t=1s) 3731 comments from 11/04/2025\nSIMP003 - è€æ¢ï¼šç»™â€œèƒ–çŒ«â€å¤šæ¡é€‰æ‹© é‡åº†â€œèƒ–çŒ«äº‹ä»¶â€ä¸æ˜¯æ€§åˆ«å¤§æˆ˜ å¦‚ä½•é¿å…æˆä¸ºâ€œèƒ–çŒ«â€(https://www.youtube.com/watch?v=mjcgg0wFpfE) 997 comments from 11/04/2025\nSIMP004 - å°ä¼™ç‚ºæ„›è·³æ±Ÿï¼Œæ‹œé‡‘çš„å¥³å‹ï¼Œå¸è¡€çš„è¦ªå§ï¼Œç„¡è‰¯çš„å•†å®¶ï¼Œç˜‹ç‹‚çš„ç¶²æ°‘ï¼Œèª°æ‰æ˜¯åŠ å®³è€…ï¼Ÿç‚ºä½•è­¦å¯Ÿèªå®šå¥³å‹ç„¡ç½ªï¼Œåè€Œæ˜¯è¦ªå§é•äº†æ³•ï¼Ÿä¸€å£æ°£çœ‹å®Œèƒ–è²“äº‹ä»¶å§‹æœ«ï¼| Wayneèª¿æŸ¥(https://www.youtube.com/watch?v=igs7GoIU4MU) 615 comments from 11/04/2025\nSIMP005 - ç¥ç´šé™ªç©ã€Œèƒ–è²“ã€é­è©ä¹¾227è¬äº¡ å¥³å‹é“æ­‰ï½œ20240506 ETåˆé–“æ–°è (https://www.youtube.com/watch?v=tAE83zZEcOY) 402 comments from 11/04/2025\nSIMP006 - è¢«æ’ˆå¥³é¨™å…‰50è¬ï¼ŒéŠæˆ²å®…ç”·è·³æ±Ÿè‡ªæ®ºï¼Œè½Ÿå‹•å…¨ç¶²ï¼æ’ˆå¥³è­šç«¹æ¦¨ä¹¾èƒ–è²“äº‹ä»¶çœŸç›¸ï¼ã€æ–°é—»æœ€å˜²ç‚¹ å§œå…‰å®‡ã€2024.0508(https://www.youtube.com/watch?v=YYngd2Yt3zk) 271 comments from 11/04/2025\n\n\nlibrary(plyr)\n\nWarning: package 'plyr' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.4.3\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(purrr)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:plyr':\n\n    compact\n\nlibrary(chromote)\n\nWarning: package 'chromote' was built under R version 4.4.3\n\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tibble)\nlibrary(nnet)\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.4.3\n\n\ncorrplot 0.95 loaded\n\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.4.3\n\n\nLoading required package: NLP\n\n\nWarning: package 'NLP' was built under R version 4.4.2\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.4.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.4.3\n\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.4.3\n\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.4.3\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(\"quanteda.textplots\")\n\nWarning: package 'quanteda.textplots' was built under R version 4.4.3\n\nlibrary(httr)\n\nWarning: package 'httr' was built under R version 4.4.3\n\n\n\nAttaching package: 'httr'\n\n\nThe following object is masked from 'package:NLP':\n\n    content\n\nlibrary(RColorBrewer)\nlibrary(RedditExtractoR)\n\nWarning: package 'RedditExtractoR' was built under R version 4.4.3\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.4.3\n\nlibrary(tidyr)\n\n\ndata1 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\ndata2 &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments.csv\")\ndata3 &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments.csv\")\ndata4 &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments.csv\")\ndata5 &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments.csv\")\ndata6 &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments.csv\")"
  },
  {
    "objectID": "604_Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "href": "604_Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "title": "DACSS604_Final_Project",
    "section": "Data Cleaning (info for the poster",
    "text": "Data Cleaning (info for the poster\nALSO GUIDE IT TO THE PART WHICH I WANT PLUS PRESENT THE CLEAN FORMAT IN THE POSTER\nThe raw dataset, collected as several CSV files, initially contained detailed comment metadata. The original structure included columns such as: videoId, commentId, parentId, author, text, likeCount, publishedAt, updatedAt, viewerRating, canRate, and reply.\nFor data cleaning, all CSV files in the Final_project_data folder were systematically processed using the R environment. The initial step was to streamline the dataset by retaining only the essential variables for textual and engagement analysis: text, likeCount, and reply.\nI utilized the R packages dplyr and stringr to focus on standardizing the text column. This involved a series of cleaning operations: normalization of whitespace (removing line breaks, tabs, and extra spaces, and trimming leading/trailing whitespace) and character filtering. Crucially, I removed non-essential symbols and unusual characters while meticulously preserving all Chinese characters to ensure the comments remained culturally authentic and meaningful for subsequent analysis.\nFinally, each cleaned and standardized dataset was saved as a new CSV file, appended with the suffix _cleaned. UTF-8 encoding was explicitly used to guarantee the accurate representation of the Chinese characters. This systematic workflow ensures the comment data are tidy, standardized, and immediately ready for downstream procedures, such as tokenization and sentiment or frequency analysis.\n\nsource(\"data_cleaning_CN.R\")\n\ndata cleaning complete!.\n\nSIMP001 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\n\n#Present the eample of the result\nhead(SIMP001)\n\n      videoId                  commentId parentId        author\n1 o5TfkwlthWU UgyekRC230MDXREkdeN4AaABAg     &lt;NA&gt;  @DanjonMeshi\n2 o5TfkwlthWU UgxangSP0zjJm6_gHfV4AaABAg     &lt;NA&gt;  @paullee4451\n3 o5TfkwlthWU UgwZdLtl6Eb2wgDWaDV4AaABAg     &lt;NA&gt;      @urikora\n4 o5TfkwlthWU UgwSmeqUyHYUXGD6l3l4AaABAg     &lt;NA&gt; @fayechen1928\n5 o5TfkwlthWU UgwpotCAmJmn2wWU7u54AaABAg     &lt;NA&gt; @running_goat\n6 o5TfkwlthWU UgxQSyQbnLT9r7I1faB4AaABAg     &lt;NA&gt;  @Jack2006103\n                                                                                                                       text\n1                                                             å•†å®¶é›†é«”çµ¦ç©ºè¢‹çœŸçš„ç¬‘æ­»ï¼Œä¸æ­¢ç”Ÿæ´»åœ¨ä¸­åœ‹ï¼Œé€£æ­»åœ¨ä¸­åœ‹éƒ½è¦å·è‘—æ¨‚ğŸ˜†\n2                                                                                                                      é ­é¦™\n3                       æ›´æ…˜çš„æ˜¯ï¼Œäººéƒ½èµ°äº†ä¸€å€‹æœˆ çµæœå°±åœ¨é€™æ™‚æ©Ÿé»è¢«æŠ“ä¾†æ“‹æ”¿åºœåšçš„é†œäº‹ï¼ˆè·¯å´©è¯ç‚ºè»Šè¡é€²å»å‘æ´ç„¶å¾Œå¿«é€Ÿç‡ƒèµ·ä¾†ï¼‰\n4                                                                                      æ¯æ¬¡çœ‹åˆ°ä¸­åœ‹é€™ç¨®æ‚²åŠ‡éƒ½è¦ºå¾—ä¸å¯æ€è­°ğŸ˜¨ğŸ˜­\n5 æˆ‘çœŸçš„å¿…é ˆå¾—èªªå°å²¸ç”·å¥³æˆ°çˆ­çœŸçš„è¶Šä¾†è¶Šåš´é‡== å•éå¥½å¹¾å€‹å°å²¸çš„å¥³ç”Ÿéƒ½èªç‚ºç”·ç”Ÿå°±æ˜¯æ‡‰è©²è¦çµ¦å½©ç¦® åƒé£¯å°±æ˜¯è¦å¹«å¥³ç”Ÿä»˜éŒ¢ç­‰ç­‰ å¾ˆå¯æ€•\n6                                                                                                é€™äº›åº—å®¶åƒäººè¡€é¥…é ­ï¼Œè¶…å™çˆ›\n  likeCount          publishedAt            updatedAt viewerRating canRate\n1        58 2024-05-09T16:01:52Z 2024-05-09T16:01:52Z         none    TRUE\n2         0 2024-05-09T16:02:23Z 2024-05-09T16:02:23Z         none    TRUE\n3       345 2024-05-09T16:05:05Z 2024-05-09T16:05:05Z         none    TRUE\n4         1 2024-05-09T16:05:57Z 2024-05-09T16:05:57Z         none    TRUE\n5       139 2024-05-09T16:06:47Z 2024-05-09T16:06:47Z         none    TRUE\n6         4 2024-05-09T16:07:29Z 2024-05-09T16:07:29Z         none    TRUE\n  reply\n1 FALSE\n2 FALSE\n3 FALSE\n4 FALSE\n5 FALSE\n6 FALSE\n\ndata1_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_cleaned.csv\")\ndata2_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments_cleaned.csv\")\ndata3_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments_cleaned.csv\")\ndata4_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments_cleaned.csv\")\ndata5_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments_cleaned.csv\")\ndata6_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments_cleaned.csv\")"
  },
  {
    "objectID": "604_Final_check-in_2.html#preprocess-the-data",
    "href": "604_Final_check-in_2.html#preprocess-the-data",
    "title": "DACSS604_Final_Project",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nFor visualizing the dominant linguistic patterns within the comment data, I employed two complementary approaches. First, a Word Cloud visualization (generated using the Word_cloud_visualization.R script) provided an intuitive, qualitative representation of high-frequency words, instantly highlighting the most common terms associated with discussions of â€œSIMPâ€ behavior.\nSecond, I conducted a quantitative rank-frequency analysis by applying Zipfâ€™s Law to the word corpus. After arranging all unique words by descending frequency and assigning a rank, I plotted the resulting distribution using the ggplot2 package. The resulting visualization confirmed that the comment discourse adheres to a Zipfian distribution, where a few words account for a disproportionate share of the total vocabulary.\nThe key terms driving the discourse were clearly identifiable:\nThese visualizations collectively offer both quantitative validation (Zipfâ€™s Law distribution) and qualitative insight (Word Cloud/Top Terms) into how the audience discusses and perceives the central event and the related concept of â€œSIMPâ€ behavior in this context. The high frequency of questioning and uncertainty (ç‚ºä»€éº¼, ä¸çŸ¥é“, æ˜¯ä¸æ˜¯) coupled with terms of exploitation (pua) and suffering (å—å®³è€…) reveals a key focus on moral judgment and accountability in the discussion.\n\nsource(\"TOKENIZATION.R\")\n\nTokenization complete!\n\nSIMP001_comments_tokens &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_tokens.csv\")\n#Present the eample of the result\nhead(SIMP001_comments_tokens)\n\n  likeCount reply     word\n1       345 FALSE   è¡é€²å»\n2         1 FALSE ä¸å¯æ€è­°\n3       139 FALSE   è¶Šä¾†è¶Š\n4       139 FALSE   å¥½å¹¾å€‹\n5       141 FALSE   éº¥ç•¶å‹\n6         3 FALSE   æœ‰äººèªª\n\n\n\nsource(\"Word_frequency.R\")\n\nWord Frequency Calculation Complete!\n\nSIMP001_wordfreq &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_wordfreq.csv\")\n\n#Present the eample of the result\nhead(SIMP001_wordfreq)\n\n    word  n rank\n1 å—å®³è€… 75    1\n2 ç‚ºä»€éº¼ 67    2\n3 ä¸çŸ¥é“ 53    3\n4    pua 42    4\n5 é€™ä»¶äº‹ 34    5\n6 ä¸€å€‹äºº 32    6\n\n\n\nsource(\"Same_Word.R\")\n\nCommon words saved to: Final_project_data/common_words_across_files.csv \n\ncommon_words &lt;- read.csv(\"Final_project_data/common_words_across_files.csv\")\n\n#Present the eample of the result\nhead(common_words)\n\n    word total_count\n1 ä¸çŸ¥é“         172\n2 ä¸ºä»€ä¹ˆ         154\n3 å—å®³è€…         118\n4    pua         115\n5 æ˜¯ä¸æ˜¯         101\n6 ç‚ºä»€éº¼         100\n\n\n\nsource(\"Change_to_Traditional_Chinese.R\")\n\nWarning: package 'textstem' was built under R version 4.4.3\n\n\nLoading required package: koRpus.lang.en\n\n\nWarning: package 'koRpus.lang.en' was built under R version 4.4.3\n\n\nLoading required package: koRpus\n\n\nWarning: package 'koRpus' was built under R version 4.4.3\n\n\nLoading required package: sylly\n\n\nWarning: package 'sylly' was built under R version 4.4.3\n\n\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\n\nAttaching package: 'koRpus'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    tokens, types\n\n\nThe following object is masked from 'package:tm':\n\n    readTagged\n\n\nThe following object is masked from 'package:readr':\n\n    tokenize\n\n\nWarning: package 'tmcn' was built under R version 4.4.3\n\n\n# tmcn Version: 0.2-13\n\n\n[1] \"girl\"    \"woman\"   \"simping\" \"lover\"  \nTranslation complete! Output saved to 'your_output_file.csv'\n\nTraditional_Chinese_data_cleaned &lt;- read.csv(\"Final_project_data/traditional_common_words_combined.csv\")\n\n#Present the data after cleaning\nhead(Traditional_Chinese_data_cleaned)\n\n  traditional_text total_count\n1           ç‚ºä»€éº¼         254\n2           ä¸çŸ¥é“         172\n3           å—å®³è€…         118\n4              pua         116\n5           æ˜¯ä¸æ˜¯         101\n6           å¥³æœ‹å‹          95"
  },
  {
    "objectID": "604_Final_check-in_2.html#visualization",
    "href": "604_Final_check-in_2.html#visualization",
    "title": "DACSS604_Final_Project",
    "section": "Visualization",
    "text": "Visualization\nFor visualizing patterns in the comments, I used two approaches. First, the Word_cloud_visualization.R script generated word clouds to highlight high-frequency words, providing a clear and intuitive view of the most common terms associated with discussions of â€œSIMPâ€ behavior. Second, I applied Zipfâ€™s Law to examine the relationship between word rank and frequency. After arranging words by descending frequency and assigning ranks, I plotted all words using ggplot2, labeling only the top five most frequent words to emphasize the key terms in the discourse. The resulting visualizations offer both quantitative and qualitative insight into how people discuss and perceive â€œSIMPâ€ behavior in YouTube comments.\n\nsource(\"Word_cloud_visualization.R\")\n\n\n\n\n\n\n\n\nWord Cloud generated for: traditional_common_words_combined.csv\n\n\n\n# Sort by frequency and assign ranks\nzipf_data_ranked &lt;- Traditional_Chinese_data_cleaned %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  mutate(rank = row_number())\n\n# Print the top 5 ranked words to confirm the data structure\nprint(head(zipf_data_ranked, 5))\n\n  traditional_text total_count rank\n1           ç‚ºä»€éº¼         254    1\n2           ä¸çŸ¥é“         172    2\n3           å—å®³è€…         118    3\n4              pua         116    4\n5           æ˜¯ä¸æ˜¯         101    5\n\n# --- Linear Scale (As Requested) ---\n\nggplot(zipf_data_ranked, aes(x = rank, y = total_count)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"darkorange\", size = 1.5) +\n  geom_text(\n    # Label the top 8 words\n    aes(label = ifelse(rank &lt;= 6, traditional_text, \"\")),\n    vjust = -0.8,\n    size = 3.5,\n    check_overlap = TRUE # Prevents overlapping labels\n  ) +\n  labs(\n    title = \"Zipfâ€™s Law: Word Rank vs Frequency\",\n    x = \"Rank of Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nTranslation\n\n\n\nRank\nWord\nTranslate\n\n\n1\nç‚ºä»€éº¼\nâ€œWhy / Why is it thatâ€¦â€\n\n\n2\nä¸çŸ¥é“\nâ€œDonâ€™t knowâ€\n\n\n3\nå—å®³è€…\nâ€œVictimâ€\n\n\n4\npua\nâ€œPUAâ€\n\n\n5\næ˜¯ä¸æ˜¯\nâ€œIs it / Is it not?â€\n\n\n6\nå¥³æœ‹å‹\nâ€œGirlfriendâ€"
  },
  {
    "objectID": "604_Final_check-in_2.html#word-embedding",
    "href": "604_Final_check-in_2.html#word-embedding",
    "title": "DACSS604_Final_Project",
    "section": "Word Embedding",
    "text": "Word Embedding\nFor semantic analysis, I applied Word2Vec using a pseudo-document approach to capture relationships between words in the comments. Each word was repeated according to its frequency (total_count) to create co-occurrence information, which is essential for small datasets where natural co-occurrences are limited. The repeated words were then combined into a single space-separated pseudo-document and used to train a skip-gram Word2Vec model with a vector dimension of 50, window size of 5, and 50 iterations, setting min_count = 1 to include all words.\nThe resulting word vectors allow calculation of cosine similarity to examine semantic relationships between words, as well as clustering and downstream supervised learning tasks. For example, the vector for a keyword such as â€œç‚ºä»€éº¼â€ can be compared with all other word vectors to identify the top semantically similar words, revealing patterns in how concepts related to â€œSIMPâ€ behavior are discussed in YouTube comments. This approach provides a robust representation of word meaning in the context of the dataset while accommodating the limited co-occurrence information inherent in smaller comment datasets.\n\n# Word2Vec can be the best option for the word embeding.\n\nsource(\"Word Embeddings.R\")\n\nWarning: package 'word2vec' was built under R version 4.4.3\n\n\nWarning: package 'text2vec' was built under R version 4.4.3"
  },
  {
    "objectID": "604_Final_check-in_2.html#sentiment-analysis",
    "href": "604_Final_check-in_2.html#sentiment-analysis",
    "title": "DACSS604_Final_Project",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nFor sentiment analysis, I applied a custom Chinese sentiment dictionary tailored to the context of â€œSIMPâ€ behavior. The dictionary categorizes words into three groups: positive (supportive or relationship-related words such as â€œå¥³æœ‹å‹â€ and â€œé—œå¿ƒâ€), negative (critical or unfairness-related words such as â€œä¸å€¼å¾—â€ and â€œä¸å…¬å¹³â€), and behavior (attention-seeking or â€œsimpâ€ behavior words such as â€œpuaâ€ and â€œè¿½æ±‚â€). Using R, I computed sentiment scores for each word in the dataset by summing occurrences in these categories. A raw polarity score was calculated as the sum of positive and behavior counts minus negative counts, then normalized by the total occurrences of all dictionary words to produce a relative polarity measure.\nThe analysis revealed that the current positive and negative categories do not fully capture the sentiment expressed in the comments. Some words were misclassified or contextually ambiguous, highlighting that the dictionary needs further adjustment and refinement to improve accuracy. Polarity distributions were visualized using a histogram, providing an overview of how positive, negative, and behavior-related language appears in discussions of â€œSIMPâ€ behavior. This approach provides a preliminary sentiment assessment while acknowledging the limitations of the existing dictionary.\n\nsource(\"Sentiment Analysis.R\")\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” forcats   1.0.0     âœ” lubridate 1.9.4\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– NLP::annotate()         masks ggplot2::annotate()\nâœ– httr::content()         masks NLP::content()\nâœ– dplyr::filter()         masks stats::filter()\nâœ– jsonlite::flatten()     masks purrr::flatten()\nâœ– rvest::guess_encoding() masks readr::guess_encoding()\nâœ– dplyr::lag()            masks stats::lag()\nâœ– koRpus::tokenize()      masks readr::tokenize()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'quanteda.textmodels' was built under R version 4.4.3\n\n\nWarning: package 'stopwords' was built under R version 4.4.3\n\n\n\nAttaching package: 'stopwords'\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nWarning: package 'caret' was built under R version 4.4.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:httr':\n\n    progress\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n[1] \"DFM after Custom Simp Dictionary Lookup (Counts per category):\"\n\n### SUPERVISED LEARNING (Naive Bayes Classification)\n[1] \"Confusion Matrix:\"\n\n### LEXICON-BASED NTUSD SENTIMENT RESULTS\n[1] \"Mean Sentiment Score by Simping Label (NTUSD):\"\n# A tibble: 2 Ã— 2\n  contains_simp_factor mean_sentiment\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 FALSE                       -0.0990\n2 TRUE                        -0.477 \n\n\n\n\n\n\n\n\n\nIn this graph:\nX-Axis: Simping Label:\n\nFALSE: Comments that do not contain any of the words from your custom â€œsimpâ€ dictionary (e.g., â€œèˆ”ç‹—,â€ â€œå·¥å…·äºº,â€ â€œä¸€å»‚æƒ…é¡˜,â€ etc.).\nTRUE: Comments that do contain at least one word from your custom â€œsimpâ€ dictionary.\n\nY-Axis: Comment Sentiment Score:\n\nPositive Scores (above 0): Indicate a more positive overall sentiment.\nZero (0): Indicates a neutral or balanced sentiment.\nNegative Scores (below 0): Indicate a more negative overall sentiment."
  },
  {
    "objectID": "604_Final_check-in_2.html#sentiment-analysis-summary",
    "href": "604_Final_check-in_2.html#sentiment-analysis-summary",
    "title": "DACSS604_Final_Project",
    "section": "Sentiment Analysis Summary",
    "text": "Sentiment Analysis Summary\nThe lexicon-based sentiment analysis, utilizing the NTUSD dictionary, reveals a pronounced negative emotional shift in texts discussing the â€œsimpâ€ phenomenon. Specifically, content that contains terms from the custom dictionaryâ€”which targets themes like â€œsimp behaviorâ€ (e.g., èˆ”ç‹—, simp), â€œvictim positionâ€ (e.g., å—å®³è€…, pua), and â€œrelationship imbalanceâ€â€”shows a highly negative mean sentiment score of -0.477. This score is significantly more negative than the average score of -0.0990 found in texts that do not contain these specific terms. This sharp difference (a nearly five-fold increase in negative sentiment magnitude) indicates that conversations about excessive one-sided effort, perceived exploitation, and unequal relationshipsâ€”the core of the â€œsimpâ€ conceptâ€”are strongly associated with negative emotional discourse within the corpus."
  },
  {
    "objectID": "604_Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "href": "604_Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "title": "DACSS604_Final_Project",
    "section": "Supervised Learning Analysis (Naive Bayes Classification)",
    "text": "Supervised Learning Analysis (Naive Bayes Classification)\nThe Naive Bayes model was employed to classify comments based on whether they contained the â€œsimpâ€ factor, using a cleaned feature set that excluded all words from the custom â€œsimpâ€ dictionary to prevent data leakage. The model achieved an overall Accuracy of 81.69%, which is slightly higher than the No Information Rate (NIR) of 80.36%, indicating its performance is marginally better than random guessing based on class prevalence.\nHowever, a closer look at the results reveals a significant class imbalance issue and skewed performance:\n\nHigh Sensitivity (Recall): The model is excellent at correctly identifying comments that do NOT contain the simp factor (the majority class, FALSE), with a high Sensitivity of 95.72%.\nLow Specificity: Conversely, the model is very poor at correctly identifying comments that DO contain the simp factor (the minority class, TRUE), with a low Specificity of 24.29%.\nKappa Value: The Kappa statistic of 0.2565 suggests only a fair level of agreement beyond chance.\n\nIn summary, the high overall accuracy is largely driven by correctly classifying the prevalent negative class (FALSE). The model struggles to reliably identify actual â€œsimpâ€ comments (TRUE), suggesting that the remaining general vocabulary in the comments lacks sufficient predictive power to consistently distinguish between the two categories without the core dictionary terms.\n\nsource(\"Supervised_Learning.R\")\n\n\n--- Solving Data Leakage: Remove the word which exist in Simp dictionary ---\n[1] \"Original (matrix_main): 2703\"\n[1] \"Remove the word in Simp dictionary (X_cleaned): 2674\"\n\n--- 6. Naive Bayes Training ---\n\n--- Naive Bayes Prediction ---\n[1] \"Confusion Matrix:\"\nConfusion Matrix and Statistics\n\n                 y_test\npredicted_cleaned FALSE TRUE\n            FALSE   693  134\n            TRUE     31   43\n                                        \n               Accuracy : 0.8169        \n                 95% CI : (0.79, 0.8416)\n    No Information Rate : 0.8036        \n    P-Value [Acc &gt; NIR] : 0.1676        \n                                        \n                  Kappa : 0.2565        \n                                        \n Mcnemar's Test P-Value : 2.011e-15     \n                                        \n            Sensitivity : 0.9572        \n            Specificity : 0.2429        \n         Pos Pred Value : 0.8380        \n         Neg Pred Value : 0.5811        \n              Precision : 0.8380        \n                 Recall : 0.9572        \n                     F1 : 0.8936        \n             Prevalence : 0.8036        \n         Detection Rate : 0.7691        \n   Detection Prevalence : 0.9179        \n      Balanced Accuracy : 0.6001        \n                                        \n       'Positive' Class : FALSE"
  },
  {
    "objectID": "604_Final_check-in_2.html#topic-modeling-lda",
    "href": "604_Final_check-in_2.html#topic-modeling-lda",
    "title": "DACSS604_Final_Project",
    "section": "Topic Modeling (LDA)",
    "text": "Topic Modeling (LDA)\nThe script follows a standard text mining workflow using the tidyverse and text2vec packages:\n\nData Preparation: It reads the individual tokenized comment files, reconstructs the full comments by assigning and aggregating tokens by a unique document ID (doc_id), and then combines the tokens back into complete text strings.\nFeature Engineering: It creates an iterator from the aggregated text and builds a vocabulary. Crucially, it prunes the vocabulary by removing words that occur less than three times (term_count_min = 3), which helps reduce noise and improves the quality of the derived topics.\nDTM Creation: The processed tokens are converted into a Document-Term Matrix (DTM), which is the input required for LDA.\nModel Training: The script initializes and trains an LDA model with a predefined number of topics (K=8) and 500 iterations.\nOutput: Finally, the code extracts and saves two key results: the Topic-Word distribution (the top 10 most characteristic words for each of the 8 topics) and the Document-Topic distribution (the probability that each comment belongs to each topic), storing both as CSV files for subsequent qualitative analysis.\n\n\nsource(\"Topic_Model.R\")\n\n[1] \"Starting LDA Topic Modeling with K = 8\"\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======================================================================| 100%\n[1] \"LDA Training Complete.\"\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nâ„¹ Using compatibility `.name_repair`.\n\n\n[1] \"Top 10 Words for Each Topic:\"\n# A tibble: 10 Ã— 9\n   Topic_Word_Rank Topic_0 Topic_1 Topic_2 Topic_3  Topic_4  Topic_5 Topic_6\n   &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1 Word_1          pua     ä¸çŸ¥é“  å—å®³è€…  æ˜¯ä¸æ˜¯   ç‚ºä»€éº¼   ä¸çŸ¥é“  è€Œä¸æ˜¯ \n 2 Word_2          ä¸ºä»€ä¹ˆ  ä¸‡ä½™å…ƒ  ä¸ºä»€ä¹ˆ  å¥³æœ‹å‹   åœ¨ä¸€èµ·   è¶Šæ¥è¶Š  é€™ä»¶äº‹ \n 3 Word_3          ä¸­å›½äºº  pua     pua     ä¸ºä»€ä¹ˆ   ä¹Ÿå¯ä»¥   å¤§éƒ¨åˆ†  ä¸çŸ¥é“ \n 4 Word_4          ç”·å­©å­  å¯èƒ½æ˜¯  çœŸçš„æ˜¯  ä¸çŸ¥é“   ä¸å¯èƒ½   éƒ½ä¸æ˜¯  ä¸€å®šæ˜¯ \n 5 Word_5          å¥³å­©å­  ä¸€å€‹äºº  éº¦å½“åŠ³  ä¸éœ€è¦   éƒ½æ²’æœ‰   çœŸçš„æ˜¯  å¥³æœ‹å‹ \n 6 Word_6          ä¹Ÿä¸æ˜¯  é€™ä»¶äº‹  å®¶åº­çš„  ä¹Ÿæ²¡æœ‰   é€™å°±æ˜¯   å—å®³è€…  èƒ½ä¸èƒ½ \n 7 Word_7          æ˜¯ä¸æ˜¯  ä¸­å›½äºº  å…¨ä¸–ç•Œ  å¤§éƒ¨åˆ†   ä¹Ÿæ²’æœ‰   å¥³æœ‹å‹  äººæ°‘å¹£ \n 8 Word_8          æ²¡ä»€ä¹ˆ  å°¤å…¶æ˜¯  å…¶å®æ˜¯  ç”·æœ‹å‹   ç”·å°Šå¥³å‘ æˆ‘çŸ¥é“  åƒåœ¾æ¡¶ \n 9 Word_9          å¯èƒ½æ˜¯  çœŸçš„æ˜¯  å¥½åƒæ˜¯  ç”·å¥³å¹³ç­‰ æ²’ä»€éº¼   æ˜¯ä¸æ˜¯  æœ‰äº›äºº \n10 Word_10         ä¸å€¼å¾—  ç‚ºä»€éº¼  è‚¯å®šæ˜¯  æ‰€è°“çš„   ä¸å­˜åœ¨   ä¹Ÿä¸æ˜¯  ä¸ºä»€ä¹ˆ \n# â„¹ 1 more variable: Topic_7 &lt;chr&gt;\n[1] \"Saved topic words to lda_topic_words.csv\"\n[1] \"Document-Topic Distribution (Head):\"\n# A tibble: 6 Ã— 9\n  doc_id    V1    V2    V3    V4    V5    V6    V7    V8\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1     0  0      0    0      0     0       0     0\n2      2     0  0      0    0      0     0       0     0\n3      3     0  0.05   0    0.15   0.6   0.2     0     0\n4      4     0  0.1    0.2  0      0.4   0.3     0     0\n5      5     0  0      0.8  0      0.2   0       0     0\n6      6     0  0      0    0      0     0       1     0\n[1] \"Saved document-topic distribution to lda_doc_topic_distr.csv\"\n\n\nTranslation for the every words in the topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic_Word_Rank\nTopic_0\nTopic_1\nTopic_2\nTopic_3\nTopic_4\nTopic_5\nTopic_6\nTopic_7\n\n\nWord_1\nPUA\nDonâ€™t know\nVictim\nIs it?\nWhy\nDonâ€™t know\nAnd not\nBuffet (Resource)\n\n\nWord_2\nWhy\nOver 10k yuan\nWhy\nGirlfriend\nBe together\nMore and more\nThis matter\nHighway\n\n\nWord_3\nChinese people\nPUA\nPUA\nWhy\nAlso can\nMajority\nDonâ€™t know\nToo good\n\n\nWord_4\nBoy / Male\nPossibly\nTruly is\nDonâ€™t know\nImpossible\nAre not all\nMust be\nWhy\n\n\nWord_5\nGirl / Female\nA person\nMcDonaldâ€™s\nDonâ€™t need\nDonâ€™t have at all\nTruly is\nGirlfriend\nMainly is\n\n\nWord_6\nAlso is not\nThis matter\nFamilyâ€™s\nAlso donâ€™t have\nThis is\nVictim\nCan or cannot\nSome people\n\n\nWord_7\nIs it?\nChinese people\nWhole world\nMajority\nAlso donâ€™t have\nGirlfriend\nRMB (Money)\nInequality\n\n\nWord_8\nNothing much\nEspecially\nActually is\nBoyfriend\nMale Superiority\nI know\nTrash Can (Worthless)\nShould be\n\n\nWord_9\nPossibly\nTruly is\nSeems like\nGender Equality\nNothing much\nIs it?\nSome people\nA person\n\n\nWord_10\nNot worth it\nWhy\nDefinitely is\nSo-called\nDoes not exist\nAlso is not\nWhy\nGender Equality\n\n\n\nThe interpretation for each topic\n\n\n\n\n\n\n\n\nTopic\nCore Keywords & Interpretation\nSuggested Topic Label\n\n\nTopic 0\nThis topic strongly links the PUA phenomenon with discussions about specific gender roles and identities within the Chinese context. The presence of â€œNot worth itâ€ suggests this cluster is focused on evaluating the value of actions/relationships under the PUA framework.\nPUA & Gender Dynamics in China\n\n\nTopic 1\nThis topic mixes uncertainty and specific financial figures (Over 10k yuan), directly linked to PUA. It suggests discussions about high-stakes financial loss or investment by an individual in a relationship where the outcome or reality is unclear.\nFinancial Dimension of PUA & Uncertainty\n\n\nTopic 2\nThe simultaneous presence of â€œVictim,â€ â€œPUA,â€ and â€œTruly isâ€ indicates a core discussion cluster dedicated to validating the existence and reality of being exploited. â€œMcDonaldâ€™sâ€ implies cheap/casual provision, while â€œFamilyâ€™sâ€ suggests the conversation may touch on the origins or impact of these dynamics within a family unit.\nValidating Victimhood & Low-Cost Exploitation\n\n\nTopic 3\nThis topic is saturated with questioning terms (â€œIs it?â€, â€œWhy?â€, â€œDonâ€™t knowâ€), applied directly to boyfriend/girlfriend roles and the concept of gender equality. It represents a pervasive atmosphere of skepticism and critical discussion about expected behavior in modern relationships.\nSkepticism & Questioning of Relationship Roles\n\n\nTopic 4\nA highly polarized topic that denies (ä¸å¯èƒ½, ä¸å­˜åœ¨) the relevance or existence of Male Superiority (ç”·å°Šå¥³å‘). It focuses on the possibility of being together (åœ¨ä¸€èµ·), suggesting a desire for modern, equal partnerships and a strong rejection of patriarchal norms.\nDenial of Traditional Patriarchy in Relationships\n\n\nTopic 5\nTerms like â€œMore and moreâ€ and â€œMajorityâ€ point to a discussion of social trends and scale. When combined with â€œVictimâ€ and â€œGirlfriend,â€ it indicates a conversation about whether victimhood is becoming increasingly common or if the perception of victimhood is changing within the female partner role.\nDiscussing Shifting Social Norms & Victim Pool\n\n\nTopic 6\nThis is the most explicitly transactional topic. It discusses financial payment (RMB) and the concept of a person being reduced to a â€œTrash Canâ€ (worthless/emotional dumping ground). The use of â€œAnd notâ€ suggests a debate over what a relationship should be versus what it currently is (i.e., not a transaction, but one of money/exploitation).\nMonetary Value vs.Â Emotional Worth (The Price of Simping)\n\n\nTopic 7\nThis topic links resource provision (implied by â€œBuffetâ€ and â€œHighway,â€ often used as metaphors for free/easy access) with discussions of Inequality and Gender Equality. It debates whether resources should be provided freely, who is responsible for providing them, and the resulting fairness in the relationship structure.\nResource Provision & Equality Debate\n\n\n\n\n\n\nDTM\nTopic_Word_Rank\n\n\nV1\nTopic_0\n\n\nV2\nTopic_1\n\n\nV3\nTopic_2\n\n\nV4\nTopic_3\n\n\nV5\nTopic_4\n\n\nV6\nTopic_5\n\n\nV7\nTopic_6\n\n\nV8\nTopic_7"
  },
  {
    "objectID": "604_Final_check-in_2.html#causal-inference",
    "href": "604_Final_check-in_2.html#causal-inference",
    "title": "DACSS604_Final_Project",
    "section": "Causal Inference",
    "text": "Causal Inference\nIn the casual inference part, I present the Ordinary Least Squares (OLS) regression model to analyze how the probability of eight LDA topics influences the number of likes received by a comment (likeCount). Topic V8 (PUA/Victim) was set as the reference group (Reference Topic) in the model. Overall, the modelâ€™s explanatory power is extremely low (\\(\\text{Adjusted R-squared} = 0.00025\\)), suggesting that the variation in likeCount is primarily influenced by factors outside the model, rather than the topics themselves. However, the coefficient tests for individual topics revealed that Topic V2 demonstrated a statistically significant positive influence. After controlling for the effects of other topics, an increase of 1 unit in the probability of Topic V2 (Financial/Money II), relative to the reference group V8, is expected to increase the number of likes by approximately 9.66 (\\(p = 0.038^{*}\\)). This suggests that specific discussion content related to money or finance is more likely to garner attention and agreement within the community. Apart from the intercept, the remaining topics (V1, V3, V4, V5, V6, and V7) did not show a statistically significant relationship with the number of likes.\n\nsource(\"Causal_Inference.R\")\n\nRows: 3006 Columns: 9\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (9): doc_id, V1, V2, V3, V4, V5, V6, V7, V8\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n[1] \"--- OLS Regression Results (Outcome: likeCount) ---\"\n[1] \"Reference Topic: V8 (PUA/Victim)\"\n\nCall:\nlm(formula = likeCount ~ V1 + V2 + V3 + V4 + V5 + V6 + V7, data = merged_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -18.59   -8.93   -6.76   -5.12 1763.41 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.934      2.301   3.882 0.000106 ***\nV1            -2.424      4.144  -0.585 0.558600    \nV2             9.655      4.656   2.074 0.038172 *  \nV3            -2.701      4.312  -0.626 0.531107    \nV4            -2.380      4.266  -0.558 0.576978    \nV5            -2.815      4.399  -0.640 0.522298    \nV6             1.729      4.351   0.398 0.691027    \nV7            -0.953      4.363  -0.218 0.827099    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.08 on 2998 degrees of freedom\nMultiple R-squared:  0.002579,  Adjusted R-squared:  0.00025 \nF-statistic: 1.107 on 7 and 2998 DF,  p-value: 0.3553"
  },
  {
    "objectID": "604_Final_check-in_2.html#conclusion",
    "href": "604_Final_check-in_2.html#conclusion",
    "title": "DACSS604_Final_Project",
    "section": "Conclusion",
    "text": "Conclusion\n(PUT THESE TWO POINT INTO THE FUTURE WORK ALSO NEED TO EXPLAIN ABOUT DIFFICULTY TO COLLECT THE ACADEMIC DATA THERE ** SIMP DOES NOT HAVE A FORMAL DEFINITION**\nâ€œSynthesizing the projectâ€™s findings, the primary discovery is that community engagement (\\(\\text{likeCount}\\)) on YouTube comments is not driven by broad emotional tone or general topics, but rather by a specific, critical narrative focused on â€˜financial exploitation and victimhoodâ€™ (\\(\\text{Topic}\\) \\(\\text{V2}\\)). This specific form of critical discussion related to â€˜SIMPâ€™ behavior is extremely negative in sentiment (\\(\\text{mean}\\) \\(\\text{sentiment} = -0.477\\)) and is so unique in its linguistic pattern that its occurrence can be accurately predicted by the supervised learning model. Consequently, the communityâ€™s response to â€˜SIMPâ€™ behavior is highly concentrated and emotionally charged, with its online visibility predominantly stemming from comments that link the behavior directly to concrete financial inequality and victim scenarios.â€"
  },
  {
    "objectID": "604_Final_check-in_2.html#future",
    "href": "604_Final_check-in_2.html#future",
    "title": "DACSS604_Final_Project",
    "section": "Future",
    "text": "Future\nFuture work should prioritize addressing the observed limitations in both the supervised classification model and the initial data preprocessing pipeline to enhance the robustness and explanatory power of the analysis. Firstly, while the initial Naive Bayes classifier provided baseline insights, its predictive performance should be critically re-evaluated. Improving the accuracy of the automated simp classification label requires exploring more sophisticated machine learning techniques, such as Support Vector Machines (SVMs), Gradient Boosting, or even Transformer-based deep learning models. Concurrently, enhancing the feature set by refining or expanding the custom dictionariesâ€”perhaps incorporating sentiment scores or incorporating word embeddingsâ€”could significantly boost the modelâ€™s ability to discriminate between classes, moving beyond simple bag-of-words approaches. Secondly, a crucial area for improvement lies in the token filtering and data processing stage. Despite standard removal procedures, the presence of numerous contextually irrelevant tokens, such as specific objects (â€œé«˜é€Ÿå…¬è·¯â€) and brands (â€œéº¥ç•¶å‹â€), confirms the necessity of a more rigorous, domain-specific cleanup. Future efforts must focus on constructing an expanded, domain-aware stop word list or implementing Named Entity Recognition (NER) to systematically identify and remove non-topical, low-information tokens, ensuring the remaining features are highly predictive and representative of the core concepts being discussed."
  },
  {
    "objectID": "604_Final_check-in_2.html#reference",
    "href": "604_Final_check-in_2.html#reference",
    "title": "DACSS604_Final_Project",
    "section": "Reference",
    "text": "Reference\nHO, Daniel. The (simp)le truth about excessive & obsessive romantic behaviors in men. (2023). https://ink.library.smu.edu.sg/etd_coll/516\nKrishnamurthy, V., & Duan, Y. (2017). Dependence Structure Analysis Of Meta-level Metrics in YouTube Videos: A Vine Copula Approach. arXiv preprint arXiv:1712.10232. â€œto explain the comment and the view of the video are relatedâ€\nLun-Wei Ku and Hsin-Hsi Chen (2007). Mining Opinions from the Web: Beyond Relevance Retrieval. Journal of American Society for Information Science and Technology, Special Issue on Mining Web Resources for Enhancing Information Retrieval, 58(12), pages 1838-1850.\nPew Research Center. (2020). Many Americans get news on YouTube, where news organizations and independent producers thrive side by side. https://www.pewresearch.org/journalism/2020/09/28/many-americans-get-news-on-youtube-where-news-organizations-and-independent-producers-thrive-side-by-side/\nZhou, W. (2024). é‡åº†è­¦æ–¹å‘å¸ƒâ€œèƒ–çŒ«â€äº‹ä»¶è­¦æƒ…é€šæŠ¥ [Chongqing police issue incident report on the â€œPangmaoâ€ incident]. Xinhua Net. http://www.news.cn/politics/20240519/fb56352660c94810a58e79bc18459a3e/c.html"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Project",
    "section": "",
    "text": "The raw dataset of approximately 8,000 YouTube comments, initially spread across six files, underwent a systematic cleaning process to prepare it for textual analysis. We first streamlined the data by retaining only the comment text, like counts, and reply status. Utilizing the stringr package, we normalized all whitespace and implemented crucial character filtering, deliberately preserving the full integrity of all Chinese characters to maintain cultural and contextual authenticity. The final output ensured a tidy, standardized dataset, encoded in UTF-8, making it immediately ready for downstream procedures like tokenization and sentiment analysis.\n\nView Full Analysis & Report (HTML)"
  },
  {
    "objectID": "projects.html#final-project-check-in-2",
    "href": "projects.html#final-project-check-in-2",
    "title": "Projects",
    "section": "",
    "text": "604_Final_check-in_2.qmd ## Project 2"
  },
  {
    "objectID": "projects.html#project-4",
    "href": "projects.html#project-4",
    "title": "Projects",
    "section": "Project 4",
    "text": "Project 4"
  },
  {
    "objectID": "Final_check-in_2.html",
    "href": "Final_check-in_2.html",
    "title": "DACSS785_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "Final_check-in_2.html#research-question-and-hypothesis",
    "href": "Final_check-in_2.html#research-question-and-hypothesis",
    "title": "DACSS785_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "Final_check-in_2.html#data-collection",
    "href": "Final_check-in_2.html#data-collection",
    "title": "DACSS785_Final_Project",
    "section": "Data Collection",
    "text": "Data Collection\nTo explore the concept of â€œsimping,â€ I collected YouTube comments from six relevant videos for textual analysis. I utilized an R scraping script to extract approximately 8,000 comments in total. Following a cleaning and filtering process, a dataset of around 7,000 practical comments was retained for analysis.\nI specifically focused on the case study known as the â€œèƒ–è²“è·³æ±Ÿäº‹ä»¶â€ (Suicide of Fat Cat). This event, which occurred in Mainland China, provides a particularly rich and relevant dataset because it was a well-documented news story officially reported by the Chinese court. This official documentation makes it a real and verifiable event, distinguishing it from mere rumors or social media anecdotes. Furthermore, the use of Chinese-language videos as the reference source is critical, as the Chinese-speaking audience possesses extensive background knowledge and cultural context directly related to the local details of this incident.\n\nSIMP001 - é™ªæ‰“éŠæˆ²è³ºç™¾è¬é¤Šå¥³å‹æ…˜é­åˆ†æ‰‹ï¼ã€Œèƒ–è²“äº‹ä»¶ã€å¼•çˆ†ä¸­åœ‹æ€§åˆ¥æˆ°çˆ­ï¼Ÿã€Œæ’ˆå¥³ã€æ»¿è¡—è·‘çš„èƒŒå¾ŒåŸå› ï¼Ÿã€TODAY çœ‹ä¸–ç•Œã€‘(https://www.youtube.com/watch?v=o5TfkwlthWU&t=13s) 1952 comments from 11/04/2025\nSIMP002 - å½“èƒ–çŒ«é‡åˆ°æå¥³ï¼Œä¸€ä¸ªå¹´è½»äººå¦‚ä½•èµ°ä¸Šä¸å½’è·¯ï¼Ÿï½œå¥³æƒï½œæå¥³ï½œèƒ–çŒ«ï½œç‹è€…è£è€€ï½œç”·å¥³å¹³æƒï½œæ—¥æœ¬ï½œæ¢…å¤§é«˜é€Ÿï½œèˆ†è®ºæ§åˆ¶ï½œç‹å±€æ‹æ¡ˆ20240507 (https://www.youtube.com/watch?v=39Gq_eOPuDY&t=1s) 3731 comments from 11/04/2025\nSIMP003 - è€æ¢ï¼šç»™â€œèƒ–çŒ«â€å¤šæ¡é€‰æ‹© é‡åº†â€œèƒ–çŒ«äº‹ä»¶â€ä¸æ˜¯æ€§åˆ«å¤§æˆ˜ å¦‚ä½•é¿å…æˆä¸ºâ€œèƒ–çŒ«â€(https://www.youtube.com/watch?v=mjcgg0wFpfE) 997 comments from 11/04/2025\nSIMP004 - å°ä¼™ç‚ºæ„›è·³æ±Ÿï¼Œæ‹œé‡‘çš„å¥³å‹ï¼Œå¸è¡€çš„è¦ªå§ï¼Œç„¡è‰¯çš„å•†å®¶ï¼Œç˜‹ç‹‚çš„ç¶²æ°‘ï¼Œèª°æ‰æ˜¯åŠ å®³è€…ï¼Ÿç‚ºä½•è­¦å¯Ÿèªå®šå¥³å‹ç„¡ç½ªï¼Œåè€Œæ˜¯è¦ªå§é•äº†æ³•ï¼Ÿä¸€å£æ°£çœ‹å®Œèƒ–è²“äº‹ä»¶å§‹æœ«ï¼| Wayneèª¿æŸ¥(https://www.youtube.com/watch?v=igs7GoIU4MU) 615 comments from 11/04/2025\nSIMP005 - ç¥ç´šé™ªç©ã€Œèƒ–è²“ã€é­è©ä¹¾227è¬äº¡ å¥³å‹é“æ­‰ï½œ20240506 ETåˆé–“æ–°è (https://www.youtube.com/watch?v=tAE83zZEcOY) 402 comments from 11/04/2025\nSIMP006 - è¢«æ’ˆå¥³é¨™å…‰50è¬ï¼ŒéŠæˆ²å®…ç”·è·³æ±Ÿè‡ªæ®ºï¼Œè½Ÿå‹•å…¨ç¶²ï¼æ’ˆå¥³è­šç«¹æ¦¨ä¹¾èƒ–è²“äº‹ä»¶çœŸç›¸ï¼ã€æ–°é—»æœ€å˜²ç‚¹ å§œå…‰å®‡ã€2024.0508(https://www.youtube.com/watch?v=YYngd2Yt3zk) 271 comments from 11/04/2025\n\n\nlibrary(plyr)\n\nWarning: package 'plyr' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.4.3\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(purrr)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:plyr':\n\n    compact\n\nlibrary(chromote)\n\nWarning: package 'chromote' was built under R version 4.4.3\n\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tibble)\nlibrary(nnet)\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.4.3\n\n\ncorrplot 0.95 loaded\n\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.4.3\n\n\nLoading required package: NLP\n\n\nWarning: package 'NLP' was built under R version 4.4.2\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.4.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.4.3\n\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.4.3\n\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.4.3\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(\"quanteda.textplots\")\n\nWarning: package 'quanteda.textplots' was built under R version 4.4.3\n\nlibrary(httr)\n\nWarning: package 'httr' was built under R version 4.4.3\n\n\n\nAttaching package: 'httr'\n\n\nThe following object is masked from 'package:NLP':\n\n    content\n\nlibrary(RColorBrewer)\nlibrary(RedditExtractoR)\n\nWarning: package 'RedditExtractoR' was built under R version 4.4.3\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.4.3\n\nlibrary(tidyr)\n\n\ndata1 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\ndata2 &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments.csv\")\ndata3 &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments.csv\")\ndata4 &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments.csv\")\ndata5 &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments.csv\")\ndata6 &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments.csv\")"
  },
  {
    "objectID": "Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "href": "Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "title": "DACSS785_Final_Project",
    "section": "Data Cleaning (info for the poster",
    "text": "Data Cleaning (info for the poster\nALSO GUIDE IT TO THE PART WHICH I WANT PLUS PRESENT THE CLEAN FORMAT IN THE POSTER\nThe raw dataset, collected as several CSV files, initially contained detailed comment metadata. The original structure included columns such as: videoId, commentId, parentId, author, text, likeCount, publishedAt, updatedAt, viewerRating, canRate, and reply.\nFor data cleaning, all CSV files in the Final_project_data folder were systematically processed using the R environment. The initial step was to streamline the dataset by retaining only the essential variables for textual and engagement analysis: text, likeCount, and reply.\nI utilized the R packages dplyr and stringr to focus on standardizing the text column. This involved a series of cleaning operations: normalization of whitespace (removing line breaks, tabs, and extra spaces, and trimming leading/trailing whitespace) and character filtering. Crucially, I removed non-essential symbols and unusual characters while meticulously preserving all Chinese characters to ensure the comments remained culturally authentic and meaningful for subsequent analysis.\nFinally, each cleaned and standardized dataset was saved as a new CSV file, appended with the suffix _cleaned. UTF-8 encoding was explicitly used to guarantee the accurate representation of the Chinese characters. This systematic workflow ensures the comment data are tidy, standardized, and immediately ready for downstream procedures, such as tokenization and sentiment or frequency analysis.\n\nsource(\"data_cleaning_CN.R\")\n\ndata cleaning complete!.\n\nSIMP001 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\n\n#Present the eample of the result\nhead(SIMP001)\n\n      videoId                  commentId parentId        author\n1 o5TfkwlthWU UgyekRC230MDXREkdeN4AaABAg     &lt;NA&gt;  @DanjonMeshi\n2 o5TfkwlthWU UgxangSP0zjJm6_gHfV4AaABAg     &lt;NA&gt;  @paullee4451\n3 o5TfkwlthWU UgwZdLtl6Eb2wgDWaDV4AaABAg     &lt;NA&gt;      @urikora\n4 o5TfkwlthWU UgwSmeqUyHYUXGD6l3l4AaABAg     &lt;NA&gt; @fayechen1928\n5 o5TfkwlthWU UgwpotCAmJmn2wWU7u54AaABAg     &lt;NA&gt; @running_goat\n6 o5TfkwlthWU UgxQSyQbnLT9r7I1faB4AaABAg     &lt;NA&gt;  @Jack2006103\n                                                                                                                       text\n1                                                             å•†å®¶é›†é«”çµ¦ç©ºè¢‹çœŸçš„ç¬‘æ­»ï¼Œä¸æ­¢ç”Ÿæ´»åœ¨ä¸­åœ‹ï¼Œé€£æ­»åœ¨ä¸­åœ‹éƒ½è¦å·è‘—æ¨‚ğŸ˜†\n2                                                                                                                      é ­é¦™\n3                       æ›´æ…˜çš„æ˜¯ï¼Œäººéƒ½èµ°äº†ä¸€å€‹æœˆ çµæœå°±åœ¨é€™æ™‚æ©Ÿé»è¢«æŠ“ä¾†æ“‹æ”¿åºœåšçš„é†œäº‹ï¼ˆè·¯å´©è¯ç‚ºè»Šè¡é€²å»å‘æ´ç„¶å¾Œå¿«é€Ÿç‡ƒèµ·ä¾†ï¼‰\n4                                                                                      æ¯æ¬¡çœ‹åˆ°ä¸­åœ‹é€™ç¨®æ‚²åŠ‡éƒ½è¦ºå¾—ä¸å¯æ€è­°ğŸ˜¨ğŸ˜­\n5 æˆ‘çœŸçš„å¿…é ˆå¾—èªªå°å²¸ç”·å¥³æˆ°çˆ­çœŸçš„è¶Šä¾†è¶Šåš´é‡== å•éå¥½å¹¾å€‹å°å²¸çš„å¥³ç”Ÿéƒ½èªç‚ºç”·ç”Ÿå°±æ˜¯æ‡‰è©²è¦çµ¦å½©ç¦® åƒé£¯å°±æ˜¯è¦å¹«å¥³ç”Ÿä»˜éŒ¢ç­‰ç­‰ å¾ˆå¯æ€•\n6                                                                                                é€™äº›åº—å®¶åƒäººè¡€é¥…é ­ï¼Œè¶…å™çˆ›\n  likeCount          publishedAt            updatedAt viewerRating canRate\n1        58 2024-05-09T16:01:52Z 2024-05-09T16:01:52Z         none    TRUE\n2         0 2024-05-09T16:02:23Z 2024-05-09T16:02:23Z         none    TRUE\n3       345 2024-05-09T16:05:05Z 2024-05-09T16:05:05Z         none    TRUE\n4         1 2024-05-09T16:05:57Z 2024-05-09T16:05:57Z         none    TRUE\n5       139 2024-05-09T16:06:47Z 2024-05-09T16:06:47Z         none    TRUE\n6         4 2024-05-09T16:07:29Z 2024-05-09T16:07:29Z         none    TRUE\n  reply\n1 FALSE\n2 FALSE\n3 FALSE\n4 FALSE\n5 FALSE\n6 FALSE\n\ndata1_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_cleaned.csv\")\ndata2_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments_cleaned.csv\")\ndata3_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments_cleaned.csv\")\ndata4_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments_cleaned.csv\")\ndata5_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments_cleaned.csv\")\ndata6_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments_cleaned.csv\")"
  },
  {
    "objectID": "Final_check-in_2.html#preprocess-the-data",
    "href": "Final_check-in_2.html#preprocess-the-data",
    "title": "DACSS785_Final_Project",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nFor visualizing the dominant linguistic patterns within the comment data, I employed two complementary approaches. First, a Word Cloud visualization (generated using the Word_cloud_visualization.R script) provided an intuitive, qualitative representation of high-frequency words, instantly highlighting the most common terms associated with discussions of â€œSIMPâ€ behavior.\nSecond, I conducted a quantitative rank-frequency analysis by applying Zipfâ€™s Law to the word corpus. After arranging all unique words by descending frequency and assigning a rank, I plotted the resulting distribution using the ggplot2 package. The resulting visualization confirmed that the comment discourse adheres to a Zipfian distribution, where a few words account for a disproportionate share of the total vocabulary.\nThe key terms driving the discourse were clearly identifiable:\nThese visualizations collectively offer both quantitative validation (Zipfâ€™s Law distribution) and qualitative insight (Word Cloud/Top Terms) into how the audience discusses and perceives the central event and the related concept of â€œSIMPâ€ behavior in this context. The high frequency of questioning and uncertainty (ç‚ºä»€éº¼, ä¸çŸ¥é“, æ˜¯ä¸æ˜¯) coupled with terms of exploitation (pua) and suffering (å—å®³è€…) reveals a key focus on moral judgment and accountability in the discussion.\n\nsource(\"TOKENIZATION.R\")\n\nTokenization complete!\n\nSIMP001_comments_tokens &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_tokens.csv\")\n#Present the eample of the result\nhead(SIMP001_comments_tokens)\n\n  likeCount reply     word\n1       345 FALSE   è¡é€²å»\n2         1 FALSE ä¸å¯æ€è­°\n3       139 FALSE   è¶Šä¾†è¶Š\n4       139 FALSE   å¥½å¹¾å€‹\n5       141 FALSE   éº¥ç•¶å‹\n6         3 FALSE   æœ‰äººèªª\n\n\n\nsource(\"Word_frequency.R\")\n\nWord Frequency Calculation Complete!\n\nSIMP001_wordfreq &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_wordfreq.csv\")\n\n#Present the eample of the result\nhead(SIMP001_wordfreq)\n\n    word  n rank\n1 å—å®³è€… 75    1\n2 ç‚ºä»€éº¼ 67    2\n3 ä¸çŸ¥é“ 53    3\n4    pua 42    4\n5 é€™ä»¶äº‹ 34    5\n6 ä¸€å€‹äºº 32    6\n\n\n\nsource(\"Same_Word.R\")\n\nCommon words saved to: Final_project_data/common_words_across_files.csv \n\ncommon_words &lt;- read.csv(\"Final_project_data/common_words_across_files.csv\")\n\n#Present the eample of the result\nhead(common_words)\n\n    word total_count\n1 ä¸çŸ¥é“         172\n2 ä¸ºä»€ä¹ˆ         154\n3 å—å®³è€…         118\n4    pua         115\n5 æ˜¯ä¸æ˜¯         101\n6 ç‚ºä»€éº¼         100\n\n\n\nsource(\"Change_to_Traditional_Chinese.R\")\n\nWarning: package 'textstem' was built under R version 4.4.3\n\n\nLoading required package: koRpus.lang.en\n\n\nWarning: package 'koRpus.lang.en' was built under R version 4.4.3\n\n\nLoading required package: koRpus\n\n\nWarning: package 'koRpus' was built under R version 4.4.3\n\n\nLoading required package: sylly\n\n\nWarning: package 'sylly' was built under R version 4.4.3\n\n\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\n\nAttaching package: 'koRpus'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    tokens, types\n\n\nThe following object is masked from 'package:tm':\n\n    readTagged\n\n\nThe following object is masked from 'package:readr':\n\n    tokenize\n\n\nWarning: package 'tmcn' was built under R version 4.4.3\n\n\n# tmcn Version: 0.2-13\n\n\n[1] \"girl\"    \"woman\"   \"simping\" \"lover\"  \nTranslation complete! Output saved to 'your_output_file.csv'\n\nTraditional_Chinese_data_cleaned &lt;- read.csv(\"Final_project_data/traditional_common_words_combined.csv\")\n\n#Present the data after cleaning\nhead(Traditional_Chinese_data_cleaned)\n\n  traditional_text total_count\n1           ç‚ºä»€éº¼         254\n2           ä¸çŸ¥é“         172\n3           å—å®³è€…         118\n4              pua         116\n5           æ˜¯ä¸æ˜¯         101\n6           å¥³æœ‹å‹          95"
  },
  {
    "objectID": "Final_check-in_2.html#visualization",
    "href": "Final_check-in_2.html#visualization",
    "title": "DACSS785_Final_Project",
    "section": "Visualization",
    "text": "Visualization\nFor visualizing patterns in the comments, I used two approaches. First, the Word_cloud_visualization.R script generated word clouds to highlight high-frequency words, providing a clear and intuitive view of the most common terms associated with discussions of â€œSIMPâ€ behavior. Second, I applied Zipfâ€™s Law to examine the relationship between word rank and frequency. After arranging words by descending frequency and assigning ranks, I plotted all words using ggplot2, labeling only the top five most frequent words to emphasize the key terms in the discourse. The resulting visualizations offer both quantitative and qualitative insight into how people discuss and perceive â€œSIMPâ€ behavior in YouTube comments.\n\nsource(\"Word_cloud_visualization.R\")\n\n\n\n\n\n\n\n\nWord Cloud generated for: traditional_common_words_combined.csv\n\n\n\n# Sort by frequency and assign ranks\nzipf_data_ranked &lt;- Traditional_Chinese_data_cleaned %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  mutate(rank = row_number())\n\n# Print the top 5 ranked words to confirm the data structure\nprint(head(zipf_data_ranked, 5))\n\n  traditional_text total_count rank\n1           ç‚ºä»€éº¼         254    1\n2           ä¸çŸ¥é“         172    2\n3           å—å®³è€…         118    3\n4              pua         116    4\n5           æ˜¯ä¸æ˜¯         101    5\n\n# --- Linear Scale (As Requested) ---\n\nggplot(zipf_data_ranked, aes(x = rank, y = total_count)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"darkorange\", size = 1.5) +\n  geom_text(\n    # Label the top 8 words\n    aes(label = ifelse(rank &lt;= 6, traditional_text, \"\")),\n    vjust = -0.8,\n    size = 3.5,\n    check_overlap = TRUE # Prevents overlapping labels\n  ) +\n  labs(\n    title = \"Zipfâ€™s Law: Word Rank vs Frequency\",\n    x = \"Rank of Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nTranslation\n\n\n\nRank\nWord\nTranslate\n\n\n1\nç‚ºä»€éº¼\nâ€œWhy / Why is it thatâ€¦â€\n\n\n2\nä¸çŸ¥é“\nâ€œDonâ€™t knowâ€\n\n\n3\nå—å®³è€…\nâ€œVictimâ€\n\n\n4\npua\nâ€œPUAâ€\n\n\n5\næ˜¯ä¸æ˜¯\nâ€œIs it / Is it not?â€\n\n\n6\nå¥³æœ‹å‹\nâ€œGirlfriendâ€"
  },
  {
    "objectID": "Final_check-in_2.html#word-embedding",
    "href": "Final_check-in_2.html#word-embedding",
    "title": "DACSS785_Final_Project",
    "section": "Word Embedding",
    "text": "Word Embedding\nFor semantic analysis, I applied Word2Vec using a pseudo-document approach to capture relationships between words in the comments. Each word was repeated according to its frequency (total_count) to create co-occurrence information, which is essential for small datasets where natural co-occurrences are limited. The repeated words were then combined into a single space-separated pseudo-document and used to train a skip-gram Word2Vec model with a vector dimension of 50, window size of 5, and 50 iterations, setting min_count = 1 to include all words.\nThe resulting word vectors allow calculation of cosine similarity to examine semantic relationships between words, as well as clustering and downstream supervised learning tasks. For example, the vector for a keyword such as â€œç‚ºä»€éº¼â€ can be compared with all other word vectors to identify the top semantically similar words, revealing patterns in how concepts related to â€œSIMPâ€ behavior are discussed in YouTube comments. This approach provides a robust representation of word meaning in the context of the dataset while accommodating the limited co-occurrence information inherent in smaller comment datasets.\n\n# Word2Vec can be the best option for the word embeding.\n\nsource(\"Word Embeddings.R\")\n\nWarning: package 'word2vec' was built under R version 4.4.3\n\n\nWarning: package 'text2vec' was built under R version 4.4.3"
  },
  {
    "objectID": "Final_check-in_2.html#sentiment-analysis",
    "href": "Final_check-in_2.html#sentiment-analysis",
    "title": "DACSS785_Final_Project",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nFor sentiment analysis, I applied a custom Chinese sentiment dictionary tailored to the context of â€œSIMPâ€ behavior. The dictionary categorizes words into three groups: positive (supportive or relationship-related words such as â€œå¥³æœ‹å‹â€ and â€œé—œå¿ƒâ€), negative (critical or unfairness-related words such as â€œä¸å€¼å¾—â€ and â€œä¸å…¬å¹³â€), and behavior (attention-seeking or â€œsimpâ€ behavior words such as â€œpuaâ€ and â€œè¿½æ±‚â€). Using R, I computed sentiment scores for each word in the dataset by summing occurrences in these categories. A raw polarity score was calculated as the sum of positive and behavior counts minus negative counts, then normalized by the total occurrences of all dictionary words to produce a relative polarity measure.\nThe analysis revealed that the current positive and negative categories do not fully capture the sentiment expressed in the comments. Some words were misclassified or contextually ambiguous, highlighting that the dictionary needs further adjustment and refinement to improve accuracy. Polarity distributions were visualized using a histogram, providing an overview of how positive, negative, and behavior-related language appears in discussions of â€œSIMPâ€ behavior. This approach provides a preliminary sentiment assessment while acknowledging the limitations of the existing dictionary.\n\nsource(\"Sentiment Analysis.R\")\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” forcats   1.0.0     âœ” lubridate 1.9.4\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– NLP::annotate()         masks ggplot2::annotate()\nâœ– httr::content()         masks NLP::content()\nâœ– dplyr::filter()         masks stats::filter()\nâœ– jsonlite::flatten()     masks purrr::flatten()\nâœ– rvest::guess_encoding() masks readr::guess_encoding()\nâœ– dplyr::lag()            masks stats::lag()\nâœ– koRpus::tokenize()      masks readr::tokenize()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'quanteda.textmodels' was built under R version 4.4.3\n\n\nWarning: package 'stopwords' was built under R version 4.4.3\n\n\n\nAttaching package: 'stopwords'\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nWarning: package 'caret' was built under R version 4.4.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:httr':\n\n    progress\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n[1] \"DFM after Custom Simp Dictionary Lookup (Counts per category):\"\n\n### SUPERVISED LEARNING (Naive Bayes Classification)\n[1] \"Confusion Matrix:\"\n\n### LEXICON-BASED NTUSD SENTIMENT RESULTS\n[1] \"Mean Sentiment Score by Simping Label (NTUSD):\"\n# A tibble: 2 Ã— 2\n  contains_simp_factor mean_sentiment\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 FALSE                       -0.0990\n2 TRUE                        -0.477 \n\n\n\n\n\n\n\n\n\nIn this graph:\nX-Axis: Simping Label:\n\nFALSE: Comments that do not contain any of the words from your custom â€œsimpâ€ dictionary (e.g., â€œèˆ”ç‹—,â€ â€œå·¥å…·äºº,â€ â€œä¸€å»‚æƒ…é¡˜,â€ etc.).\nTRUE: Comments that do contain at least one word from your custom â€œsimpâ€ dictionary.\n\nY-Axis: Comment Sentiment Score:\n\nPositive Scores (above 0): Indicate a more positive overall sentiment.\nZero (0): Indicates a neutral or balanced sentiment.\nNegative Scores (below 0): Indicate a more negative overall sentiment."
  },
  {
    "objectID": "Final_check-in_2.html#sentiment-analysis-summary",
    "href": "Final_check-in_2.html#sentiment-analysis-summary",
    "title": "DACSS785_Final_Project",
    "section": "Sentiment Analysis Summary",
    "text": "Sentiment Analysis Summary\nThe lexicon-based sentiment analysis, utilizing the NTUSD dictionary, reveals a pronounced negative emotional shift in texts discussing the â€œsimpâ€ phenomenon. Specifically, content that contains terms from the custom dictionaryâ€”which targets themes like â€œsimp behaviorâ€ (e.g., èˆ”ç‹—, simp), â€œvictim positionâ€ (e.g., å—å®³è€…, pua), and â€œrelationship imbalanceâ€â€”shows a highly negative mean sentiment score of -0.477. This score is significantly more negative than the average score of -0.0990 found in texts that do not contain these specific terms. This sharp difference (a nearly five-fold increase in negative sentiment magnitude) indicates that conversations about excessive one-sided effort, perceived exploitation, and unequal relationshipsâ€”the core of the â€œsimpâ€ conceptâ€”are strongly associated with negative emotional discourse within the corpus."
  },
  {
    "objectID": "Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "href": "Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "title": "DACSS785_Final_Project",
    "section": "Supervised Learning Analysis (Naive Bayes Classification)",
    "text": "Supervised Learning Analysis (Naive Bayes Classification)\nThe Naive Bayes model was employed to classify comments based on whether they contained the â€œsimpâ€ factor, using a cleaned feature set that excluded all words from the custom â€œsimpâ€ dictionary to prevent data leakage. The model achieved an overall Accuracy of 81.69%, which is slightly higher than the No Information Rate (NIR) of 80.36%, indicating its performance is marginally better than random guessing based on class prevalence.\nHowever, a closer look at the results reveals a significant class imbalance issue and skewed performance:\n\nHigh Sensitivity (Recall): The model is excellent at correctly identifying comments that do NOT contain the simp factor (the majority class, FALSE), with a high Sensitivity of 95.72%.\nLow Specificity: Conversely, the model is very poor at correctly identifying comments that DO contain the simp factor (the minority class, TRUE), with a low Specificity of 24.29%.\nKappa Value: The Kappa statistic of 0.2565 suggests only a fair level of agreement beyond chance.\n\nIn summary, the high overall accuracy is largely driven by correctly classifying the prevalent negative class (FALSE). The model struggles to reliably identify actual â€œsimpâ€ comments (TRUE), suggesting that the remaining general vocabulary in the comments lacks sufficient predictive power to consistently distinguish between the two categories without the core dictionary terms.\n\nsource(\"Supervised_Learning.R\")\n\n\n--- Solving Data Leakage: Remove the word which exist in Simp dictionary ---\n[1] \"Original (matrix_main): 2703\"\n[1] \"Remove the word in Simp dictionary (X_cleaned): 2674\"\n\n--- 6. Naive Bayes Training ---\n\n--- Naive Bayes Prediction ---\n[1] \"Confusion Matrix:\"\nConfusion Matrix and Statistics\n\n                 y_test\npredicted_cleaned FALSE TRUE\n            FALSE   693  134\n            TRUE     31   43\n                                        \n               Accuracy : 0.8169        \n                 95% CI : (0.79, 0.8416)\n    No Information Rate : 0.8036        \n    P-Value [Acc &gt; NIR] : 0.1676        \n                                        \n                  Kappa : 0.2565        \n                                        \n Mcnemar's Test P-Value : 2.011e-15     \n                                        \n            Sensitivity : 0.9572        \n            Specificity : 0.2429        \n         Pos Pred Value : 0.8380        \n         Neg Pred Value : 0.5811        \n              Precision : 0.8380        \n                 Recall : 0.9572        \n                     F1 : 0.8936        \n             Prevalence : 0.8036        \n         Detection Rate : 0.7691        \n   Detection Prevalence : 0.9179        \n      Balanced Accuracy : 0.6001        \n                                        \n       'Positive' Class : FALSE"
  },
  {
    "objectID": "Final_check-in_2.html#topic-modeling-lda",
    "href": "Final_check-in_2.html#topic-modeling-lda",
    "title": "DACSS785_Final_Project",
    "section": "Topic Modeling (LDA)",
    "text": "Topic Modeling (LDA)\nThe script follows a standard text mining workflow using the tidyverse and text2vec packages:\n\nData Preparation: It reads the individual tokenized comment files, reconstructs the full comments by assigning and aggregating tokens by a unique document ID (doc_id), and then combines the tokens back into complete text strings.\nFeature Engineering: It creates an iterator from the aggregated text and builds a vocabulary. Crucially, it prunes the vocabulary by removing words that occur less than three times (term_count_min = 3), which helps reduce noise and improves the quality of the derived topics.\nDTM Creation: The processed tokens are converted into a Document-Term Matrix (DTM), which is the input required for LDA.\nModel Training: The script initializes and trains an LDA model with a predefined number of topics (K=8) and 500 iterations.\nOutput: Finally, the code extracts and saves two key results: the Topic-Word distribution (the top 10 most characteristic words for each of the 8 topics) and the Document-Topic distribution (the probability that each comment belongs to each topic), storing both as CSV files for subsequent qualitative analysis.\n\n\nsource(\"Topic_Model.R\")\n\n[1] \"Starting LDA Topic Modeling with K = 8\"\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======================================================================| 100%\n[1] \"LDA Training Complete.\"\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nâ„¹ Using compatibility `.name_repair`.\n\n\n[1] \"Top 10 Words for Each Topic:\"\n# A tibble: 10 Ã— 9\n   Topic_Word_Rank Topic_0 Topic_1 Topic_2 Topic_3  Topic_4  Topic_5 Topic_6\n   &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1 Word_1          pua     ä¸çŸ¥é“  å—å®³è€…  æ˜¯ä¸æ˜¯   ç‚ºä»€éº¼   ä¸çŸ¥é“  è€Œä¸æ˜¯ \n 2 Word_2          ä¸ºä»€ä¹ˆ  ä¸‡ä½™å…ƒ  ä¸ºä»€ä¹ˆ  å¥³æœ‹å‹   åœ¨ä¸€èµ·   è¶Šæ¥è¶Š  é€™ä»¶äº‹ \n 3 Word_3          ä¸­å›½äºº  pua     pua     ä¸ºä»€ä¹ˆ   ä¹Ÿå¯ä»¥   å¤§éƒ¨åˆ†  ä¸çŸ¥é“ \n 4 Word_4          ç”·å­©å­  å¯èƒ½æ˜¯  çœŸçš„æ˜¯  ä¸çŸ¥é“   ä¸å¯èƒ½   éƒ½ä¸æ˜¯  ä¸€å®šæ˜¯ \n 5 Word_5          å¥³å­©å­  ä¸€å€‹äºº  éº¦å½“åŠ³  ä¸éœ€è¦   éƒ½æ²’æœ‰   çœŸçš„æ˜¯  å¥³æœ‹å‹ \n 6 Word_6          ä¹Ÿä¸æ˜¯  é€™ä»¶äº‹  å®¶åº­çš„  ä¹Ÿæ²¡æœ‰   é€™å°±æ˜¯   å—å®³è€…  èƒ½ä¸èƒ½ \n 7 Word_7          æ˜¯ä¸æ˜¯  ä¸­å›½äºº  å…¨ä¸–ç•Œ  å¤§éƒ¨åˆ†   ä¹Ÿæ²’æœ‰   å¥³æœ‹å‹  äººæ°‘å¹£ \n 8 Word_8          æ²¡ä»€ä¹ˆ  å°¤å…¶æ˜¯  å…¶å®æ˜¯  ç”·æœ‹å‹   ç”·å°Šå¥³å‘ æˆ‘çŸ¥é“  åƒåœ¾æ¡¶ \n 9 Word_9          å¯èƒ½æ˜¯  çœŸçš„æ˜¯  å¥½åƒæ˜¯  ç”·å¥³å¹³ç­‰ æ²’ä»€éº¼   æ˜¯ä¸æ˜¯  æœ‰äº›äºº \n10 Word_10         ä¸å€¼å¾—  ç‚ºä»€éº¼  è‚¯å®šæ˜¯  æ‰€è°“çš„   ä¸å­˜åœ¨   ä¹Ÿä¸æ˜¯  ä¸ºä»€ä¹ˆ \n# â„¹ 1 more variable: Topic_7 &lt;chr&gt;\n[1] \"Saved topic words to lda_topic_words.csv\"\n[1] \"Document-Topic Distribution (Head):\"\n# A tibble: 6 Ã— 9\n  doc_id    V1    V2    V3    V4    V5    V6    V7    V8\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1     0  0      0    0      0     0       0     0\n2      2     0  0      0    0      0     0       0     0\n3      3     0  0.05   0    0.15   0.6   0.2     0     0\n4      4     0  0.1    0.2  0      0.4   0.3     0     0\n5      5     0  0      0.8  0      0.2   0       0     0\n6      6     0  0      0    0      0     0       1     0\n[1] \"Saved document-topic distribution to lda_doc_topic_distr.csv\"\n\n\nTranslation for the every words in the topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic_Word_Rank\nTopic_0\nTopic_1\nTopic_2\nTopic_3\nTopic_4\nTopic_5\nTopic_6\nTopic_7\n\n\nWord_1\nPUA\nDonâ€™t know\nVictim\nIs it?\nWhy\nDonâ€™t know\nAnd not\nBuffet (Resource)\n\n\nWord_2\nWhy\nOver 10k yuan\nWhy\nGirlfriend\nBe together\nMore and more\nThis matter\nHighway\n\n\nWord_3\nChinese people\nPUA\nPUA\nWhy\nAlso can\nMajority\nDonâ€™t know\nToo good\n\n\nWord_4\nBoy / Male\nPossibly\nTruly is\nDonâ€™t know\nImpossible\nAre not all\nMust be\nWhy\n\n\nWord_5\nGirl / Female\nA person\nMcDonaldâ€™s\nDonâ€™t need\nDonâ€™t have at all\nTruly is\nGirlfriend\nMainly is\n\n\nWord_6\nAlso is not\nThis matter\nFamilyâ€™s\nAlso donâ€™t have\nThis is\nVictim\nCan or cannot\nSome people\n\n\nWord_7\nIs it?\nChinese people\nWhole world\nMajority\nAlso donâ€™t have\nGirlfriend\nRMB (Money)\nInequality\n\n\nWord_8\nNothing much\nEspecially\nActually is\nBoyfriend\nMale Superiority\nI know\nTrash Can (Worthless)\nShould be\n\n\nWord_9\nPossibly\nTruly is\nSeems like\nGender Equality\nNothing much\nIs it?\nSome people\nA person\n\n\nWord_10\nNot worth it\nWhy\nDefinitely is\nSo-called\nDoes not exist\nAlso is not\nWhy\nGender Equality\n\n\n\nThe interpretation for each topic\n\n\n\n\n\n\n\n\nTopic\nCore Keywords & Interpretation\nSuggested Topic Label\n\n\nTopic 0\nThis topic strongly links the PUA phenomenon with discussions about specific gender roles and identities within the Chinese context. The presence of â€œNot worth itâ€ suggests this cluster is focused on evaluating the value of actions/relationships under the PUA framework.\nPUA & Gender Dynamics in China\n\n\nTopic 1\nThis topic mixes uncertainty and specific financial figures (Over 10k yuan), directly linked to PUA. It suggests discussions about high-stakes financial loss or investment by an individual in a relationship where the outcome or reality is unclear.\nFinancial Dimension of PUA & Uncertainty\n\n\nTopic 2\nThe simultaneous presence of â€œVictim,â€ â€œPUA,â€ and â€œTruly isâ€ indicates a core discussion cluster dedicated to validating the existence and reality of being exploited. â€œMcDonaldâ€™sâ€ implies cheap/casual provision, while â€œFamilyâ€™sâ€ suggests the conversation may touch on the origins or impact of these dynamics within a family unit.\nValidating Victimhood & Low-Cost Exploitation\n\n\nTopic 3\nThis topic is saturated with questioning terms (â€œIs it?â€, â€œWhy?â€, â€œDonâ€™t knowâ€), applied directly to boyfriend/girlfriend roles and the concept of gender equality. It represents a pervasive atmosphere of skepticism and critical discussion about expected behavior in modern relationships.\nSkepticism & Questioning of Relationship Roles\n\n\nTopic 4\nA highly polarized topic that denies (ä¸å¯èƒ½, ä¸å­˜åœ¨) the relevance or existence of Male Superiority (ç”·å°Šå¥³å‘). It focuses on the possibility of being together (åœ¨ä¸€èµ·), suggesting a desire for modern, equal partnerships and a strong rejection of patriarchal norms.\nDenial of Traditional Patriarchy in Relationships\n\n\nTopic 5\nTerms like â€œMore and moreâ€ and â€œMajorityâ€ point to a discussion of social trends and scale. When combined with â€œVictimâ€ and â€œGirlfriend,â€ it indicates a conversation about whether victimhood is becoming increasingly common or if the perception of victimhood is changing within the female partner role.\nDiscussing Shifting Social Norms & Victim Pool\n\n\nTopic 6\nThis is the most explicitly transactional topic. It discusses financial payment (RMB) and the concept of a person being reduced to a â€œTrash Canâ€ (worthless/emotional dumping ground). The use of â€œAnd notâ€ suggests a debate over what a relationship should be versus what it currently is (i.e., not a transaction, but one of money/exploitation).\nMonetary Value vs.Â Emotional Worth (The Price of Simping)\n\n\nTopic 7\nThis topic links resource provision (implied by â€œBuffetâ€ and â€œHighway,â€ often used as metaphors for free/easy access) with discussions of Inequality and Gender Equality. It debates whether resources should be provided freely, who is responsible for providing them, and the resulting fairness in the relationship structure.\nResource Provision & Equality Debate\n\n\n\n\n\n\nDTM\nTopic_Word_Rank\n\n\nV1\nTopic_0\n\n\nV2\nTopic_1\n\n\nV3\nTopic_2\n\n\nV4\nTopic_3\n\n\nV5\nTopic_4\n\n\nV6\nTopic_5\n\n\nV7\nTopic_6\n\n\nV8\nTopic_7"
  },
  {
    "objectID": "Final_check-in_2.html#causal-inference",
    "href": "Final_check-in_2.html#causal-inference",
    "title": "DACSS785_Final_Project",
    "section": "Causal Inference",
    "text": "Causal Inference\nIn the casual inference part, I present the Ordinary Least Squares (OLS) regression model to analyze how the probability of eight LDA topics influences the number of likes received by a comment (likeCount). Topic V8 (PUA/Victim) was set as the reference group (Reference Topic) in the model. Overall, the modelâ€™s explanatory power is extremely low (\\(\\text{Adjusted R-squared} = 0.00025\\)), suggesting that the variation in likeCount is primarily influenced by factors outside the model, rather than the topics themselves. However, the coefficient tests for individual topics revealed that Topic V2 demonstrated a statistically significant positive influence. After controlling for the effects of other topics, an increase of 1 unit in the probability of Topic V2 (Financial/Money II), relative to the reference group V8, is expected to increase the number of likes by approximately 9.66 (\\(p = 0.038^{*}\\)). This suggests that specific discussion content related to money or finance is more likely to garner attention and agreement within the community. Apart from the intercept, the remaining topics (V1, V3, V4, V5, V6, and V7) did not show a statistically significant relationship with the number of likes.\n\nsource(\"Causal_Inference.R\")\n\nRows: 3006 Columns: 9\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (9): doc_id, V1, V2, V3, V4, V5, V6, V7, V8\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n[1] \"--- OLS Regression Results (Outcome: likeCount) ---\"\n[1] \"Reference Topic: V8 (PUA/Victim)\"\n\nCall:\nlm(formula = likeCount ~ V1 + V2 + V3 + V4 + V5 + V6 + V7, data = merged_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -18.59   -8.93   -6.76   -5.12 1763.41 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.934      2.301   3.882 0.000106 ***\nV1            -2.424      4.144  -0.585 0.558600    \nV2             9.655      4.656   2.074 0.038172 *  \nV3            -2.701      4.312  -0.626 0.531107    \nV4            -2.380      4.266  -0.558 0.576978    \nV5            -2.815      4.399  -0.640 0.522298    \nV6             1.729      4.351   0.398 0.691027    \nV7            -0.953      4.363  -0.218 0.827099    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.08 on 2998 degrees of freedom\nMultiple R-squared:  0.002579,  Adjusted R-squared:  0.00025 \nF-statistic: 1.107 on 7 and 2998 DF,  p-value: 0.3553"
  },
  {
    "objectID": "Final_check-in_2.html#conclusion",
    "href": "Final_check-in_2.html#conclusion",
    "title": "DACSS785_Final_Project",
    "section": "Conclusion",
    "text": "Conclusion\n(PUT THESE TWO POINT INTO THE FUTURE WORK ALSO NEED TO EXPLAIN ABOUT DIFFICULTY TO COLLECT THE ACADEMIC DATA THERE ** SIMP DOES NOT HAVE A FORMAL DEFINITION**\nâ€œSynthesizing the projectâ€™s findings, the primary discovery is that community engagement (\\(\\text{likeCount}\\)) on YouTube comments is not driven by broad emotional tone or general topics, but rather by a specific, critical narrative focused on â€˜financial exploitation and victimhoodâ€™ (\\(\\text{Topic}\\) \\(\\text{V2}\\)). This specific form of critical discussion related to â€˜SIMPâ€™ behavior is extremely negative in sentiment (\\(\\text{mean}\\) \\(\\text{sentiment} = -0.477\\)) and is so unique in its linguistic pattern that its occurrence can be accurately predicted by the supervised learning model. Consequently, the communityâ€™s response to â€˜SIMPâ€™ behavior is highly concentrated and emotionally charged, with its online visibility predominantly stemming from comments that link the behavior directly to concrete financial inequality and victim scenarios.â€"
  },
  {
    "objectID": "Final_check-in_2.html#future",
    "href": "Final_check-in_2.html#future",
    "title": "DACSS785_Final_Project",
    "section": "Future",
    "text": "Future\nFuture work should prioritize addressing the observed limitations in both the supervised classification model and the initial data preprocessing pipeline to enhance the robustness and explanatory power of the analysis. Firstly, while the initial Naive Bayes classifier provided baseline insights, its predictive performance should be critically re-evaluated. Improving the accuracy of the automated simp classification label requires exploring more sophisticated machine learning techniques, such as Support Vector Machines (SVMs), Gradient Boosting, or even Transformer-based deep learning models. Concurrently, enhancing the feature set by refining or expanding the custom dictionariesâ€”perhaps incorporating sentiment scores or incorporating word embeddingsâ€”could significantly boost the modelâ€™s ability to discriminate between classes, moving beyond simple bag-of-words approaches. Secondly, a crucial area for improvement lies in the token filtering and data processing stage. Despite standard removal procedures, the presence of numerous contextually irrelevant tokens, such as specific objects (â€œé«˜é€Ÿå…¬è·¯â€) and brands (â€œéº¥ç•¶å‹â€), confirms the necessity of a more rigorous, domain-specific cleanup. Future efforts must focus on constructing an expanded, domain-aware stop word list or implementing Named Entity Recognition (NER) to systematically identify and remove non-topical, low-information tokens, ensuring the remaining features are highly predictive and representative of the core concepts being discussed."
  },
  {
    "objectID": "Final_check-in_2.html#reference",
    "href": "Final_check-in_2.html#reference",
    "title": "DACSS785_Final_Project",
    "section": "Reference",
    "text": "Reference\nHO, Daniel. The (simp)le truth about excessive & obsessive romantic behaviors in men. (2023). https://ink.library.smu.edu.sg/etd_coll/516\nKrishnamurthy, V., & Duan, Y. (2017). Dependence Structure Analysis Of Meta-level Metrics in YouTube Videos: A Vine Copula Approach. arXiv preprint arXiv:1712.10232. â€œto explain the comment and the view of the video are relatedâ€\nLun-Wei Ku and Hsin-Hsi Chen (2007). Mining Opinions from the Web: Beyond Relevance Retrieval. Journal of American Society for Information Science and Technology, Special Issue on Mining Web Resources for Enhancing Information Retrieval, 58(12), pages 1838-1850.\nPew Research Center. (2020). Many Americans get news on YouTube, where news organizations and independent producers thrive side by side. https://www.pewresearch.org/journalism/2020/09/28/many-americans-get-news-on-youtube-where-news-organizations-and-independent-producers-thrive-side-by-side/\nZhou, W. (2024). é‡åº†è­¦æ–¹å‘å¸ƒâ€œèƒ–çŒ«â€äº‹ä»¶è­¦æƒ…é€šæŠ¥ [Chongqing police issue incident report on the â€œPangmaoâ€ incident]. Xinhua Net. http://www.news.cn/politics/20240519/fb56352660c94810a58e79bc18459a3e/c.html"
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "About Me",
    "section": "",
    "text": "My primary professional and academic goals are currently focused on two key areas:\n\nAcademic Completion: Successfully completing my Masterâ€™s program in the Data Analytics and Computational Social Science (DACSS) program at the University of Massachusetts Amherst (UMass Amherst).\nCareer Transition: Securing a professional position focused on Data Analysis or Data Science, where I can apply the quantitative, computational, and analytical skills gained during my studies."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "",
    "text": "I hold a Bachelor of Science degree in Computer Science from Arizona State University, and I am currently preparing to pursue a Masterâ€™s degree in Data Analysis and Computational Social Science at the University of Massachusetts Amherst. My primary academic interest lies in data analysis, which is driven by my enthusiasm for collecting information from various platforms and employing analytical methods to uncover intriguing patterns and phenomena, particularly when working with text as data. I enjoy the process of transforming raw text into actionable insights and exploring the subtle nuances hidden within large textual datasets."
  },
  {
    "objectID": "projects.html#final-project",
    "href": "projects.html#final-project",
    "title": "Project",
    "section": "",
    "text": "The raw dataset of approximately 8,000 YouTube comments, initially spread across six files, underwent a systematic cleaning process to prepare it for textual analysis. We first streamlined the data by retaining only the comment text, like counts, and reply status. Utilizing the stringr package, we normalized all whitespace and implemented crucial character filtering, deliberately preserving the full integrity of all Chinese characters to maintain cultural and contextual authenticity. The final output ensured a tidy, standardized dataset, encoded in UTF-8, making it immediately ready for downstream procedures like tokenization and sentiment analysis.\n\nView Full Analysis & Report (HTML)"
  },
  {
    "objectID": "projects.html#project-5",
    "href": "projects.html#project-5",
    "title": "Projects",
    "section": "Project 5",
    "text": "Project 5"
  },
  {
    "objectID": "758/Final_check-in_2.html",
    "href": "758/Final_check-in_2.html",
    "title": "DACSS785_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "758/Final_check-in_2.html#research-question-and-hypothesis",
    "href": "758/Final_check-in_2.html#research-question-and-hypothesis",
    "title": "DACSS785_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "758/Final_check-in_2.html#data-collection",
    "href": "758/Final_check-in_2.html#data-collection",
    "title": "DACSS785_Final_Project",
    "section": "Data Collection",
    "text": "Data Collection\nTo explore the concept of â€œsimping,â€ I collected YouTube comments from six relevant videos for textual analysis. I utilized an R scraping script to extract approximately 8,000 comments in total. Following a cleaning and filtering process, a dataset of around 7,000 practical comments was retained for analysis.\nI specifically focused on the case study known as the â€œèƒ–è²“è·³æ±Ÿäº‹ä»¶â€ (Suicide of Fat Cat). This event, which occurred in Mainland China, provides a particularly rich and relevant dataset because it was a well-documented news story officially reported by the Chinese court. This official documentation makes it a real and verifiable event, distinguishing it from mere rumors or social media anecdotes. Furthermore, the use of Chinese-language videos as the reference source is critical, as the Chinese-speaking audience possesses extensive background knowledge and cultural context directly related to the local details of this incident.\n\nSIMP001 - é™ªæ‰“éŠæˆ²è³ºç™¾è¬é¤Šå¥³å‹æ…˜é­åˆ†æ‰‹ï¼ã€Œèƒ–è²“äº‹ä»¶ã€å¼•çˆ†ä¸­åœ‹æ€§åˆ¥æˆ°çˆ­ï¼Ÿã€Œæ’ˆå¥³ã€æ»¿è¡—è·‘çš„èƒŒå¾ŒåŸå› ï¼Ÿã€TODAY çœ‹ä¸–ç•Œã€‘(https://www.youtube.com/watch?v=o5TfkwlthWU&t=13s) 1952 comments from 11/04/2025\nSIMP002 - å½“èƒ–çŒ«é‡åˆ°æå¥³ï¼Œä¸€ä¸ªå¹´è½»äººå¦‚ä½•èµ°ä¸Šä¸å½’è·¯ï¼Ÿï½œå¥³æƒï½œæå¥³ï½œèƒ–çŒ«ï½œç‹è€…è£è€€ï½œç”·å¥³å¹³æƒï½œæ—¥æœ¬ï½œæ¢…å¤§é«˜é€Ÿï½œèˆ†è®ºæ§åˆ¶ï½œç‹å±€æ‹æ¡ˆ20240507 (https://www.youtube.com/watch?v=39Gq_eOPuDY&t=1s) 3731 comments from 11/04/2025\nSIMP003 - è€æ¢ï¼šç»™â€œèƒ–çŒ«â€å¤šæ¡é€‰æ‹© é‡åº†â€œèƒ–çŒ«äº‹ä»¶â€ä¸æ˜¯æ€§åˆ«å¤§æˆ˜ å¦‚ä½•é¿å…æˆä¸ºâ€œèƒ–çŒ«â€(https://www.youtube.com/watch?v=mjcgg0wFpfE) 997 comments from 11/04/2025\nSIMP004 - å°ä¼™ç‚ºæ„›è·³æ±Ÿï¼Œæ‹œé‡‘çš„å¥³å‹ï¼Œå¸è¡€çš„è¦ªå§ï¼Œç„¡è‰¯çš„å•†å®¶ï¼Œç˜‹ç‹‚çš„ç¶²æ°‘ï¼Œèª°æ‰æ˜¯åŠ å®³è€…ï¼Ÿç‚ºä½•è­¦å¯Ÿèªå®šå¥³å‹ç„¡ç½ªï¼Œåè€Œæ˜¯è¦ªå§é•äº†æ³•ï¼Ÿä¸€å£æ°£çœ‹å®Œèƒ–è²“äº‹ä»¶å§‹æœ«ï¼| Wayneèª¿æŸ¥(https://www.youtube.com/watch?v=igs7GoIU4MU) 615 comments from 11/04/2025\nSIMP005 - ç¥ç´šé™ªç©ã€Œèƒ–è²“ã€é­è©ä¹¾227è¬äº¡ å¥³å‹é“æ­‰ï½œ20240506 ETåˆé–“æ–°è (https://www.youtube.com/watch?v=tAE83zZEcOY) 402 comments from 11/04/2025\nSIMP006 - è¢«æ’ˆå¥³é¨™å…‰50è¬ï¼ŒéŠæˆ²å®…ç”·è·³æ±Ÿè‡ªæ®ºï¼Œè½Ÿå‹•å…¨ç¶²ï¼æ’ˆå¥³è­šç«¹æ¦¨ä¹¾èƒ–è²“äº‹ä»¶çœŸç›¸ï¼ã€æ–°é—»æœ€å˜²ç‚¹ å§œå…‰å®‡ã€2024.0508(https://www.youtube.com/watch?v=YYngd2Yt3zk) 271 comments from 11/04/2025\n\n\nlibrary(plyr)\n\nWarning: package 'plyr' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.4.3\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(purrr)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:plyr':\n\n    compact\n\nlibrary(chromote)\n\nWarning: package 'chromote' was built under R version 4.4.3\n\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tibble)\nlibrary(nnet)\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.4.3\n\n\ncorrplot 0.95 loaded\n\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.4.3\n\n\nLoading required package: NLP\n\n\nWarning: package 'NLP' was built under R version 4.4.2\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.4.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.4.3\n\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.4.3\n\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.4.3\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(\"quanteda.textplots\")\n\nWarning: package 'quanteda.textplots' was built under R version 4.4.3\n\nlibrary(httr)\n\nWarning: package 'httr' was built under R version 4.4.3\n\n\n\nAttaching package: 'httr'\n\n\nThe following object is masked from 'package:NLP':\n\n    content\n\nlibrary(RColorBrewer)\nlibrary(RedditExtractoR)\n\nWarning: package 'RedditExtractoR' was built under R version 4.4.3\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.4.3\n\nlibrary(tidyr)\n\n\ndata1 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\ndata2 &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments.csv\")\ndata3 &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments.csv\")\ndata4 &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments.csv\")\ndata5 &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments.csv\")\ndata6 &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments.csv\")"
  },
  {
    "objectID": "758/Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "href": "758/Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "title": "DACSS785_Final_Project",
    "section": "Data Cleaning (info for the poster",
    "text": "Data Cleaning (info for the poster\nALSO GUIDE IT TO THE PART WHICH I WANT PLUS PRESENT THE CLEAN FORMAT IN THE POSTER\nThe raw dataset, collected as several CSV files, initially contained detailed comment metadata. The original structure included columns such as: videoId, commentId, parentId, author, text, likeCount, publishedAt, updatedAt, viewerRating, canRate, and reply.\nFor data cleaning, all CSV files in the Final_project_data folder were systematically processed using the R environment. The initial step was to streamline the dataset by retaining only the essential variables for textual and engagement analysis: text, likeCount, and reply.\nI utilized the R packages dplyr and stringr to focus on standardizing the text column. This involved a series of cleaning operations: normalization of whitespace (removing line breaks, tabs, and extra spaces, and trimming leading/trailing whitespace) and character filtering. Crucially, I removed non-essential symbols and unusual characters while meticulously preserving all Chinese characters to ensure the comments remained culturally authentic and meaningful for subsequent analysis.\nFinally, each cleaned and standardized dataset was saved as a new CSV file, appended with the suffix _cleaned. UTF-8 encoding was explicitly used to guarantee the accurate representation of the Chinese characters. This systematic workflow ensures the comment data are tidy, standardized, and immediately ready for downstream procedures, such as tokenization and sentiment or frequency analysis.\n\nsource(\"data_cleaning_CN.R\")\n\ndata cleaning complete!.\n\nSIMP001 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\n\n#Present the eample of the result\nhead(SIMP001)\n\n      videoId                  commentId parentId        author\n1 o5TfkwlthWU UgyekRC230MDXREkdeN4AaABAg     &lt;NA&gt;  @DanjonMeshi\n2 o5TfkwlthWU UgxangSP0zjJm6_gHfV4AaABAg     &lt;NA&gt;  @paullee4451\n3 o5TfkwlthWU UgwZdLtl6Eb2wgDWaDV4AaABAg     &lt;NA&gt;      @urikora\n4 o5TfkwlthWU UgwSmeqUyHYUXGD6l3l4AaABAg     &lt;NA&gt; @fayechen1928\n5 o5TfkwlthWU UgwpotCAmJmn2wWU7u54AaABAg     &lt;NA&gt; @running_goat\n6 o5TfkwlthWU UgxQSyQbnLT9r7I1faB4AaABAg     &lt;NA&gt;  @Jack2006103\n                                                                                                                       text\n1                                                             å•†å®¶é›†é«”çµ¦ç©ºè¢‹çœŸçš„ç¬‘æ­»ï¼Œä¸æ­¢ç”Ÿæ´»åœ¨ä¸­åœ‹ï¼Œé€£æ­»åœ¨ä¸­åœ‹éƒ½è¦å·è‘—æ¨‚ğŸ˜†\n2                                                                                                                      é ­é¦™\n3                       æ›´æ…˜çš„æ˜¯ï¼Œäººéƒ½èµ°äº†ä¸€å€‹æœˆ çµæœå°±åœ¨é€™æ™‚æ©Ÿé»è¢«æŠ“ä¾†æ“‹æ”¿åºœåšçš„é†œäº‹ï¼ˆè·¯å´©è¯ç‚ºè»Šè¡é€²å»å‘æ´ç„¶å¾Œå¿«é€Ÿç‡ƒèµ·ä¾†ï¼‰\n4                                                                                      æ¯æ¬¡çœ‹åˆ°ä¸­åœ‹é€™ç¨®æ‚²åŠ‡éƒ½è¦ºå¾—ä¸å¯æ€è­°ğŸ˜¨ğŸ˜­\n5 æˆ‘çœŸçš„å¿…é ˆå¾—èªªå°å²¸ç”·å¥³æˆ°çˆ­çœŸçš„è¶Šä¾†è¶Šåš´é‡== å•éå¥½å¹¾å€‹å°å²¸çš„å¥³ç”Ÿéƒ½èªç‚ºç”·ç”Ÿå°±æ˜¯æ‡‰è©²è¦çµ¦å½©ç¦® åƒé£¯å°±æ˜¯è¦å¹«å¥³ç”Ÿä»˜éŒ¢ç­‰ç­‰ å¾ˆå¯æ€•\n6                                                                                                é€™äº›åº—å®¶åƒäººè¡€é¥…é ­ï¼Œè¶…å™çˆ›\n  likeCount          publishedAt            updatedAt viewerRating canRate\n1        58 2024-05-09T16:01:52Z 2024-05-09T16:01:52Z         none    TRUE\n2         0 2024-05-09T16:02:23Z 2024-05-09T16:02:23Z         none    TRUE\n3       345 2024-05-09T16:05:05Z 2024-05-09T16:05:05Z         none    TRUE\n4         1 2024-05-09T16:05:57Z 2024-05-09T16:05:57Z         none    TRUE\n5       139 2024-05-09T16:06:47Z 2024-05-09T16:06:47Z         none    TRUE\n6         4 2024-05-09T16:07:29Z 2024-05-09T16:07:29Z         none    TRUE\n  reply\n1 FALSE\n2 FALSE\n3 FALSE\n4 FALSE\n5 FALSE\n6 FALSE\n\ndata1_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_cleaned.csv\")\ndata2_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments_cleaned.csv\")\ndata3_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments_cleaned.csv\")\ndata4_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments_cleaned.csv\")\ndata5_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments_cleaned.csv\")\ndata6_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments_cleaned.csv\")"
  },
  {
    "objectID": "758/Final_check-in_2.html#preprocess-the-data",
    "href": "758/Final_check-in_2.html#preprocess-the-data",
    "title": "DACSS785_Final_Project",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nFor visualizing the dominant linguistic patterns within the comment data, I employed two complementary approaches. First, a Word Cloud visualization (generated using the Word_cloud_visualization.R script) provided an intuitive, qualitative representation of high-frequency words, instantly highlighting the most common terms associated with discussions of â€œSIMPâ€ behavior.\nSecond, I conducted a quantitative rank-frequency analysis by applying Zipfâ€™s Law to the word corpus. After arranging all unique words by descending frequency and assigning a rank, I plotted the resulting distribution using the ggplot2 package. The resulting visualization confirmed that the comment discourse adheres to a Zipfian distribution, where a few words account for a disproportionate share of the total vocabulary.\nThe key terms driving the discourse were clearly identifiable:\nThese visualizations collectively offer both quantitative validation (Zipfâ€™s Law distribution) and qualitative insight (Word Cloud/Top Terms) into how the audience discusses and perceives the central event and the related concept of â€œSIMPâ€ behavior in this context. The high frequency of questioning and uncertainty (ç‚ºä»€éº¼, ä¸çŸ¥é“, æ˜¯ä¸æ˜¯) coupled with terms of exploitation (pua) and suffering (å—å®³è€…) reveals a key focus on moral judgment and accountability in the discussion.\n\nsource(\"TOKENIZATION.R\")\n\nTokenization complete!\n\nSIMP001_comments_tokens &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_tokens.csv\")\n#Present the eample of the result\nhead(SIMP001_comments_tokens)\n\n  likeCount reply     word\n1       345 FALSE   è¡é€²å»\n2         1 FALSE ä¸å¯æ€è­°\n3       139 FALSE   è¶Šä¾†è¶Š\n4       139 FALSE   å¥½å¹¾å€‹\n5       141 FALSE   éº¥ç•¶å‹\n6         3 FALSE   æœ‰äººèªª\n\n\n\nsource(\"Word_frequency.R\")\n\nWord Frequency Calculation Complete!\n\nSIMP001_wordfreq &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_wordfreq.csv\")\n\n#Present the eample of the result\nhead(SIMP001_wordfreq)\n\n    word  n rank\n1 å—å®³è€… 75    1\n2 ç‚ºä»€éº¼ 67    2\n3 ä¸çŸ¥é“ 53    3\n4    pua 42    4\n5 é€™ä»¶äº‹ 34    5\n6 ä¸€å€‹äºº 32    6\n\n\n\nsource(\"Same_Word.R\")\n\nCommon words saved to: Final_project_data/common_words_across_files.csv \n\ncommon_words &lt;- read.csv(\"Final_project_data/common_words_across_files.csv\")\n\n#Present the eample of the result\nhead(common_words)\n\n    word total_count\n1 ä¸çŸ¥é“         172\n2 ä¸ºä»€ä¹ˆ         154\n3 å—å®³è€…         118\n4    pua         115\n5 æ˜¯ä¸æ˜¯         101\n6 ç‚ºä»€éº¼         100\n\n\n\nsource(\"Change_to_Traditional_Chinese.R\")\n\nWarning: package 'textstem' was built under R version 4.4.3\n\n\nLoading required package: koRpus.lang.en\n\n\nWarning: package 'koRpus.lang.en' was built under R version 4.4.3\n\n\nLoading required package: koRpus\n\n\nWarning: package 'koRpus' was built under R version 4.4.3\n\n\nLoading required package: sylly\n\n\nWarning: package 'sylly' was built under R version 4.4.3\n\n\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\n\nAttaching package: 'koRpus'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    tokens, types\n\n\nThe following object is masked from 'package:tm':\n\n    readTagged\n\n\nThe following object is masked from 'package:readr':\n\n    tokenize\n\n\nWarning: package 'tmcn' was built under R version 4.4.3\n\n\n# tmcn Version: 0.2-13\n\n\n[1] \"girl\"    \"woman\"   \"simping\" \"lover\"  \nTranslation complete! Output saved to 'your_output_file.csv'\n\nTraditional_Chinese_data_cleaned &lt;- read.csv(\"Final_project_data/traditional_common_words_combined.csv\")\n\n#Present the data after cleaning\nhead(Traditional_Chinese_data_cleaned)\n\n  traditional_text total_count\n1           ç‚ºä»€éº¼         254\n2           ä¸çŸ¥é“         172\n3           å—å®³è€…         118\n4              pua         116\n5           æ˜¯ä¸æ˜¯         101\n6           å¥³æœ‹å‹          95"
  },
  {
    "objectID": "758/Final_check-in_2.html#visualization",
    "href": "758/Final_check-in_2.html#visualization",
    "title": "DACSS785_Final_Project",
    "section": "Visualization",
    "text": "Visualization\nFor visualizing patterns in the comments, I used two approaches. First, the Word_cloud_visualization.R script generated word clouds to highlight high-frequency words, providing a clear and intuitive view of the most common terms associated with discussions of â€œSIMPâ€ behavior. Second, I applied Zipfâ€™s Law to examine the relationship between word rank and frequency. After arranging words by descending frequency and assigning ranks, I plotted all words using ggplot2, labeling only the top five most frequent words to emphasize the key terms in the discourse. The resulting visualizations offer both quantitative and qualitative insight into how people discuss and perceive â€œSIMPâ€ behavior in YouTube comments.\n\nsource(\"Word_cloud_visualization.R\")\n\n\n\n\n\n\n\n\nWord Cloud generated for: traditional_common_words_combined.csv\n\n\n\n# Sort by frequency and assign ranks\nzipf_data_ranked &lt;- Traditional_Chinese_data_cleaned %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  mutate(rank = row_number())\n\n# Print the top 5 ranked words to confirm the data structure\nprint(head(zipf_data_ranked, 5))\n\n  traditional_text total_count rank\n1           ç‚ºä»€éº¼         254    1\n2           ä¸çŸ¥é“         172    2\n3           å—å®³è€…         118    3\n4              pua         116    4\n5           æ˜¯ä¸æ˜¯         101    5\n\n# --- Linear Scale (As Requested) ---\n\nggplot(zipf_data_ranked, aes(x = rank, y = total_count)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"darkorange\", size = 1.5) +\n  geom_text(\n    # Label the top 8 words\n    aes(label = ifelse(rank &lt;= 6, traditional_text, \"\")),\n    vjust = -0.8,\n    size = 3.5,\n    check_overlap = TRUE # Prevents overlapping labels\n  ) +\n  labs(\n    title = \"Zipfâ€™s Law: Word Rank vs Frequency\",\n    x = \"Rank of Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nTranslation\n\n\n\nRank\nWord\nTranslate\n\n\n1\nç‚ºä»€éº¼\nâ€œWhy / Why is it thatâ€¦â€\n\n\n2\nä¸çŸ¥é“\nâ€œDonâ€™t knowâ€\n\n\n3\nå—å®³è€…\nâ€œVictimâ€\n\n\n4\npua\nâ€œPUAâ€\n\n\n5\næ˜¯ä¸æ˜¯\nâ€œIs it / Is it not?â€\n\n\n6\nå¥³æœ‹å‹\nâ€œGirlfriendâ€"
  },
  {
    "objectID": "758/Final_check-in_2.html#word-embedding",
    "href": "758/Final_check-in_2.html#word-embedding",
    "title": "DACSS785_Final_Project",
    "section": "Word Embedding",
    "text": "Word Embedding\nFor semantic analysis, I applied Word2Vec using a pseudo-document approach to capture relationships between words in the comments. Each word was repeated according to its frequency (total_count) to create co-occurrence information, which is essential for small datasets where natural co-occurrences are limited. The repeated words were then combined into a single space-separated pseudo-document and used to train a skip-gram Word2Vec model with a vector dimension of 50, window size of 5, and 50 iterations, setting min_count = 1 to include all words.\nThe resulting word vectors allow calculation of cosine similarity to examine semantic relationships between words, as well as clustering and downstream supervised learning tasks. For example, the vector for a keyword such as â€œç‚ºä»€éº¼â€ can be compared with all other word vectors to identify the top semantically similar words, revealing patterns in how concepts related to â€œSIMPâ€ behavior are discussed in YouTube comments. This approach provides a robust representation of word meaning in the context of the dataset while accommodating the limited co-occurrence information inherent in smaller comment datasets.\n\n# Word2Vec can be the best option for the word embeding.\n\nsource(\"Word Embeddings.R\")\n\nWarning: package 'word2vec' was built under R version 4.4.3\n\n\nWarning: package 'text2vec' was built under R version 4.4.3"
  },
  {
    "objectID": "758/Final_check-in_2.html#sentiment-analysis",
    "href": "758/Final_check-in_2.html#sentiment-analysis",
    "title": "DACSS785_Final_Project",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nFor sentiment analysis, I applied a custom Chinese sentiment dictionary tailored to the context of â€œSIMPâ€ behavior. The dictionary categorizes words into three groups: positive (supportive or relationship-related words such as â€œå¥³æœ‹å‹â€ and â€œé—œå¿ƒâ€), negative (critical or unfairness-related words such as â€œä¸å€¼å¾—â€ and â€œä¸å…¬å¹³â€), and behavior (attention-seeking or â€œsimpâ€ behavior words such as â€œpuaâ€ and â€œè¿½æ±‚â€). Using R, I computed sentiment scores for each word in the dataset by summing occurrences in these categories. A raw polarity score was calculated as the sum of positive and behavior counts minus negative counts, then normalized by the total occurrences of all dictionary words to produce a relative polarity measure.\nThe analysis revealed that the current positive and negative categories do not fully capture the sentiment expressed in the comments. Some words were misclassified or contextually ambiguous, highlighting that the dictionary needs further adjustment and refinement to improve accuracy. Polarity distributions were visualized using a histogram, providing an overview of how positive, negative, and behavior-related language appears in discussions of â€œSIMPâ€ behavior. This approach provides a preliminary sentiment assessment while acknowledging the limitations of the existing dictionary.\n\nsource(\"Sentiment Analysis.R\")\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” forcats   1.0.0     âœ” lubridate 1.9.4\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– NLP::annotate()         masks ggplot2::annotate()\nâœ– httr::content()         masks NLP::content()\nâœ– dplyr::filter()         masks stats::filter()\nâœ– jsonlite::flatten()     masks purrr::flatten()\nâœ– rvest::guess_encoding() masks readr::guess_encoding()\nâœ– dplyr::lag()            masks stats::lag()\nâœ– koRpus::tokenize()      masks readr::tokenize()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'quanteda.textmodels' was built under R version 4.4.3\n\n\nWarning: package 'stopwords' was built under R version 4.4.3\n\n\n\nAttaching package: 'stopwords'\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nWarning: package 'caret' was built under R version 4.4.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:httr':\n\n    progress\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n[1] \"DFM after Custom Simp Dictionary Lookup (Counts per category):\"\n\n### SUPERVISED LEARNING (Naive Bayes Classification)\n[1] \"Confusion Matrix:\"\n\n### LEXICON-BASED NTUSD SENTIMENT RESULTS\n[1] \"Mean Sentiment Score by Simping Label (NTUSD):\"\n# A tibble: 2 Ã— 2\n  contains_simp_factor mean_sentiment\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 FALSE                       -0.0990\n2 TRUE                        -0.477 \n\n\n\n\n\n\n\n\n\nIn this graph:\nX-Axis: Simping Label:\n\nFALSE: Comments that do not contain any of the words from your custom â€œsimpâ€ dictionary (e.g., â€œèˆ”ç‹—,â€ â€œå·¥å…·äºº,â€ â€œä¸€å»‚æƒ…é¡˜,â€ etc.).\nTRUE: Comments that do contain at least one word from your custom â€œsimpâ€ dictionary.\n\nY-Axis: Comment Sentiment Score:\n\nPositive Scores (above 0): Indicate a more positive overall sentiment.\nZero (0): Indicates a neutral or balanced sentiment.\nNegative Scores (below 0): Indicate a more negative overall sentiment."
  },
  {
    "objectID": "758/Final_check-in_2.html#sentiment-analysis-summary",
    "href": "758/Final_check-in_2.html#sentiment-analysis-summary",
    "title": "DACSS785_Final_Project",
    "section": "Sentiment Analysis Summary",
    "text": "Sentiment Analysis Summary\nThe lexicon-based sentiment analysis, utilizing the NTUSD dictionary, reveals a pronounced negative emotional shift in texts discussing the â€œsimpâ€ phenomenon. Specifically, content that contains terms from the custom dictionaryâ€”which targets themes like â€œsimp behaviorâ€ (e.g., èˆ”ç‹—, simp), â€œvictim positionâ€ (e.g., å—å®³è€…, pua), and â€œrelationship imbalanceâ€â€”shows a highly negative mean sentiment score of -0.477. This score is significantly more negative than the average score of -0.0990 found in texts that do not contain these specific terms. This sharp difference (a nearly five-fold increase in negative sentiment magnitude) indicates that conversations about excessive one-sided effort, perceived exploitation, and unequal relationshipsâ€”the core of the â€œsimpâ€ conceptâ€”are strongly associated with negative emotional discourse within the corpus."
  },
  {
    "objectID": "758/Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "href": "758/Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "title": "DACSS785_Final_Project",
    "section": "Supervised Learning Analysis (Naive Bayes Classification)",
    "text": "Supervised Learning Analysis (Naive Bayes Classification)\nThe Naive Bayes model was employed to classify comments based on whether they contained the â€œsimpâ€ factor, using a cleaned feature set that excluded all words from the custom â€œsimpâ€ dictionary to prevent data leakage. The model achieved an overall Accuracy of 81.69%, which is slightly higher than the No Information Rate (NIR) of 80.36%, indicating its performance is marginally better than random guessing based on class prevalence.\nHowever, a closer look at the results reveals a significant class imbalance issue and skewed performance:\n\nHigh Sensitivity (Recall): The model is excellent at correctly identifying comments that do NOT contain the simp factor (the majority class, FALSE), with a high Sensitivity of 95.72%.\nLow Specificity: Conversely, the model is very poor at correctly identifying comments that DO contain the simp factor (the minority class, TRUE), with a low Specificity of 24.29%.\nKappa Value: The Kappa statistic of 0.2565 suggests only a fair level of agreement beyond chance.\n\nIn summary, the high overall accuracy is largely driven by correctly classifying the prevalent negative class (FALSE). The model struggles to reliably identify actual â€œsimpâ€ comments (TRUE), suggesting that the remaining general vocabulary in the comments lacks sufficient predictive power to consistently distinguish between the two categories without the core dictionary terms.\n\nsource(\"Supervised_Learning.R\")\n\n\n--- Solving Data Leakage: Remove the word which exist in Simp dictionary ---\n[1] \"Original (matrix_main): 2703\"\n[1] \"Remove the word in Simp dictionary (X_cleaned): 2674\"\n\n--- 6. Naive Bayes Training ---\n\n--- Naive Bayes Prediction ---\n[1] \"Confusion Matrix:\"\nConfusion Matrix and Statistics\n\n                 y_test\npredicted_cleaned FALSE TRUE\n            FALSE   693  134\n            TRUE     31   43\n                                        \n               Accuracy : 0.8169        \n                 95% CI : (0.79, 0.8416)\n    No Information Rate : 0.8036        \n    P-Value [Acc &gt; NIR] : 0.1676        \n                                        \n                  Kappa : 0.2565        \n                                        \n Mcnemar's Test P-Value : 2.011e-15     \n                                        \n            Sensitivity : 0.9572        \n            Specificity : 0.2429        \n         Pos Pred Value : 0.8380        \n         Neg Pred Value : 0.5811        \n              Precision : 0.8380        \n                 Recall : 0.9572        \n                     F1 : 0.8936        \n             Prevalence : 0.8036        \n         Detection Rate : 0.7691        \n   Detection Prevalence : 0.9179        \n      Balanced Accuracy : 0.6001        \n                                        \n       'Positive' Class : FALSE"
  },
  {
    "objectID": "758/Final_check-in_2.html#topic-modeling-lda",
    "href": "758/Final_check-in_2.html#topic-modeling-lda",
    "title": "DACSS785_Final_Project",
    "section": "Topic Modeling (LDA)",
    "text": "Topic Modeling (LDA)\nThe script follows a standard text mining workflow using the tidyverse and text2vec packages:\n\nData Preparation: It reads the individual tokenized comment files, reconstructs the full comments by assigning and aggregating tokens by a unique document ID (doc_id), and then combines the tokens back into complete text strings.\nFeature Engineering: It creates an iterator from the aggregated text and builds a vocabulary. Crucially, it prunes the vocabulary by removing words that occur less than three times (term_count_min = 3), which helps reduce noise and improves the quality of the derived topics.\nDTM Creation: The processed tokens are converted into a Document-Term Matrix (DTM), which is the input required for LDA.\nModel Training: The script initializes and trains an LDA model with a predefined number of topics (K=8) and 500 iterations.\nOutput: Finally, the code extracts and saves two key results: the Topic-Word distribution (the top 10 most characteristic words for each of the 8 topics) and the Document-Topic distribution (the probability that each comment belongs to each topic), storing both as CSV files for subsequent qualitative analysis.\n\n\nsource(\"Topic_Model.R\")\n\n[1] \"Starting LDA Topic Modeling with K = 8\"\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======================================================================| 100%\n[1] \"LDA Training Complete.\"\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nâ„¹ Using compatibility `.name_repair`.\n\n\n[1] \"Top 10 Words for Each Topic:\"\n# A tibble: 10 Ã— 9\n   Topic_Word_Rank Topic_0 Topic_1 Topic_2 Topic_3  Topic_4  Topic_5 Topic_6\n   &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1 Word_1          pua     ä¸çŸ¥é“  å—å®³è€…  æ˜¯ä¸æ˜¯   ç‚ºä»€éº¼   ä¸çŸ¥é“  è€Œä¸æ˜¯ \n 2 Word_2          ä¸ºä»€ä¹ˆ  ä¸‡ä½™å…ƒ  ä¸ºä»€ä¹ˆ  å¥³æœ‹å‹   åœ¨ä¸€èµ·   è¶Šæ¥è¶Š  é€™ä»¶äº‹ \n 3 Word_3          ä¸­å›½äºº  pua     pua     ä¸ºä»€ä¹ˆ   ä¹Ÿå¯ä»¥   å¤§éƒ¨åˆ†  ä¸çŸ¥é“ \n 4 Word_4          ç”·å­©å­  å¯èƒ½æ˜¯  çœŸçš„æ˜¯  ä¸çŸ¥é“   ä¸å¯èƒ½   éƒ½ä¸æ˜¯  ä¸€å®šæ˜¯ \n 5 Word_5          å¥³å­©å­  ä¸€å€‹äºº  éº¦å½“åŠ³  ä¸éœ€è¦   éƒ½æ²’æœ‰   çœŸçš„æ˜¯  å¥³æœ‹å‹ \n 6 Word_6          ä¹Ÿä¸æ˜¯  é€™ä»¶äº‹  å®¶åº­çš„  ä¹Ÿæ²¡æœ‰   é€™å°±æ˜¯   å—å®³è€…  èƒ½ä¸èƒ½ \n 7 Word_7          æ˜¯ä¸æ˜¯  ä¸­å›½äºº  å…¨ä¸–ç•Œ  å¤§éƒ¨åˆ†   ä¹Ÿæ²’æœ‰   å¥³æœ‹å‹  äººæ°‘å¹£ \n 8 Word_8          æ²¡ä»€ä¹ˆ  å°¤å…¶æ˜¯  å…¶å®æ˜¯  ç”·æœ‹å‹   ç”·å°Šå¥³å‘ æˆ‘çŸ¥é“  åƒåœ¾æ¡¶ \n 9 Word_9          å¯èƒ½æ˜¯  çœŸçš„æ˜¯  å¥½åƒæ˜¯  ç”·å¥³å¹³ç­‰ æ²’ä»€éº¼   æ˜¯ä¸æ˜¯  æœ‰äº›äºº \n10 Word_10         ä¸å€¼å¾—  ç‚ºä»€éº¼  è‚¯å®šæ˜¯  æ‰€è°“çš„   ä¸å­˜åœ¨   ä¹Ÿä¸æ˜¯  ä¸ºä»€ä¹ˆ \n# â„¹ 1 more variable: Topic_7 &lt;chr&gt;\n[1] \"Saved topic words to lda_topic_words.csv\"\n[1] \"Document-Topic Distribution (Head):\"\n# A tibble: 6 Ã— 9\n  doc_id    V1    V2    V3    V4    V5    V6    V7    V8\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1     0  0      0    0      0     0       0     0\n2      2     0  0      0    0      0     0       0     0\n3      3     0  0.05   0    0.15   0.6   0.2     0     0\n4      4     0  0.1    0.2  0      0.4   0.3     0     0\n5      5     0  0      0.8  0      0.2   0       0     0\n6      6     0  0      0    0      0     0       1     0\n[1] \"Saved document-topic distribution to lda_doc_topic_distr.csv\"\n\n\nTranslation for the every words in the topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic_Word_Rank\nTopic_0\nTopic_1\nTopic_2\nTopic_3\nTopic_4\nTopic_5\nTopic_6\nTopic_7\n\n\nWord_1\nPUA\nDonâ€™t know\nVictim\nIs it?\nWhy\nDonâ€™t know\nAnd not\nBuffet (Resource)\n\n\nWord_2\nWhy\nOver 10k yuan\nWhy\nGirlfriend\nBe together\nMore and more\nThis matter\nHighway\n\n\nWord_3\nChinese people\nPUA\nPUA\nWhy\nAlso can\nMajority\nDonâ€™t know\nToo good\n\n\nWord_4\nBoy / Male\nPossibly\nTruly is\nDonâ€™t know\nImpossible\nAre not all\nMust be\nWhy\n\n\nWord_5\nGirl / Female\nA person\nMcDonaldâ€™s\nDonâ€™t need\nDonâ€™t have at all\nTruly is\nGirlfriend\nMainly is\n\n\nWord_6\nAlso is not\nThis matter\nFamilyâ€™s\nAlso donâ€™t have\nThis is\nVictim\nCan or cannot\nSome people\n\n\nWord_7\nIs it?\nChinese people\nWhole world\nMajority\nAlso donâ€™t have\nGirlfriend\nRMB (Money)\nInequality\n\n\nWord_8\nNothing much\nEspecially\nActually is\nBoyfriend\nMale Superiority\nI know\nTrash Can (Worthless)\nShould be\n\n\nWord_9\nPossibly\nTruly is\nSeems like\nGender Equality\nNothing much\nIs it?\nSome people\nA person\n\n\nWord_10\nNot worth it\nWhy\nDefinitely is\nSo-called\nDoes not exist\nAlso is not\nWhy\nGender Equality\n\n\n\nThe interpretation for each topic\n\n\n\n\n\n\n\n\nTopic\nCore Keywords & Interpretation\nSuggested Topic Label\n\n\nTopic 0\nThis topic strongly links the PUA phenomenon with discussions about specific gender roles and identities within the Chinese context. The presence of â€œNot worth itâ€ suggests this cluster is focused on evaluating the value of actions/relationships under the PUA framework.\nPUA & Gender Dynamics in China\n\n\nTopic 1\nThis topic mixes uncertainty and specific financial figures (Over 10k yuan), directly linked to PUA. It suggests discussions about high-stakes financial loss or investment by an individual in a relationship where the outcome or reality is unclear.\nFinancial Dimension of PUA & Uncertainty\n\n\nTopic 2\nThe simultaneous presence of â€œVictim,â€ â€œPUA,â€ and â€œTruly isâ€ indicates a core discussion cluster dedicated to validating the existence and reality of being exploited. â€œMcDonaldâ€™sâ€ implies cheap/casual provision, while â€œFamilyâ€™sâ€ suggests the conversation may touch on the origins or impact of these dynamics within a family unit.\nValidating Victimhood & Low-Cost Exploitation\n\n\nTopic 3\nThis topic is saturated with questioning terms (â€œIs it?â€, â€œWhy?â€, â€œDonâ€™t knowâ€), applied directly to boyfriend/girlfriend roles and the concept of gender equality. It represents a pervasive atmosphere of skepticism and critical discussion about expected behavior in modern relationships.\nSkepticism & Questioning of Relationship Roles\n\n\nTopic 4\nA highly polarized topic that denies (ä¸å¯èƒ½, ä¸å­˜åœ¨) the relevance or existence of Male Superiority (ç”·å°Šå¥³å‘). It focuses on the possibility of being together (åœ¨ä¸€èµ·), suggesting a desire for modern, equal partnerships and a strong rejection of patriarchal norms.\nDenial of Traditional Patriarchy in Relationships\n\n\nTopic 5\nTerms like â€œMore and moreâ€ and â€œMajorityâ€ point to a discussion of social trends and scale. When combined with â€œVictimâ€ and â€œGirlfriend,â€ it indicates a conversation about whether victimhood is becoming increasingly common or if the perception of victimhood is changing within the female partner role.\nDiscussing Shifting Social Norms & Victim Pool\n\n\nTopic 6\nThis is the most explicitly transactional topic. It discusses financial payment (RMB) and the concept of a person being reduced to a â€œTrash Canâ€ (worthless/emotional dumping ground). The use of â€œAnd notâ€ suggests a debate over what a relationship should be versus what it currently is (i.e., not a transaction, but one of money/exploitation).\nMonetary Value vs.Â Emotional Worth (The Price of Simping)\n\n\nTopic 7\nThis topic links resource provision (implied by â€œBuffetâ€ and â€œHighway,â€ often used as metaphors for free/easy access) with discussions of Inequality and Gender Equality. It debates whether resources should be provided freely, who is responsible for providing them, and the resulting fairness in the relationship structure.\nResource Provision & Equality Debate\n\n\n\n\n\n\nDTM\nTopic_Word_Rank\n\n\nV1\nTopic_0\n\n\nV2\nTopic_1\n\n\nV3\nTopic_2\n\n\nV4\nTopic_3\n\n\nV5\nTopic_4\n\n\nV6\nTopic_5\n\n\nV7\nTopic_6\n\n\nV8\nTopic_7"
  },
  {
    "objectID": "758/Final_check-in_2.html#causal-inference",
    "href": "758/Final_check-in_2.html#causal-inference",
    "title": "DACSS785_Final_Project",
    "section": "Causal Inference",
    "text": "Causal Inference\nIn the casual inference part, I present the Ordinary Least Squares (OLS) regression model to analyze how the probability of eight LDA topics influences the number of likes received by a comment (likeCount). Topic V8 (PUA/Victim) was set as the reference group (Reference Topic) in the model. Overall, the modelâ€™s explanatory power is extremely low (\\(\\text{Adjusted R-squared} = 0.00025\\)), suggesting that the variation in likeCount is primarily influenced by factors outside the model, rather than the topics themselves. However, the coefficient tests for individual topics revealed that Topic V2 demonstrated a statistically significant positive influence. After controlling for the effects of other topics, an increase of 1 unit in the probability of Topic V2 (Financial/Money II), relative to the reference group V8, is expected to increase the number of likes by approximately 9.66 (\\(p = 0.038^{*}\\)). This suggests that specific discussion content related to money or finance is more likely to garner attention and agreement within the community. Apart from the intercept, the remaining topics (V1, V3, V4, V5, V6, and V7) did not show a statistically significant relationship with the number of likes.\n\nsource(\"Causal_Inference.R\")\n\nRows: 3006 Columns: 9\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (9): doc_id, V1, V2, V3, V4, V5, V6, V7, V8\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n[1] \"--- OLS Regression Results (Outcome: likeCount) ---\"\n[1] \"Reference Topic: V8 (PUA/Victim)\"\n\nCall:\nlm(formula = likeCount ~ V1 + V2 + V3 + V4 + V5 + V6 + V7, data = merged_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -18.59   -8.93   -6.76   -5.12 1763.41 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.934      2.301   3.882 0.000106 ***\nV1            -2.424      4.144  -0.585 0.558600    \nV2             9.655      4.656   2.074 0.038172 *  \nV3            -2.701      4.312  -0.626 0.531107    \nV4            -2.380      4.266  -0.558 0.576978    \nV5            -2.815      4.399  -0.640 0.522298    \nV6             1.729      4.351   0.398 0.691027    \nV7            -0.953      4.363  -0.218 0.827099    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.08 on 2998 degrees of freedom\nMultiple R-squared:  0.002579,  Adjusted R-squared:  0.00025 \nF-statistic: 1.107 on 7 and 2998 DF,  p-value: 0.3553"
  },
  {
    "objectID": "758/Final_check-in_2.html#conclusion",
    "href": "758/Final_check-in_2.html#conclusion",
    "title": "DACSS785_Final_Project",
    "section": "Conclusion",
    "text": "Conclusion\n(PUT THESE TWO POINT INTO THE FUTURE WORK ALSO NEED TO EXPLAIN ABOUT DIFFICULTY TO COLLECT THE ACADEMIC DATA THERE ** SIMP DOES NOT HAVE A FORMAL DEFINITION**\nâ€œSynthesizing the projectâ€™s findings, the primary discovery is that community engagement (\\(\\text{likeCount}\\)) on YouTube comments is not driven by broad emotional tone or general topics, but rather by a specific, critical narrative focused on â€˜financial exploitation and victimhoodâ€™ (\\(\\text{Topic}\\) \\(\\text{V2}\\)). This specific form of critical discussion related to â€˜SIMPâ€™ behavior is extremely negative in sentiment (\\(\\text{mean}\\) \\(\\text{sentiment} = -0.477\\)) and is so unique in its linguistic pattern that its occurrence can be accurately predicted by the supervised learning model. Consequently, the communityâ€™s response to â€˜SIMPâ€™ behavior is highly concentrated and emotionally charged, with its online visibility predominantly stemming from comments that link the behavior directly to concrete financial inequality and victim scenarios.â€"
  },
  {
    "objectID": "758/Final_check-in_2.html#future",
    "href": "758/Final_check-in_2.html#future",
    "title": "DACSS785_Final_Project",
    "section": "Future",
    "text": "Future\nFuture work should prioritize addressing the observed limitations in both the supervised classification model and the initial data preprocessing pipeline to enhance the robustness and explanatory power of the analysis. Firstly, while the initial Naive Bayes classifier provided baseline insights, its predictive performance should be critically re-evaluated. Improving the accuracy of the automated simp classification label requires exploring more sophisticated machine learning techniques, such as Support Vector Machines (SVMs), Gradient Boosting, or even Transformer-based deep learning models. Concurrently, enhancing the feature set by refining or expanding the custom dictionariesâ€”perhaps incorporating sentiment scores or incorporating word embeddingsâ€”could significantly boost the modelâ€™s ability to discriminate between classes, moving beyond simple bag-of-words approaches. Secondly, a crucial area for improvement lies in the token filtering and data processing stage. Despite standard removal procedures, the presence of numerous contextually irrelevant tokens, such as specific objects (â€œé«˜é€Ÿå…¬è·¯â€) and brands (â€œéº¥ç•¶å‹â€), confirms the necessity of a more rigorous, domain-specific cleanup. Future efforts must focus on constructing an expanded, domain-aware stop word list or implementing Named Entity Recognition (NER) to systematically identify and remove non-topical, low-information tokens, ensuring the remaining features are highly predictive and representative of the core concepts being discussed."
  },
  {
    "objectID": "758/Final_check-in_2.html#reference",
    "href": "758/Final_check-in_2.html#reference",
    "title": "DACSS785_Final_Project",
    "section": "Reference",
    "text": "Reference\nHO, Daniel. The (simp)le truth about excessive & obsessive romantic behaviors in men. (2023). https://ink.library.smu.edu.sg/etd_coll/516\nKrishnamurthy, V., & Duan, Y. (2017). Dependence Structure Analysis Of Meta-level Metrics in YouTube Videos: A Vine Copula Approach. arXiv preprint arXiv:1712.10232. â€œto explain the comment and the view of the video are relatedâ€\nLun-Wei Ku and Hsin-Hsi Chen (2007). Mining Opinions from the Web: Beyond Relevance Retrieval. Journal of American Society for Information Science and Technology, Special Issue on Mining Web Resources for Enhancing Information Retrieval, 58(12), pages 1838-1850.\nPew Research Center. (2020). Many Americans get news on YouTube, where news organizations and independent producers thrive side by side. https://www.pewresearch.org/journalism/2020/09/28/many-americans-get-news-on-youtube-where-news-organizations-and-independent-producers-thrive-side-by-side/\nZhou, W. (2024). é‡åº†è­¦æ–¹å‘å¸ƒâ€œèƒ–çŒ«â€äº‹ä»¶è­¦æƒ…é€šæŠ¥ [Chongqing police issue incident report on the â€œPangmaoâ€ incident]. Xinhua Net. http://www.news.cn/politics/20240519/fb56352660c94810a58e79bc18459a3e/c.html"
  },
  {
    "objectID": "604/604_Final_check-in_2.html",
    "href": "604/604_Final_check-in_2.html",
    "title": "DACSS604_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "604/604_Final_check-in_2.html#research-question-and-hypothesis",
    "href": "604/604_Final_check-in_2.html#research-question-and-hypothesis",
    "title": "DACSS604_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "604/604_Final_check-in_2.html#data-collection",
    "href": "604/604_Final_check-in_2.html#data-collection",
    "title": "DACSS604_Final_Project",
    "section": "Data Collection",
    "text": "Data Collection\nTo explore the concept of â€œsimping,â€ I collected YouTube comments from six relevant videos for textual analysis. I utilized an R scraping script to extract approximately 8,000 comments in total. Following a cleaning and filtering process, a dataset of around 7,000 practical comments was retained for analysis.\nI specifically focused on the case study known as the â€œèƒ–è²“è·³æ±Ÿäº‹ä»¶â€ (Suicide of Fat Cat). This event, which occurred in Mainland China, provides a particularly rich and relevant dataset because it was a well-documented news story officially reported by the Chinese court. This official documentation makes it a real and verifiable event, distinguishing it from mere rumors or social media anecdotes. Furthermore, the use of Chinese-language videos as the reference source is critical, as the Chinese-speaking audience possesses extensive background knowledge and cultural context directly related to the local details of this incident.\n\nSIMP001 - é™ªæ‰“éŠæˆ²è³ºç™¾è¬é¤Šå¥³å‹æ…˜é­åˆ†æ‰‹ï¼ã€Œèƒ–è²“äº‹ä»¶ã€å¼•çˆ†ä¸­åœ‹æ€§åˆ¥æˆ°çˆ­ï¼Ÿã€Œæ’ˆå¥³ã€æ»¿è¡—è·‘çš„èƒŒå¾ŒåŸå› ï¼Ÿã€TODAY çœ‹ä¸–ç•Œã€‘(https://www.youtube.com/watch?v=o5TfkwlthWU&t=13s) 1952 comments from 11/04/2025\nSIMP002 - å½“èƒ–çŒ«é‡åˆ°æå¥³ï¼Œä¸€ä¸ªå¹´è½»äººå¦‚ä½•èµ°ä¸Šä¸å½’è·¯ï¼Ÿï½œå¥³æƒï½œæå¥³ï½œèƒ–çŒ«ï½œç‹è€…è£è€€ï½œç”·å¥³å¹³æƒï½œæ—¥æœ¬ï½œæ¢…å¤§é«˜é€Ÿï½œèˆ†è®ºæ§åˆ¶ï½œç‹å±€æ‹æ¡ˆ20240507 (https://www.youtube.com/watch?v=39Gq_eOPuDY&t=1s) 3731 comments from 11/04/2025\nSIMP003 - è€æ¢ï¼šç»™â€œèƒ–çŒ«â€å¤šæ¡é€‰æ‹© é‡åº†â€œèƒ–çŒ«äº‹ä»¶â€ä¸æ˜¯æ€§åˆ«å¤§æˆ˜ å¦‚ä½•é¿å…æˆä¸ºâ€œèƒ–çŒ«â€(https://www.youtube.com/watch?v=mjcgg0wFpfE) 997 comments from 11/04/2025\nSIMP004 - å°ä¼™ç‚ºæ„›è·³æ±Ÿï¼Œæ‹œé‡‘çš„å¥³å‹ï¼Œå¸è¡€çš„è¦ªå§ï¼Œç„¡è‰¯çš„å•†å®¶ï¼Œç˜‹ç‹‚çš„ç¶²æ°‘ï¼Œèª°æ‰æ˜¯åŠ å®³è€…ï¼Ÿç‚ºä½•è­¦å¯Ÿèªå®šå¥³å‹ç„¡ç½ªï¼Œåè€Œæ˜¯è¦ªå§é•äº†æ³•ï¼Ÿä¸€å£æ°£çœ‹å®Œèƒ–è²“äº‹ä»¶å§‹æœ«ï¼| Wayneèª¿æŸ¥(https://www.youtube.com/watch?v=igs7GoIU4MU) 615 comments from 11/04/2025\nSIMP005 - ç¥ç´šé™ªç©ã€Œèƒ–è²“ã€é­è©ä¹¾227è¬äº¡ å¥³å‹é“æ­‰ï½œ20240506 ETåˆé–“æ–°è (https://www.youtube.com/watch?v=tAE83zZEcOY) 402 comments from 11/04/2025\nSIMP006 - è¢«æ’ˆå¥³é¨™å…‰50è¬ï¼ŒéŠæˆ²å®…ç”·è·³æ±Ÿè‡ªæ®ºï¼Œè½Ÿå‹•å…¨ç¶²ï¼æ’ˆå¥³è­šç«¹æ¦¨ä¹¾èƒ–è²“äº‹ä»¶çœŸç›¸ï¼ã€æ–°é—»æœ€å˜²ç‚¹ å§œå…‰å®‡ã€2024.0508(https://www.youtube.com/watch?v=YYngd2Yt3zk) 271 comments from 11/04/2025\n\n\nlibrary(plyr)\n\nWarning: package 'plyr' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.4.3\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(purrr)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:plyr':\n\n    compact\n\nlibrary(chromote)\n\nWarning: package 'chromote' was built under R version 4.4.3\n\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tibble)\nlibrary(nnet)\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.4.3\n\n\ncorrplot 0.95 loaded\n\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.4.3\n\n\nLoading required package: NLP\n\n\nWarning: package 'NLP' was built under R version 4.4.2\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.4.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.4.3\n\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.4.3\n\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.4.3\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(\"quanteda.textplots\")\n\nWarning: package 'quanteda.textplots' was built under R version 4.4.3\n\nlibrary(httr)\n\nWarning: package 'httr' was built under R version 4.4.3\n\n\n\nAttaching package: 'httr'\n\n\nThe following object is masked from 'package:NLP':\n\n    content\n\nlibrary(RColorBrewer)\nlibrary(RedditExtractoR)\n\nWarning: package 'RedditExtractoR' was built under R version 4.4.3\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.4.3\n\nlibrary(tidyr)\n\n\ndata1 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\ndata2 &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments.csv\")\ndata3 &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments.csv\")\ndata4 &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments.csv\")\ndata5 &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments.csv\")\ndata6 &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments.csv\")"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "href": "604/604_Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "title": "DACSS604_Final_Project",
    "section": "Data Cleaning (info for the poster",
    "text": "Data Cleaning (info for the poster\nALSO GUIDE IT TO THE PART WHICH I WANT PLUS PRESENT THE CLEAN FORMAT IN THE POSTER\nThe raw dataset, collected as several CSV files, initially contained detailed comment metadata. The original structure included columns such as: videoId, commentId, parentId, author, text, likeCount, publishedAt, updatedAt, viewerRating, canRate, and reply.\nFor data cleaning, all CSV files in the Final_project_data folder were systematically processed using the R environment. The initial step was to streamline the dataset by retaining only the essential variables for textual and engagement analysis: text, likeCount, and reply.\nI utilized the R packages dplyr and stringr to focus on standardizing the text column. This involved a series of cleaning operations: normalization of whitespace (removing line breaks, tabs, and extra spaces, and trimming leading/trailing whitespace) and character filtering. Crucially, I removed non-essential symbols and unusual characters while meticulously preserving all Chinese characters to ensure the comments remained culturally authentic and meaningful for subsequent analysis.\nFinally, each cleaned and standardized dataset was saved as a new CSV file, appended with the suffix _cleaned. UTF-8 encoding was explicitly used to guarantee the accurate representation of the Chinese characters. This systematic workflow ensures the comment data are tidy, standardized, and immediately ready for downstream procedures, such as tokenization and sentiment or frequency analysis.\n\nsource(\"data_cleaning_CN.R\")\n\ndata cleaning complete!.\n\nSIMP001 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\n\n#Present the eample of the result\nhead(SIMP001)\n\n      videoId                  commentId parentId        author\n1 o5TfkwlthWU UgyekRC230MDXREkdeN4AaABAg     &lt;NA&gt;  @DanjonMeshi\n2 o5TfkwlthWU UgxangSP0zjJm6_gHfV4AaABAg     &lt;NA&gt;  @paullee4451\n3 o5TfkwlthWU UgwZdLtl6Eb2wgDWaDV4AaABAg     &lt;NA&gt;      @urikora\n4 o5TfkwlthWU UgwSmeqUyHYUXGD6l3l4AaABAg     &lt;NA&gt; @fayechen1928\n5 o5TfkwlthWU UgwpotCAmJmn2wWU7u54AaABAg     &lt;NA&gt; @running_goat\n6 o5TfkwlthWU UgxQSyQbnLT9r7I1faB4AaABAg     &lt;NA&gt;  @Jack2006103\n                                                                                                                       text\n1                                                             å•†å®¶é›†é«”çµ¦ç©ºè¢‹çœŸçš„ç¬‘æ­»ï¼Œä¸æ­¢ç”Ÿæ´»åœ¨ä¸­åœ‹ï¼Œé€£æ­»åœ¨ä¸­åœ‹éƒ½è¦å·è‘—æ¨‚ğŸ˜†\n2                                                                                                                      é ­é¦™\n3                       æ›´æ…˜çš„æ˜¯ï¼Œäººéƒ½èµ°äº†ä¸€å€‹æœˆ çµæœå°±åœ¨é€™æ™‚æ©Ÿé»è¢«æŠ“ä¾†æ“‹æ”¿åºœåšçš„é†œäº‹ï¼ˆè·¯å´©è¯ç‚ºè»Šè¡é€²å»å‘æ´ç„¶å¾Œå¿«é€Ÿç‡ƒèµ·ä¾†ï¼‰\n4                                                                                      æ¯æ¬¡çœ‹åˆ°ä¸­åœ‹é€™ç¨®æ‚²åŠ‡éƒ½è¦ºå¾—ä¸å¯æ€è­°ğŸ˜¨ğŸ˜­\n5 æˆ‘çœŸçš„å¿…é ˆå¾—èªªå°å²¸ç”·å¥³æˆ°çˆ­çœŸçš„è¶Šä¾†è¶Šåš´é‡== å•éå¥½å¹¾å€‹å°å²¸çš„å¥³ç”Ÿéƒ½èªç‚ºç”·ç”Ÿå°±æ˜¯æ‡‰è©²è¦çµ¦å½©ç¦® åƒé£¯å°±æ˜¯è¦å¹«å¥³ç”Ÿä»˜éŒ¢ç­‰ç­‰ å¾ˆå¯æ€•\n6                                                                                                é€™äº›åº—å®¶åƒäººè¡€é¥…é ­ï¼Œè¶…å™çˆ›\n  likeCount          publishedAt            updatedAt viewerRating canRate\n1        58 2024-05-09T16:01:52Z 2024-05-09T16:01:52Z         none    TRUE\n2         0 2024-05-09T16:02:23Z 2024-05-09T16:02:23Z         none    TRUE\n3       345 2024-05-09T16:05:05Z 2024-05-09T16:05:05Z         none    TRUE\n4         1 2024-05-09T16:05:57Z 2024-05-09T16:05:57Z         none    TRUE\n5       139 2024-05-09T16:06:47Z 2024-05-09T16:06:47Z         none    TRUE\n6         4 2024-05-09T16:07:29Z 2024-05-09T16:07:29Z         none    TRUE\n  reply\n1 FALSE\n2 FALSE\n3 FALSE\n4 FALSE\n5 FALSE\n6 FALSE\n\ndata1_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_cleaned.csv\")\ndata2_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments_cleaned.csv\")\ndata3_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments_cleaned.csv\")\ndata4_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments_cleaned.csv\")\ndata5_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments_cleaned.csv\")\ndata6_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments_cleaned.csv\")"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#preprocess-the-data",
    "href": "604/604_Final_check-in_2.html#preprocess-the-data",
    "title": "DACSS604_Final_Project",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nFor visualizing the dominant linguistic patterns within the comment data, I employed two complementary approaches. First, a Word Cloud visualization (generated using the Word_cloud_visualization.R script) provided an intuitive, qualitative representation of high-frequency words, instantly highlighting the most common terms associated with discussions of â€œSIMPâ€ behavior.\nSecond, I conducted a quantitative rank-frequency analysis by applying Zipfâ€™s Law to the word corpus. After arranging all unique words by descending frequency and assigning a rank, I plotted the resulting distribution using the ggplot2 package. The resulting visualization confirmed that the comment discourse adheres to a Zipfian distribution, where a few words account for a disproportionate share of the total vocabulary.\nThe key terms driving the discourse were clearly identifiable:\nThese visualizations collectively offer both quantitative validation (Zipfâ€™s Law distribution) and qualitative insight (Word Cloud/Top Terms) into how the audience discusses and perceives the central event and the related concept of â€œSIMPâ€ behavior in this context. The high frequency of questioning and uncertainty (ç‚ºä»€éº¼, ä¸çŸ¥é“, æ˜¯ä¸æ˜¯) coupled with terms of exploitation (pua) and suffering (å—å®³è€…) reveals a key focus on moral judgment and accountability in the discussion.\n\nsource(\"TOKENIZATION.R\")\n\nTokenization complete!\n\nSIMP001_comments_tokens &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_tokens.csv\")\n#Present the eample of the result\nhead(SIMP001_comments_tokens)\n\n  likeCount reply     word\n1       345 FALSE   è¡é€²å»\n2         1 FALSE ä¸å¯æ€è­°\n3       139 FALSE   è¶Šä¾†è¶Š\n4       139 FALSE   å¥½å¹¾å€‹\n5       141 FALSE   éº¥ç•¶å‹\n6         3 FALSE   æœ‰äººèªª\n\n\n\nsource(\"Word_frequency.R\")\n\nWord Frequency Calculation Complete!\n\nSIMP001_wordfreq &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_wordfreq.csv\")\n\n#Present the eample of the result\nhead(SIMP001_wordfreq)\n\n    word  n rank\n1 å—å®³è€… 75    1\n2 ç‚ºä»€éº¼ 67    2\n3 ä¸çŸ¥é“ 53    3\n4    pua 42    4\n5 é€™ä»¶äº‹ 34    5\n6 ä¸€å€‹äºº 32    6\n\n\n\nsource(\"Same_Word.R\")\n\nCommon words saved to: Final_project_data/common_words_across_files.csv \n\ncommon_words &lt;- read.csv(\"Final_project_data/common_words_across_files.csv\")\n\n#Present the eample of the result\nhead(common_words)\n\n    word total_count\n1 ä¸çŸ¥é“         172\n2 ä¸ºä»€ä¹ˆ         154\n3 å—å®³è€…         118\n4    pua         115\n5 æ˜¯ä¸æ˜¯         101\n6 ç‚ºä»€éº¼         100\n\n\n\nsource(\"Change_to_Traditional_Chinese.R\")\n\nWarning: package 'textstem' was built under R version 4.4.3\n\n\nLoading required package: koRpus.lang.en\n\n\nWarning: package 'koRpus.lang.en' was built under R version 4.4.3\n\n\nLoading required package: koRpus\n\n\nWarning: package 'koRpus' was built under R version 4.4.3\n\n\nLoading required package: sylly\n\n\nWarning: package 'sylly' was built under R version 4.4.3\n\n\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\n\nAttaching package: 'koRpus'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    tokens, types\n\n\nThe following object is masked from 'package:tm':\n\n    readTagged\n\n\nThe following object is masked from 'package:readr':\n\n    tokenize\n\n\nWarning: package 'tmcn' was built under R version 4.4.3\n\n\n# tmcn Version: 0.2-13\n\n\n[1] \"girl\"    \"woman\"   \"simping\" \"lover\"  \nTranslation complete! Output saved to 'your_output_file.csv'\n\nTraditional_Chinese_data_cleaned &lt;- read.csv(\"Final_project_data/traditional_common_words_combined.csv\")\n\n#Present the data after cleaning\nhead(Traditional_Chinese_data_cleaned)\n\n  traditional_text total_count\n1           ç‚ºä»€éº¼         254\n2           ä¸çŸ¥é“         172\n3           å—å®³è€…         118\n4              pua         116\n5           æ˜¯ä¸æ˜¯         101\n6           å¥³æœ‹å‹          95"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#visualization",
    "href": "604/604_Final_check-in_2.html#visualization",
    "title": "DACSS604_Final_Project",
    "section": "Visualization",
    "text": "Visualization\nFor visualizing patterns in the comments, I used two approaches. First, the Word_cloud_visualization.R script generated word clouds to highlight high-frequency words, providing a clear and intuitive view of the most common terms associated with discussions of â€œSIMPâ€ behavior. Second, I applied Zipfâ€™s Law to examine the relationship between word rank and frequency. After arranging words by descending frequency and assigning ranks, I plotted all words using ggplot2, labeling only the top five most frequent words to emphasize the key terms in the discourse. The resulting visualizations offer both quantitative and qualitative insight into how people discuss and perceive â€œSIMPâ€ behavior in YouTube comments.\n\nsource(\"Word_cloud_visualization.R\")\n\n\n\n\n\n\n\n\nWord Cloud generated for: traditional_common_words_combined.csv\n\n\n\n# Sort by frequency and assign ranks\nzipf_data_ranked &lt;- Traditional_Chinese_data_cleaned %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  mutate(rank = row_number())\n\n# Print the top 5 ranked words to confirm the data structure\nprint(head(zipf_data_ranked, 5))\n\n  traditional_text total_count rank\n1           ç‚ºä»€éº¼         254    1\n2           ä¸çŸ¥é“         172    2\n3           å—å®³è€…         118    3\n4              pua         116    4\n5           æ˜¯ä¸æ˜¯         101    5\n\n# --- Linear Scale (As Requested) ---\n\nggplot(zipf_data_ranked, aes(x = rank, y = total_count)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"darkorange\", size = 1.5) +\n  geom_text(\n    # Label the top 8 words\n    aes(label = ifelse(rank &lt;= 6, traditional_text, \"\")),\n    vjust = -0.8,\n    size = 3.5,\n    check_overlap = TRUE # Prevents overlapping labels\n  ) +\n  labs(\n    title = \"Zipfâ€™s Law: Word Rank vs Frequency\",\n    x = \"Rank of Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nTranslation\n\n\n\nRank\nWord\nTranslate\n\n\n1\nç‚ºä»€éº¼\nâ€œWhy / Why is it thatâ€¦â€\n\n\n2\nä¸çŸ¥é“\nâ€œDonâ€™t knowâ€\n\n\n3\nå—å®³è€…\nâ€œVictimâ€\n\n\n4\npua\nâ€œPUAâ€\n\n\n5\næ˜¯ä¸æ˜¯\nâ€œIs it / Is it not?â€\n\n\n6\nå¥³æœ‹å‹\nâ€œGirlfriendâ€"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#word-embedding",
    "href": "604/604_Final_check-in_2.html#word-embedding",
    "title": "DACSS604_Final_Project",
    "section": "Word Embedding",
    "text": "Word Embedding\nFor semantic analysis, I applied Word2Vec using a pseudo-document approach to capture relationships between words in the comments. Each word was repeated according to its frequency (total_count) to create co-occurrence information, which is essential for small datasets where natural co-occurrences are limited. The repeated words were then combined into a single space-separated pseudo-document and used to train a skip-gram Word2Vec model with a vector dimension of 50, window size of 5, and 50 iterations, setting min_count = 1 to include all words.\nThe resulting word vectors allow calculation of cosine similarity to examine semantic relationships between words, as well as clustering and downstream supervised learning tasks. For example, the vector for a keyword such as â€œç‚ºä»€éº¼â€ can be compared with all other word vectors to identify the top semantically similar words, revealing patterns in how concepts related to â€œSIMPâ€ behavior are discussed in YouTube comments. This approach provides a robust representation of word meaning in the context of the dataset while accommodating the limited co-occurrence information inherent in smaller comment datasets.\n\n# Word2Vec can be the best option for the word embeding.\n\nsource(\"Word Embeddings.R\")\n\nWarning: package 'word2vec' was built under R version 4.4.3\n\n\nWarning: package 'text2vec' was built under R version 4.4.3"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#sentiment-analysis",
    "href": "604/604_Final_check-in_2.html#sentiment-analysis",
    "title": "DACSS604_Final_Project",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nFor sentiment analysis, I applied a custom Chinese sentiment dictionary tailored to the context of â€œSIMPâ€ behavior. The dictionary categorizes words into three groups: positive (supportive or relationship-related words such as â€œå¥³æœ‹å‹â€ and â€œé—œå¿ƒâ€), negative (critical or unfairness-related words such as â€œä¸å€¼å¾—â€ and â€œä¸å…¬å¹³â€), and behavior (attention-seeking or â€œsimpâ€ behavior words such as â€œpuaâ€ and â€œè¿½æ±‚â€). Using R, I computed sentiment scores for each word in the dataset by summing occurrences in these categories. A raw polarity score was calculated as the sum of positive and behavior counts minus negative counts, then normalized by the total occurrences of all dictionary words to produce a relative polarity measure.\nThe analysis revealed that the current positive and negative categories do not fully capture the sentiment expressed in the comments. Some words were misclassified or contextually ambiguous, highlighting that the dictionary needs further adjustment and refinement to improve accuracy. Polarity distributions were visualized using a histogram, providing an overview of how positive, negative, and behavior-related language appears in discussions of â€œSIMPâ€ behavior. This approach provides a preliminary sentiment assessment while acknowledging the limitations of the existing dictionary.\n\nsource(\"Sentiment Analysis.R\")\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” forcats   1.0.0     âœ” lubridate 1.9.4\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– NLP::annotate()         masks ggplot2::annotate()\nâœ– httr::content()         masks NLP::content()\nâœ– dplyr::filter()         masks stats::filter()\nâœ– jsonlite::flatten()     masks purrr::flatten()\nâœ– rvest::guess_encoding() masks readr::guess_encoding()\nâœ– dplyr::lag()            masks stats::lag()\nâœ– koRpus::tokenize()      masks readr::tokenize()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'quanteda.textmodels' was built under R version 4.4.3\n\n\nWarning: package 'stopwords' was built under R version 4.4.3\n\n\n\nAttaching package: 'stopwords'\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nWarning: package 'caret' was built under R version 4.4.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:httr':\n\n    progress\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n[1] \"DFM after Custom Simp Dictionary Lookup (Counts per category):\"\n\n### SUPERVISED LEARNING (Naive Bayes Classification)\n[1] \"Confusion Matrix:\"\n\n### LEXICON-BASED NTUSD SENTIMENT RESULTS\n[1] \"Mean Sentiment Score by Simping Label (NTUSD):\"\n# A tibble: 2 Ã— 2\n  contains_simp_factor mean_sentiment\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 FALSE                       -0.0990\n2 TRUE                        -0.477 \n\n\n\n\n\n\n\n\n\nIn this graph:\nX-Axis: Simping Label:\n\nFALSE: Comments that do not contain any of the words from your custom â€œsimpâ€ dictionary (e.g., â€œèˆ”ç‹—,â€ â€œå·¥å…·äºº,â€ â€œä¸€å»‚æƒ…é¡˜,â€ etc.).\nTRUE: Comments that do contain at least one word from your custom â€œsimpâ€ dictionary.\n\nY-Axis: Comment Sentiment Score:\n\nPositive Scores (above 0): Indicate a more positive overall sentiment.\nZero (0): Indicates a neutral or balanced sentiment.\nNegative Scores (below 0): Indicate a more negative overall sentiment."
  },
  {
    "objectID": "604/604_Final_check-in_2.html#sentiment-analysis-summary",
    "href": "604/604_Final_check-in_2.html#sentiment-analysis-summary",
    "title": "DACSS604_Final_Project",
    "section": "Sentiment Analysis Summary",
    "text": "Sentiment Analysis Summary\nThe lexicon-based sentiment analysis, utilizing the NTUSD dictionary, reveals a pronounced negative emotional shift in texts discussing the â€œsimpâ€ phenomenon. Specifically, content that contains terms from the custom dictionaryâ€”which targets themes like â€œsimp behaviorâ€ (e.g., èˆ”ç‹—, simp), â€œvictim positionâ€ (e.g., å—å®³è€…, pua), and â€œrelationship imbalanceâ€â€”shows a highly negative mean sentiment score of -0.477. This score is significantly more negative than the average score of -0.0990 found in texts that do not contain these specific terms. This sharp difference (a nearly five-fold increase in negative sentiment magnitude) indicates that conversations about excessive one-sided effort, perceived exploitation, and unequal relationshipsâ€”the core of the â€œsimpâ€ conceptâ€”are strongly associated with negative emotional discourse within the corpus."
  },
  {
    "objectID": "604/604_Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "href": "604/604_Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "title": "DACSS604_Final_Project",
    "section": "Supervised Learning Analysis (Naive Bayes Classification)",
    "text": "Supervised Learning Analysis (Naive Bayes Classification)\nThe Naive Bayes model was employed to classify comments based on whether they contained the â€œsimpâ€ factor, using a cleaned feature set that excluded all words from the custom â€œsimpâ€ dictionary to prevent data leakage. The model achieved an overall Accuracy of 81.69%, which is slightly higher than the No Information Rate (NIR) of 80.36%, indicating its performance is marginally better than random guessing based on class prevalence.\nHowever, a closer look at the results reveals a significant class imbalance issue and skewed performance:\n\nHigh Sensitivity (Recall): The model is excellent at correctly identifying comments that do NOT contain the simp factor (the majority class, FALSE), with a high Sensitivity of 95.72%.\nLow Specificity: Conversely, the model is very poor at correctly identifying comments that DO contain the simp factor (the minority class, TRUE), with a low Specificity of 24.29%.\nKappa Value: The Kappa statistic of 0.2565 suggests only a fair level of agreement beyond chance.\n\nIn summary, the high overall accuracy is largely driven by correctly classifying the prevalent negative class (FALSE). The model struggles to reliably identify actual â€œsimpâ€ comments (TRUE), suggesting that the remaining general vocabulary in the comments lacks sufficient predictive power to consistently distinguish between the two categories without the core dictionary terms.\n\nsource(\"Supervised_Learning.R\")\n\n\n--- Solving Data Leakage: Remove the word which exist in Simp dictionary ---\n[1] \"Original (matrix_main): 2703\"\n[1] \"Remove the word in Simp dictionary (X_cleaned): 2674\"\n\n--- 6. Naive Bayes Training ---\n\n--- Naive Bayes Prediction ---\n[1] \"Confusion Matrix:\"\nConfusion Matrix and Statistics\n\n                 y_test\npredicted_cleaned FALSE TRUE\n            FALSE   693  134\n            TRUE     31   43\n                                        \n               Accuracy : 0.8169        \n                 95% CI : (0.79, 0.8416)\n    No Information Rate : 0.8036        \n    P-Value [Acc &gt; NIR] : 0.1676        \n                                        \n                  Kappa : 0.2565        \n                                        \n Mcnemar's Test P-Value : 2.011e-15     \n                                        \n            Sensitivity : 0.9572        \n            Specificity : 0.2429        \n         Pos Pred Value : 0.8380        \n         Neg Pred Value : 0.5811        \n              Precision : 0.8380        \n                 Recall : 0.9572        \n                     F1 : 0.8936        \n             Prevalence : 0.8036        \n         Detection Rate : 0.7691        \n   Detection Prevalence : 0.9179        \n      Balanced Accuracy : 0.6001        \n                                        \n       'Positive' Class : FALSE"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#topic-modeling-lda",
    "href": "604/604_Final_check-in_2.html#topic-modeling-lda",
    "title": "DACSS604_Final_Project",
    "section": "Topic Modeling (LDA)",
    "text": "Topic Modeling (LDA)\nThe script follows a standard text mining workflow using the tidyverse and text2vec packages:\n\nData Preparation: It reads the individual tokenized comment files, reconstructs the full comments by assigning and aggregating tokens by a unique document ID (doc_id), and then combines the tokens back into complete text strings.\nFeature Engineering: It creates an iterator from the aggregated text and builds a vocabulary. Crucially, it prunes the vocabulary by removing words that occur less than three times (term_count_min = 3), which helps reduce noise and improves the quality of the derived topics.\nDTM Creation: The processed tokens are converted into a Document-Term Matrix (DTM), which is the input required for LDA.\nModel Training: The script initializes and trains an LDA model with a predefined number of topics (K=8) and 500 iterations.\nOutput: Finally, the code extracts and saves two key results: the Topic-Word distribution (the top 10 most characteristic words for each of the 8 topics) and the Document-Topic distribution (the probability that each comment belongs to each topic), storing both as CSV files for subsequent qualitative analysis.\n\n\nsource(\"Topic_Model.R\")\n\n[1] \"Starting LDA Topic Modeling with K = 8\"\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======================================================================| 100%\n[1] \"LDA Training Complete.\"\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nâ„¹ Using compatibility `.name_repair`.\n\n\n[1] \"Top 10 Words for Each Topic:\"\n# A tibble: 10 Ã— 9\n   Topic_Word_Rank Topic_0 Topic_1 Topic_2 Topic_3  Topic_4  Topic_5 Topic_6\n   &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1 Word_1          pua     ä¸çŸ¥é“  å—å®³è€…  æ˜¯ä¸æ˜¯   ç‚ºä»€éº¼   ä¸çŸ¥é“  è€Œä¸æ˜¯ \n 2 Word_2          ä¸ºä»€ä¹ˆ  ä¸‡ä½™å…ƒ  ä¸ºä»€ä¹ˆ  å¥³æœ‹å‹   åœ¨ä¸€èµ·   è¶Šæ¥è¶Š  é€™ä»¶äº‹ \n 3 Word_3          ä¸­å›½äºº  pua     pua     ä¸ºä»€ä¹ˆ   ä¹Ÿå¯ä»¥   å¤§éƒ¨åˆ†  ä¸çŸ¥é“ \n 4 Word_4          ç”·å­©å­  å¯èƒ½æ˜¯  çœŸçš„æ˜¯  ä¸çŸ¥é“   ä¸å¯èƒ½   éƒ½ä¸æ˜¯  ä¸€å®šæ˜¯ \n 5 Word_5          å¥³å­©å­  ä¸€å€‹äºº  éº¦å½“åŠ³  ä¸éœ€è¦   éƒ½æ²’æœ‰   çœŸçš„æ˜¯  å¥³æœ‹å‹ \n 6 Word_6          ä¹Ÿä¸æ˜¯  é€™ä»¶äº‹  å®¶åº­çš„  ä¹Ÿæ²¡æœ‰   é€™å°±æ˜¯   å—å®³è€…  èƒ½ä¸èƒ½ \n 7 Word_7          æ˜¯ä¸æ˜¯  ä¸­å›½äºº  å…¨ä¸–ç•Œ  å¤§éƒ¨åˆ†   ä¹Ÿæ²’æœ‰   å¥³æœ‹å‹  äººæ°‘å¹£ \n 8 Word_8          æ²¡ä»€ä¹ˆ  å°¤å…¶æ˜¯  å…¶å®æ˜¯  ç”·æœ‹å‹   ç”·å°Šå¥³å‘ æˆ‘çŸ¥é“  åƒåœ¾æ¡¶ \n 9 Word_9          å¯èƒ½æ˜¯  çœŸçš„æ˜¯  å¥½åƒæ˜¯  ç”·å¥³å¹³ç­‰ æ²’ä»€éº¼   æ˜¯ä¸æ˜¯  æœ‰äº›äºº \n10 Word_10         ä¸å€¼å¾—  ç‚ºä»€éº¼  è‚¯å®šæ˜¯  æ‰€è°“çš„   ä¸å­˜åœ¨   ä¹Ÿä¸æ˜¯  ä¸ºä»€ä¹ˆ \n# â„¹ 1 more variable: Topic_7 &lt;chr&gt;\n[1] \"Saved topic words to lda_topic_words.csv\"\n[1] \"Document-Topic Distribution (Head):\"\n# A tibble: 6 Ã— 9\n  doc_id    V1    V2    V3    V4    V5    V6    V7    V8\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1     0  0      0    0      0     0       0     0\n2      2     0  0      0    0      0     0       0     0\n3      3     0  0.05   0    0.15   0.6   0.2     0     0\n4      4     0  0.1    0.2  0      0.4   0.3     0     0\n5      5     0  0      0.8  0      0.2   0       0     0\n6      6     0  0      0    0      0     0       1     0\n[1] \"Saved document-topic distribution to lda_doc_topic_distr.csv\"\n\n\nTranslation for the every words in the topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic_Word_Rank\nTopic_0\nTopic_1\nTopic_2\nTopic_3\nTopic_4\nTopic_5\nTopic_6\nTopic_7\n\n\nWord_1\nPUA\nDonâ€™t know\nVictim\nIs it?\nWhy\nDonâ€™t know\nAnd not\nBuffet (Resource)\n\n\nWord_2\nWhy\nOver 10k yuan\nWhy\nGirlfriend\nBe together\nMore and more\nThis matter\nHighway\n\n\nWord_3\nChinese people\nPUA\nPUA\nWhy\nAlso can\nMajority\nDonâ€™t know\nToo good\n\n\nWord_4\nBoy / Male\nPossibly\nTruly is\nDonâ€™t know\nImpossible\nAre not all\nMust be\nWhy\n\n\nWord_5\nGirl / Female\nA person\nMcDonaldâ€™s\nDonâ€™t need\nDonâ€™t have at all\nTruly is\nGirlfriend\nMainly is\n\n\nWord_6\nAlso is not\nThis matter\nFamilyâ€™s\nAlso donâ€™t have\nThis is\nVictim\nCan or cannot\nSome people\n\n\nWord_7\nIs it?\nChinese people\nWhole world\nMajority\nAlso donâ€™t have\nGirlfriend\nRMB (Money)\nInequality\n\n\nWord_8\nNothing much\nEspecially\nActually is\nBoyfriend\nMale Superiority\nI know\nTrash Can (Worthless)\nShould be\n\n\nWord_9\nPossibly\nTruly is\nSeems like\nGender Equality\nNothing much\nIs it?\nSome people\nA person\n\n\nWord_10\nNot worth it\nWhy\nDefinitely is\nSo-called\nDoes not exist\nAlso is not\nWhy\nGender Equality\n\n\n\nThe interpretation for each topic\n\n\n\n\n\n\n\n\nTopic\nCore Keywords & Interpretation\nSuggested Topic Label\n\n\nTopic 0\nThis topic strongly links the PUA phenomenon with discussions about specific gender roles and identities within the Chinese context. The presence of â€œNot worth itâ€ suggests this cluster is focused on evaluating the value of actions/relationships under the PUA framework.\nPUA & Gender Dynamics in China\n\n\nTopic 1\nThis topic mixes uncertainty and specific financial figures (Over 10k yuan), directly linked to PUA. It suggests discussions about high-stakes financial loss or investment by an individual in a relationship where the outcome or reality is unclear.\nFinancial Dimension of PUA & Uncertainty\n\n\nTopic 2\nThe simultaneous presence of â€œVictim,â€ â€œPUA,â€ and â€œTruly isâ€ indicates a core discussion cluster dedicated to validating the existence and reality of being exploited. â€œMcDonaldâ€™sâ€ implies cheap/casual provision, while â€œFamilyâ€™sâ€ suggests the conversation may touch on the origins or impact of these dynamics within a family unit.\nValidating Victimhood & Low-Cost Exploitation\n\n\nTopic 3\nThis topic is saturated with questioning terms (â€œIs it?â€, â€œWhy?â€, â€œDonâ€™t knowâ€), applied directly to boyfriend/girlfriend roles and the concept of gender equality. It represents a pervasive atmosphere of skepticism and critical discussion about expected behavior in modern relationships.\nSkepticism & Questioning of Relationship Roles\n\n\nTopic 4\nA highly polarized topic that denies (ä¸å¯èƒ½, ä¸å­˜åœ¨) the relevance or existence of Male Superiority (ç”·å°Šå¥³å‘). It focuses on the possibility of being together (åœ¨ä¸€èµ·), suggesting a desire for modern, equal partnerships and a strong rejection of patriarchal norms.\nDenial of Traditional Patriarchy in Relationships\n\n\nTopic 5\nTerms like â€œMore and moreâ€ and â€œMajorityâ€ point to a discussion of social trends and scale. When combined with â€œVictimâ€ and â€œGirlfriend,â€ it indicates a conversation about whether victimhood is becoming increasingly common or if the perception of victimhood is changing within the female partner role.\nDiscussing Shifting Social Norms & Victim Pool\n\n\nTopic 6\nThis is the most explicitly transactional topic. It discusses financial payment (RMB) and the concept of a person being reduced to a â€œTrash Canâ€ (worthless/emotional dumping ground). The use of â€œAnd notâ€ suggests a debate over what a relationship should be versus what it currently is (i.e., not a transaction, but one of money/exploitation).\nMonetary Value vs.Â Emotional Worth (The Price of Simping)\n\n\nTopic 7\nThis topic links resource provision (implied by â€œBuffetâ€ and â€œHighway,â€ often used as metaphors for free/easy access) with discussions of Inequality and Gender Equality. It debates whether resources should be provided freely, who is responsible for providing them, and the resulting fairness in the relationship structure.\nResource Provision & Equality Debate\n\n\n\n\n\n\nDTM\nTopic_Word_Rank\n\n\nV1\nTopic_0\n\n\nV2\nTopic_1\n\n\nV3\nTopic_2\n\n\nV4\nTopic_3\n\n\nV5\nTopic_4\n\n\nV6\nTopic_5\n\n\nV7\nTopic_6\n\n\nV8\nTopic_7"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#causal-inference",
    "href": "604/604_Final_check-in_2.html#causal-inference",
    "title": "DACSS604_Final_Project",
    "section": "Causal Inference",
    "text": "Causal Inference\nIn the casual inference part, I present the Ordinary Least Squares (OLS) regression model to analyze how the probability of eight LDA topics influences the number of likes received by a comment (likeCount). Topic V8 (PUA/Victim) was set as the reference group (Reference Topic) in the model. Overall, the modelâ€™s explanatory power is extremely low (\\(\\text{Adjusted R-squared} = 0.00025\\)), suggesting that the variation in likeCount is primarily influenced by factors outside the model, rather than the topics themselves. However, the coefficient tests for individual topics revealed that Topic V2 demonstrated a statistically significant positive influence. After controlling for the effects of other topics, an increase of 1 unit in the probability of Topic V2 (Financial/Money II), relative to the reference group V8, is expected to increase the number of likes by approximately 9.66 (\\(p = 0.038^{*}\\)). This suggests that specific discussion content related to money or finance is more likely to garner attention and agreement within the community. Apart from the intercept, the remaining topics (V1, V3, V4, V5, V6, and V7) did not show a statistically significant relationship with the number of likes.\n\nsource(\"Causal_Inference.R\")\n\nRows: 3006 Columns: 9\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (9): doc_id, V1, V2, V3, V4, V5, V6, V7, V8\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n[1] \"--- OLS Regression Results (Outcome: likeCount) ---\"\n[1] \"Reference Topic: V8 (PUA/Victim)\"\n\nCall:\nlm(formula = likeCount ~ V1 + V2 + V3 + V4 + V5 + V6 + V7, data = merged_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -18.59   -8.93   -6.76   -5.12 1763.41 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.934      2.301   3.882 0.000106 ***\nV1            -2.424      4.144  -0.585 0.558600    \nV2             9.655      4.656   2.074 0.038172 *  \nV3            -2.701      4.312  -0.626 0.531107    \nV4            -2.380      4.266  -0.558 0.576978    \nV5            -2.815      4.399  -0.640 0.522298    \nV6             1.729      4.351   0.398 0.691027    \nV7            -0.953      4.363  -0.218 0.827099    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.08 on 2998 degrees of freedom\nMultiple R-squared:  0.002579,  Adjusted R-squared:  0.00025 \nF-statistic: 1.107 on 7 and 2998 DF,  p-value: 0.3553"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#conclusion",
    "href": "604/604_Final_check-in_2.html#conclusion",
    "title": "DACSS604_Final_Project",
    "section": "Conclusion",
    "text": "Conclusion\n(PUT THESE TWO POINT INTO THE FUTURE WORK ALSO NEED TO EXPLAIN ABOUT DIFFICULTY TO COLLECT THE ACADEMIC DATA THERE ** SIMP DOES NOT HAVE A FORMAL DEFINITION**\nâ€œSynthesizing the projectâ€™s findings, the primary discovery is that community engagement (\\(\\text{likeCount}\\)) on YouTube comments is not driven by broad emotional tone or general topics, but rather by a specific, critical narrative focused on â€˜financial exploitation and victimhoodâ€™ (\\(\\text{Topic}\\) \\(\\text{V2}\\)). This specific form of critical discussion related to â€˜SIMPâ€™ behavior is extremely negative in sentiment (\\(\\text{mean}\\) \\(\\text{sentiment} = -0.477\\)) and is so unique in its linguistic pattern that its occurrence can be accurately predicted by the supervised learning model. Consequently, the communityâ€™s response to â€˜SIMPâ€™ behavior is highly concentrated and emotionally charged, with its online visibility predominantly stemming from comments that link the behavior directly to concrete financial inequality and victim scenarios.â€"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#future",
    "href": "604/604_Final_check-in_2.html#future",
    "title": "DACSS604_Final_Project",
    "section": "Future",
    "text": "Future\nFuture work should prioritize addressing the observed limitations in both the supervised classification model and the initial data preprocessing pipeline to enhance the robustness and explanatory power of the analysis. Firstly, while the initial Naive Bayes classifier provided baseline insights, its predictive performance should be critically re-evaluated. Improving the accuracy of the automated simp classification label requires exploring more sophisticated machine learning techniques, such as Support Vector Machines (SVMs), Gradient Boosting, or even Transformer-based deep learning models. Concurrently, enhancing the feature set by refining or expanding the custom dictionariesâ€”perhaps incorporating sentiment scores or incorporating word embeddingsâ€”could significantly boost the modelâ€™s ability to discriminate between classes, moving beyond simple bag-of-words approaches. Secondly, a crucial area for improvement lies in the token filtering and data processing stage. Despite standard removal procedures, the presence of numerous contextually irrelevant tokens, such as specific objects (â€œé«˜é€Ÿå…¬è·¯â€) and brands (â€œéº¥ç•¶å‹â€), confirms the necessity of a more rigorous, domain-specific cleanup. Future efforts must focus on constructing an expanded, domain-aware stop word list or implementing Named Entity Recognition (NER) to systematically identify and remove non-topical, low-information tokens, ensuring the remaining features are highly predictive and representative of the core concepts being discussed."
  },
  {
    "objectID": "604/604_Final_check-in_2.html#reference",
    "href": "604/604_Final_check-in_2.html#reference",
    "title": "DACSS604_Final_Project",
    "section": "Reference",
    "text": "Reference\nHO, Daniel. The (simp)le truth about excessive & obsessive romantic behaviors in men. (2023). https://ink.library.smu.edu.sg/etd_coll/516\nKrishnamurthy, V., & Duan, Y. (2017). Dependence Structure Analysis Of Meta-level Metrics in YouTube Videos: A Vine Copula Approach. arXiv preprint arXiv:1712.10232. â€œto explain the comment and the view of the video are relatedâ€\nLun-Wei Ku and Hsin-Hsi Chen (2007). Mining Opinions from the Web: Beyond Relevance Retrieval. Journal of American Society for Information Science and Technology, Special Issue on Mining Web Resources for Enhancing Information Retrieval, 58(12), pages 1838-1850.\nPew Research Center. (2020). Many Americans get news on YouTube, where news organizations and independent producers thrive side by side. https://www.pewresearch.org/journalism/2020/09/28/many-americans-get-news-on-youtube-where-news-organizations-and-independent-producers-thrive-side-by-side/\nZhou, W. (2024). é‡åº†è­¦æ–¹å‘å¸ƒâ€œèƒ–çŒ«â€äº‹ä»¶è­¦æƒ…é€šæŠ¥ [Chongqing police issue incident report on the â€œPangmaoâ€ incident]. Xinhua Net. http://www.news.cn/politics/20240519/fb56352660c94810a58e79bc18459a3e/c.html"
  },
  {
    "objectID": "projects.html#final-project-1",
    "href": "projects.html#final-project-1",
    "title": "Project",
    "section": "604 Final Project",
    "text": "604 Final Project\nThe final project will follow a structured process to collect, clean, and analyze data from YouTube news channels in order to explore public trust in AI. First of all, find the top three global news organizations, such as BBC News, CNN, and Reuters, based on their reputation and audience reach. After that, I will locate their official YouTube channels and select videos with the highest view counts that discuss AI-related topics or trust in AI. This ensures that the data reflects widely viewed content and captures a diverse audience perspective. The third process, using the YouTube API and collecting the comments from the videos that I have selected. After collecting the comments, a data cleaning process will be implemented to remove irrelevant content, such as comments unrelated to AI, spam, emojis, and other unnecessary symbols. This step ensures that the dataset contains meaningful textual information for analysis. After doing the first part of the data cleaning, the comments will be sorted into categories reflecting whether the commenter expresses trust, distrust, or neutral sentiment toward AI decision-making.\n\nView Full Analysis & Report (HTML)"
  },
  {
    "objectID": "index.html#resume",
    "href": "index.html#resume",
    "title": "About Me",
    "section": "",
    "text": "View Full Resume"
  },
  {
    "objectID": "projects.html#dashboard",
    "href": "projects.html#dashboard",
    "title": "Project",
    "section": "Dashboard",
    "text": "Dashboard\nThe interactive dashboard below provides a dynamic visualization of the study results, specifically focusing on the Mean Trust Score across all six Risk Groups and the accompanying statistical summary tables.\n\nView Full Analysis & Report (HTML)"
  },
  {
    "objectID": "603/Final_Project_part1.html",
    "href": "603/Final_Project_part1.html",
    "title": "603_Final_Project",
    "section": "",
    "text": "# Load necessary libraries\n#install.packages(\"stargazer\")  # if not already installed\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(nnet)\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.4.3\n\n\ncorrplot 0.95 loaded\n\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.4.3\n\n\nLoading required package: NLP\n\n\nWarning: package 'NLP' was built under R version 4.4.2\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.4.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(corrplot)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gganimate)\n\nWarning: package 'gganimate' was built under R version 4.4.3"
  },
  {
    "objectID": "603/Final_Project_part1.html#introduction",
    "href": "603/Final_Project_part1.html#introduction",
    "title": "603_Final_Project",
    "section": "Introduction",
    "text": "Introduction\nAs generative AI tools like ChatGPT become increasingly integrated into higher education, understanding how students perceive and engage with these technologies has become a critical area of inquiry. This study aims to explore two key questions: first, whether students who use generative AI more frequently tend to trust AI-generated content more; and second, whether students from different academic fields vary in their ethical concerns about the use of generative AI. To examine these questions, survey data were collected from university students, focusing on their frequency of AI use, trust levels, and ethical attitudes. The analysis included hypothesis testing and regression modeling to assess the relationships between these variables and provide insight into studentsâ€™ perspectives on this rapidly evolving technology."
  },
  {
    "objectID": "603/Final_Project_part1.html#research-question",
    "href": "603/Final_Project_part1.html#research-question",
    "title": "603_Final_Project",
    "section": "Research Question",
    "text": "Research Question\nâ€œHow are university studentsâ€™ self-reported knowledge and ethical concerns about generative AI influenced by their frequency of AI use and their academic field of study?â€\n\n# Read the dataset\ndata &lt;- read.csv(\"AI_Chatbots_Students_Attitude_Dataset_EN.csv\", sep = \",\", stringsAsFactors = FALSE)"
  },
  {
    "objectID": "603/Final_Project_part1.html#description",
    "href": "603/Final_Project_part1.html#description",
    "title": "603_Final_Project",
    "section": "Description",
    "text": "Description\nThis project investigates university studentsâ€™ perceptions, usage, and trust in generative AI tools (like ChatGPT), focusing on how demographic factors, usage frequency, and ethical concerns relate to trust and future intentions."
  },
  {
    "objectID": "603/Final_Project_part1.html#hypothesis",
    "href": "603/Final_Project_part1.html#hypothesis",
    "title": "603_Final_Project",
    "section": "Hypothesis",
    "text": "Hypothesis\n\nHypothesis 1 (Frequency Knowledge Influence Using AI)\nHâ‚€ (Null Hypothesis): There is no relationship between studentsâ€™ frequency of generative AI use and their self-reported knowledge of AI.\nHâ‚ (Alternative Hypothesis): Students who use generative AI more frequently report higher levels of knowledge about AI.\n\n\n\nHypothesis 2 (Ethics and Field of Study)\nHâ‚€ (Null Hypothesis): Students from different academic fields do not differ in their ethical concerns about using generative AI.\nHâ‚ (Alternative Hypothesis): Students from different academic fields have significantly different ethical concerns about using generative AI."
  },
  {
    "objectID": "603/Final_Project_part1.html#descriptive-statistics",
    "href": "603/Final_Project_part1.html#descriptive-statistics",
    "title": "603_Final_Project",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThis dataset contains survey responses from university students regarding their usage patterns, perceptions, and attitudes toward generative AI technologies like ChatGPT in educational settings. The data frame consists of 184 rows (participants) and multiple columns representing different survey questions.\n\nSelected Variables:\n\nQ1: Academic level.\nQ2: Field of study.\nQ3: Gender identity.\nQ4: Frequency of generative AI use.\nQ5.1 â€“ Q5.6: Perceived benefits of using AI in learning.\nQ6.1 â€“ Q6.8: Concerns or perceived risks.\nQ7.1 â€“ Q7.4: Trust in AI-generated information.\nQ8.1 â€“ Q8.5: Benefit on academic performance or habits.\nQ9.1 â€“ Q9.5: Ethical to use AI in future educational tasks.\n\nFactorizing the Dataset Some variables in the dataset are categorical by nature but may initially be read as character strings or numeric values. To enable appropriate analysis and visualization, these columns were converted into factors with meaningful levels. This step helps in treating variables like gender, academic level, and frequency of AI use as categorical variables rather than continuous or plain text.\nQ1 (Academic Level): Converted into a binary value with levels between Undergraduate and Graduate.\nQ2 (Field of Study): Converted into a factor with levels like Marketing, Finance, Accounting, etc.\nQ3 (Gender): Treated as a binary value from male and female into 0 and 1.\nLikert-Scale Responses (Q4) (Frequency of Generative AI Use): There are five different frequency from Never to Very Often. In this case, we use Likert-Scale to make the data can present correctly.\nLikert-Scale Responses (Q5 to Q9): These responses are ordinal in nature, typically ranging from 1 to 5 (e.g., Strongly Disagree to Strongly Agree). These were converted into ordered factors to reflect their ordinal scale for better statistical interpretation.\nThis factorization ensures accurate summarization, plotting, and modeling in further stages of analysis.\n\n# Convert categorical variables to factors\n#Factorizing categorical columns\ndata$Q1 &lt;- factor(data$Q1,\n                  levels = c('Bachelor', 'Master'),\n                  ordered = TRUE)\nQ1_likert_scale &lt;- c(\"Bachelor\" = 0, \"Master\" = 1)\n#Q1_likert_scale &lt;- c(\"Bachelor\" = 0, \"Master\" = 1)\n\n\n#data$Q2 &lt;- relevel(data$Q2, ref = \"Finance\")\ndata$Q2 &lt;- factor(data$Q2,\n                  levels = c(\"Marketing\", \"Finance\", \"Accounting\", \"International Economic Relations\", \"Economics and Business\", \"Economics\"),\n                  ordered = FALSE)\ndata$Q2 &lt;- relevel(data$Q2, ref = \"Finance\")\n\ndata$Q3 &lt;- factor(data$Q3,\n                  levels = c(\"Male\", \"Female\"),\n                  ordered = TRUE)\n\nQ3_likert_scale &lt;- c(\"Male\" = 0, \"Female\" = 1)\n\n\n#data$Q3 &lt;- factor(data$Q3, levels = c(\"Male\", \"Female\"))                   # Gender\n\n# Step 1: Ensure Q4 is an ordered factor\ndata$Q4 &lt;- factor(data$Q4,\n                  levels = c(\"Never\", \"Rarely\", \"Sometimes\", \"Often\", \"Very often\"),\n                  ordered = TRUE)\n\n\ndata$Q4 &lt;- factor(data$Q4,\n                  levels = c(\"Never\", \"Rarely\", \"Sometimes\", \"Often\", \"Very often\"),\n                  ordered = TRUE)\n\nQ4_likert_scale &lt;- c(\"Never\"=1, \"Rarely\"=2, \"Sometimes\"=3, \"Often\"=4, \"Very often\"=5)\n\n\n\n# Convert Likert items (Q5.*, Q6.*, Q7.*, Q8.*, Q9.*) to ordered factors\nlikert_levels &lt;- c(\"Strongly disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly agree\")\n\nfor (col in grep(\"^Q[5-9]\\\\.\", names(data), value = TRUE)) {\n  data[[col]] &lt;- ordered(data[[col]], levels = likert_levels)\n}\n\n\n\n\n\n# Define Likert scale\nlikert_scale &lt;- c(\"Strongly Disagree\"=1, \"Disagree\"=2, \"Neutral\"=3, \"Agree\"=4, \"Strongly Agree\"=5)\n\n# Optional: Check structure\nstr(data)\n\n'data.frame':   159 obs. of  34 variables:\n $ Timestamp: chr  \"5.14.2023 21:22:19\" \"5.15.2023 8:45:52\" \"5.15.2023 9:54:15\" \"5.15.2023 15:17:41\" ...\n $ Q1       : Ord.factor w/ 2 levels \"Bachelor\"&lt;\"Master\": 2 1 1 1 1 1 1 2 1 1 ...\n $ Q2       : Factor w/ 6 levels \"Finance\",\"Marketing\",..: 4 NA 4 4 4 4 4 6 6 2 ...\n $ Q3       : Ord.factor w/ 2 levels \"Male\"&lt;\"Female\": 2 2 2 2 2 1 1 1 1 2 ...\n $ Q4       : Ord.factor w/ 5 levels \"Never\"&lt;\"Rarely\"&lt;..: 4 3 2 2 3 2 3 2 3 4 ...\n $ Q5.1     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 2 3 4 4 NA 3 2 3 4 ...\n $ Q5.2     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 4 3 4 4 4 4 4 4 2 ...\n $ Q5.3     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA 2 3 4 3 4 4 4 4 3 ...\n $ Q5.4     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA NA 2 2 2 2 3 3 3 NA ...\n $ Q5.5     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 3 3 4 4 4 3 2 3 3 ...\n $ Q5.6     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 3 4 4 4 4 NA NA NA NA 4 ...\n $ Q6.1     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 NA 3 NA 4 4 4 NA NA 4 ...\n $ Q6.2     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA 3 3 3 4 4 4 NA NA 3 ...\n $ Q6.3     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 3 4 4 4 4 3 3 NA NA 3 ...\n $ Q6.4     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 NA 4 NA 4 4 NA NA NA NA ...\n $ Q6.5     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA NA 2 3 3 2 3 NA 4 3 ...\n $ Q6.6     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA 4 4 4 3 NA NA NA 4 3 ...\n $ Q6.7     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA NA 4 NA 4 4 4 NA NA NA ...\n $ Q6.8     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 2 4 4 4 4 3 3 NA 3 4 ...\n $ Q7.1     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 3 3 2 4 3 NA 4 NA 2 3 ...\n $ Q7.2     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 2 3 2 4 2 3 2 NA 2 3 ...\n $ Q7.3     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 2 2 2 4 2 2 NA 2 3 ...\n $ Q7.4     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA NA NA 4 3 NA 4 4 3 4 ...\n $ Q8.1     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 4 4 4 3 NA 4 3 3 4 ...\n $ Q8.2     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA 4 4 4 4 4 4 4 4 4 ...\n $ Q8.3     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA 4 4 4 4 3 4 4 4 4 ...\n $ Q8.4     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 2 3 4 3 2 3 4 3 2 ...\n $ Q8.5     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA 3 3 4 3 NA 3 3 3 3 ...\n $ Q9.1     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 3 3 4 3 3 4 4 4 3 ...\n $ Q9.2     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA 4 3 4 3 4 3 3 3 4 ...\n $ Q9.3     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: NA 3 2 3 3 3 3 3 3 3 ...\n $ Q9.4     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 2 2 4 3 NA 4 4 3 3 ...\n $ Q9.5     : Ord.factor w/ 5 levels \"Strongly disagree\"&lt;..: 4 2 3 4 3 2 3 3 2 3 ...\n $ Q10      : chr  \"\" \"\" \"\" \"\" ...\n\n\n\n# --------------------------------------\n# 1. Demographics\n# --------------------------------------\n# Q1: Level of education\ncat(\"Education Levels:\\n\")\n\nEducation Levels:\n\nprint(table(data$Q1))\n\n\nBachelor   Master \n     128        3 \n\ndata$Q1_likert &lt;- Q1_likert_scale[as.character(data$Q1)]\n#print(prop.table(table(data$Q1)))\n#par(mfrow=c(1,1))  # Reset for the each bar chart\n\n# Q2: Field of study\ncat(\"\\nField of Study:\\n\")\n\n\nField of Study:\n\nprint(table(data$Q2))\n\n\n                         Finance                        Marketing \n                              23                               28 \n                      Accounting International Economic Relations \n                              17                               14 \n          Economics and Business                        Economics \n                              12                                2 \n\n#par(mfrow=c(1,1))  # Reset for the each bar chart\n\n# Gender Distribution (Q3)\ncat(\"\\nGender Distribution:\\n\")\n\n\nGender Distribution:\n\nprint(table(data$Q3))  # Frequency of each gender\n\n\n  Male Female \n    22    108 \n\ndata$Q3_likert &lt;- Q3_likert_scale[as.character(data$Q3)]\n#barplot(table(data$Q3), main = \"Gender Distribution\", col = \"lightgreen\")\n#par(mfrow=c(1,1))  # Reset for the each bar chart\n\nnames(data)\n\n [1] \"Timestamp\" \"Q1\"        \"Q2\"        \"Q3\"        \"Q4\"        \"Q5.1\"     \n [7] \"Q5.2\"      \"Q5.3\"      \"Q5.4\"      \"Q5.5\"      \"Q5.6\"      \"Q6.1\"     \n[13] \"Q6.2\"      \"Q6.3\"      \"Q6.4\"      \"Q6.5\"      \"Q6.6\"      \"Q6.7\"     \n[19] \"Q6.8\"      \"Q7.1\"      \"Q7.2\"      \"Q7.3\"      \"Q7.4\"      \"Q8.1\"     \n[25] \"Q8.2\"      \"Q8.3\"      \"Q8.4\"      \"Q8.5\"      \"Q9.1\"      \"Q9.2\"     \n[31] \"Q9.3\"      \"Q9.4\"      \"Q9.5\"      \"Q10\"       \"Q1_likert\" \"Q3_likert\"\n\n\n\n\nGeneral Visualizations\n\n# Create a new numeric variable using the mapping\ndata$Q4_likert &lt;- Q4_likert_scale[as.character(data$Q4)]\nhist(data$Q4_likert,\n     breaks = 5,\n     col = \"lightgreen\",\n     main = \"Distribution of AI Use Likert Scores\",\n     xlab = \"Likert Scale (1 = Never, 5 = Very often)\",\n     ylab = \"Count\")\n\n\n\n\n\n\n\n# --------------------------------------\n# 3. Trust in AI - Q5.1 to Q5.6\n# --------------------------------------\n\n# Frequency of AI Use (Q5)\nknowledge_items &lt;- grep(\"^Q5\\\\.\", names(data), value = TRUE)\n\n# Generate the graph for each sub questions\npar(mfrow=c(3,2))  # two rows and three columns\n\nlabels &lt;- c(\n  \"Q5.1\" = \"Q5.1 Complexity Limitations\",\n  \"Q5.2\" = \"Q5.2 Factual Inacurracy\",\n  \"Q5.3\" = \"Q5.3 Context Issues\",\n  \"Q5.4\" = \"Q5.4 Bias/Unfairness\",\n  \"Q5.5\" = \"Q5.5 Statistical Limits\",\n  \"Q5.6\" = \"Q5.6 Lack of Empathy\"\n)\n\n\nfor (item in knowledge_items) {\n  \n  barplot(table(data[[item]]),\n          main = labels[item],\n          col = \"lightblue\",\n          ylab = \"Count\",\n          las = 2)\n  \n}\n\n\n\n\n\n\n\npar(mfrow=c(1,1))  # Reset for the each bar chart\n\n\n# --------------------------------------\n# 5. Ethical Concerns - Q7.* and Q9.*\n# --------------------------------------\n#\nethics_items &lt;- grep(\"^Q7\\\\.|^Q9\\\\.\", names(data), value = TRUE)\n\nlikert_scale &lt;- c(\"Strongly Disagree\"=1, \"Disagree\"=2, \"Neutral\"=3, \"Agree\"=4, \"Strongly Agree\"=5)\n\ndata_numeric &lt;- data %&gt;%\n  mutate(across(all_of(ethics_items), ~ likert_scale[.])) %&gt;%\n  mutate(ethics_index = rowMeans(select(., all_of(ethics_items)), na.rm = TRUE))\n\ncat(\"Summary of Ethical Concern Items:\\n\")\n\nSummary of Ethical Concern Items:\n\nprint(summary(data_numeric[ethics_items]))\n\n      Q7.1            Q7.2            Q7.3           Q7.4            Q9.1      \n Min.   :2.000   Min.   :2.000   Min.   :2.00   Min.   :2.000   Min.   :2.000  \n 1st Qu.:3.000   1st Qu.:2.000   1st Qu.:2.00   1st Qu.:3.000   1st Qu.:3.000  \n Median :3.000   Median :3.000   Median :3.00   Median :4.000   Median :3.000  \n Mean   :3.092   Mean   :3.108   Mean   :3.12   Mean   :3.292   Mean   :3.352  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.00   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :4.000   Max.   :4.000   Max.   :4.00   Max.   :4.000   Max.   :4.000  \n NA's   :50      NA's   :57      NA's   :59     NA's   :70      NA's   :37     \n      Q9.2            Q9.3            Q9.4            Q9.5      \n Min.   :2.000   Min.   :2.000   Min.   :2.000   Min.   :2.000  \n 1st Qu.:3.000   1st Qu.:3.000   1st Qu.:3.000   1st Qu.:3.000  \n Median :4.000   Median :3.000   Median :3.000   Median :3.000  \n Mean   :3.422   Mean   :3.208   Mean   :3.322   Mean   :3.133  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n NA's   :43      NA's   :34      NA's   :41      NA's   :46     \n\ncat(\"\\nEthics Index Summary:\\n\")\n\n\nEthics Index Summary:\n\nprint(summary(data_numeric$ethics_index))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  2.000   3.000   3.183   3.221   3.444   4.000      29 \n\npar(mfrow = c(3, 3))  # layout for multiple plots\n\n\nlabels &lt;- c(\n  \"Q7.1\" = \"Q7.1 Undermines Education\",\n  \"Q7.2\" = \"Q7.2 Limits Socialization\",\n  \"Q7.3\" = \"Q7.3 Hinders Skill Development\",\n  \"Q7.4\" = \"Q7.4 Over-Reliance\",\n  \"Q9.1\" = \"Q9.1 Accuracy and Transparency\",\n  \"Q9.2\" = \"Q9.2 Privacy and ethical issues\",\n  \"Q9.3\" = \"Q9.3 Holistic Competencies\",\n  \"Q9.4\" = \"Q9.4 Career Prospects\",\n  \"Q9.5\" = \"Q9.5 Human Values\"\n)\n\nfor (item in ethics_items) {\n  \n  barplot(table(data[[item]]),\n          main = labels[item],\n          col = \"pink\",\n          las = 2)\n}\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))  # reset layout\n\n\n\n# --------------------------------------\n# 6. Ethical Concerns - Q8.1 to Q8.5\n# --------------------------------------\n#\n\n\nbenefit_items &lt;- grep(\"^Q8\\\\.\", names(data), value = TRUE)\n\nlikert_scale &lt;- c(\"Strongly Disagree\"=1, \"Disagree\"=2, \"Neutral\"=3, \"Agree\"=4, \"Strongly Agree\"=5)\n\ndata_numeric &lt;- data %&gt;%\n  mutate(across(all_of(benefit_items), ~ likert_scale[.])) %&gt;%\n  mutate(benefit_index = rowMeans(select(., all_of(benefit_items)), na.rm = TRUE))\n\ncat(\"Summary of Ethical Concern Items:\\n\")\n\nSummary of Ethical Concern Items:\n\nprint(summary(data_numeric[benefit_items]))\n\n      Q8.1            Q8.2            Q8.3            Q8.4            Q8.5     \n Min.   :2.000   Min.   :2.000   Min.   :2.000   Min.   :2.000   Min.   :2.00  \n 1st Qu.:4.000   1st Qu.:4.000   1st Qu.:4.000   1st Qu.:3.000   1st Qu.:3.00  \n Median :4.000   Median :4.000   Median :4.000   Median :3.000   Median :3.00  \n Mean   :3.721   Mean   :3.699   Mean   :3.693   Mean   :3.243   Mean   :3.35  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.00  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.00  \n NA's   :55      NA's   :56      NA's   :58      NA's   :44      NA's   :42    \n\ncat(\"\\nEthics Index Summary:\\n\")\n\n\nEthics Index Summary:\n\nprint(summary(data_numeric$benefit_index))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  2.200   3.200   3.600   3.515   4.000   4.000      32 \n\npar(mfrow = c(3, 2))  # layout for multiple plots\n\n\nlabels &lt;- c(\n  \"Q8.1\" = \"Q8.1 Learning Support\",\n  \"Q8.2\" = \"Q8.2 Writing Support\",\n  \"Q8.3\" = \"Q8.3 Research Support\",\n  \"Q8.4\" = \"Q8.4 Media Support\",\n  \"Q8.5\" = \"Q8.5 Administrative Support\"\n)\n\nfor (item in benefit_items) {\n  \n  barplot(table(data[[item]]),\n          main = labels[item],\n          col = \"pink\",\n          las = 2)\n}\n\npar(mfrow = c(1, 1))  # reset layout"
  },
  {
    "objectID": "603/Final_Project_part1.html#hypothesis-testing",
    "href": "603/Final_Project_part1.html#hypothesis-testing",
    "title": "603_Final_Project",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nModel Comparisons\nThe analysis included four linear regression models to test the studyâ€™s hypotheses.\n\nModel 1 examined the relationship between studentsâ€™ frequency of generative AI use (Q4) and their self-reported AI knowledge (Q5).\nModel 2 assessed whether students from different academic fields (Q2) differ in their ethical concerns about generative AI (Q7+Q9).\nModel 3 extended Model 1 by adding control variables such as academic level (Q1), gender (Q3), and trust in AI (Q8).\nModel 4 extended Model 2 by including additional control variables: academic level (Q1), gender (Q3), and trust in AI (Q8).\n\nThe resulting p-values for each model were as follows:\n\nModel 1: 0.7718\nModel 2: 0.1848\nModel 3: 0.8886\nModel 4: 0.0295\n\nThese values indicate that only Model 4 showed a statistically significant relationship, suggesting that academic field and background characteristics influence studentsâ€™ ethical concerns about AI.\n\nmodel_data &lt;- data %&gt;%\n  # Recode Likert items\n  mutate(across(c(starts_with(\"Q5.\"), starts_with(\"Q7.\"), starts_with(\"Q8.\"), starts_with(\"Q9.\")), ~ likert_scale[.])) %&gt;%\n  \n  # Create indices\n  mutate(\n    knowledge_index = rowMeans(select(., starts_with(\"Q5.\")), na.rm = TRUE),\n    ethics_index = rowMeans(select(., matches(\"^Q7\\\\.|^Q9\\\\.\")), na.rm = TRUE),\n    benefit_index = rowMeans(select(., starts_with(\"Q8.\")), na.rm = TRUE),\n    Q1_likert_index = rowMeans(select(., starts_with(\"Q1.\")), na.rm = TRUE),\n    Q3_likert_index = rowMeans(select(., starts_with(\"Q3.\")), na.rm = TRUE),\n    Q4_likert_index = rowMeans(select(., starts_with(\"Q4.\")), na.rm = TRUE)\n  ) %&gt;%\n  \n  # Select modeling variables\n  select(knowledge_index, ethics_index, benefit_index, Q1_likert, Q2, Q3_likert, Q4_likert) %&gt;%\n  na.omit()\n#data$Q2 &lt;- relevel(data$Q2, ref = \"Finance\")\n\n# Histogram for knowledge_index2\nggplot(model_data, aes(x = knowledge_index)) +\n  geom_histogram(binwidth = 0.5, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of AI Knowledge Index\",\n       x = \"The Mean of the Question in Knowledge Index\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram for ethics_index2\nggplot(model_data, aes(x = ethics_index)) +\n  geom_histogram(binwidth = 0.5, fill = \"darkgreen\", color = \"white\") +\n  labs(title = \"Distribution of AI Ethical Concern Index\",\n       x = \"The Mean of the Question in Ethics Index\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 2. Fit models using the same dataset\nmodel1 &lt;- lm(knowledge_index ~ Q4_likert, data = model_data) \nmodel2 &lt;- lm(ethics_index ~ Q2, data = model_data) \nmodel3 &lt;- lm(knowledge_index ~ Q4_likert + Q1_likert + Q3_likert + benefit_index, data = model_data)\nmodel4 &lt;- lm(ethics_index ~ Q2 + Q1_likert + Q3_likert + benefit_index, data = model_data)\n\n\n\nsummary(model1)\n\n\nCall:\nlm(formula = knowledge_index ~ Q4_likert, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23122 -0.23122 -0.04251  0.28347  0.79817 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.26062    0.15730  20.729   &lt;2e-16 ***\nQ4_likert   -0.01470    0.05052  -0.291    0.772    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4403 on 90 degrees of freedom\nMultiple R-squared:  0.0009396, Adjusted R-squared:  -0.01016 \nF-statistic: 0.08464 on 1 and 90 DF,  p-value: 0.7718\n\nsummary(model2)\n\n\nCall:\nlm(formula = ethics_index ~ Q2, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.13413 -0.24005 -0.02524  0.26087  0.86587 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         3.150242   0.082620  38.129   &lt;2e-16 ***\nQ2Marketing                         0.077490   0.113422   0.683   0.4963    \nQ2Accounting                       -0.016115   0.126734  -0.127   0.8991    \nQ2International Economic Relations  0.003694   0.141101   0.026   0.9792    \nQ2Economics and Business            0.341822   0.141101   2.423   0.0175 *  \nQ2Economics                        -0.066908   0.292106  -0.229   0.8194    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3962 on 86 degrees of freedom\nMultiple R-squared:  0.08237,   Adjusted R-squared:  0.02902 \nF-statistic: 1.544 on 5 and 86 DF,  p-value: 0.1848\n\nsummary(model3)\n\n\nCall:\nlm(formula = knowledge_index ~ Q4_likert + Q1_likert + Q3_likert + \n    benefit_index, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.18653 -0.24052 -0.02855  0.30634  0.82888 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.06722    0.38154   8.039 4.12e-12 ***\nQ4_likert     -0.02306    0.05192  -0.444    0.658    \nQ1_likert      0.11703    0.32253   0.363    0.718    \nQ3_likert     -0.06571    0.12160  -0.540    0.590    \nbenefit_index  0.07705    0.10279   0.750    0.456    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4451 on 87 degrees of freedom\nMultiple R-squared:  0.01282,   Adjusted R-squared:  -0.03257 \nF-statistic: 0.2825 on 4 and 87 DF,  p-value: 0.8886\n\nsummary(model4)\n\n\nCall:\nlm(formula = ethics_index ~ Q2 + Q1_likert + Q3_likert + benefit_index, \n    data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.04588 -0.21387 -0.03236  0.21835  0.78834 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         2.34767    0.32797   7.158    3e-10 ***\nQ2Marketing                         0.05183    0.10958   0.473  0.63742    \nQ2Accounting                        0.01033    0.12236   0.084  0.93294    \nQ2International Economic Relations -0.02668    0.14066  -0.190  0.85002    \nQ2Economics and Business            0.31177    0.13687   2.278  0.02530 *  \nQ2Economics                        -0.30659    0.34168  -0.897  0.37216    \nQ1_likert                           0.43209    0.32390   1.334  0.18585    \nQ3_likert                          -0.02030    0.11085  -0.183  0.85517    \nbenefit_index                       0.23606    0.08883   2.657  0.00944 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3813 on 83 degrees of freedom\nMultiple R-squared:  0.1801,    Adjusted R-squared:  0.101 \nF-statistic: 2.278 on 8 and 83 DF,  p-value: 0.02952\n\n# --------------------------------------\n# Question 5: Standardized Regression\n# --------------------------------------\n\n# Standardize continuous variables only\nmodel_data$Z_Q1  &lt;- scale(model_data$Q1_likert)\nmodel_data$Z_Q3  &lt;- scale(model_data$Q3_likert)\nmodel_data$Z_Q4  &lt;- scale(model_data$Q4_likert)\nmodel_data$Z_benefit &lt;- scale(model_data$benefit_index)\n\n# Standardize dependent variable\nmodel_data$Z_ethics &lt;- scale(model_data$ethics_index)\n\n# Run standardized Model 4\nmodel4_std &lt;- lm(Z_ethics ~ Q2 + Z_Q1 + Z_Q3 + Z_Q4 + Z_benefit, data = model_data)\n\nsummary(model4_std)\n\n\nCall:\nlm(formula = Z_ethics ~ Q2 + Z_Q1 + Z_Q3 + Z_Q4 + Z_benefit, \n    data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.51453 -0.57137 -0.05285  0.53194  2.00841 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)                        -0.132265   0.200068  -0.661   0.5104  \nQ2Marketing                         0.136491   0.272918   0.500   0.6183  \nQ2Accounting                        0.030716   0.304655   0.101   0.9199  \nQ2International Economic Relations -0.028740   0.352613  -0.082   0.9352  \nQ2Economics and Business            0.810208   0.342880   2.363   0.0205 *\nQ2Economics                        -0.640099   0.861208  -0.743   0.4595  \nZ_Q1                                0.150996   0.118455   1.275   0.2060  \nZ_Q3                               -0.002198   0.109405  -0.020   0.9840  \nZ_Q4                                0.093721   0.103141   0.909   0.3662  \nZ_benefit                           0.257675   0.103390   2.492   0.0147 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9491 on 82 degrees of freedom\nMultiple R-squared:  0.1882,    Adjusted R-squared:  0.09913 \nF-statistic: 2.113 on 9 and 82 DF,  p-value: 0.03745\n\nstargazer(model4_std,\n          title = \"Standardized Model 4 Results\",\n          type = \"html\",\n          out = \"standardized_model4_results.html\")\n\n\n&lt;table style=\"text-align:center\"&gt;&lt;caption&gt;&lt;strong&gt;Standardized Model 4 Results&lt;/strong&gt;&lt;/caption&gt;\n&lt;tr&gt;&lt;td colspan=\"2\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;em&gt;Dependent variable:&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan=\"1\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;Z_ethics&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td colspan=\"2\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q2Marketing&lt;/td&gt;&lt;td&gt;0.136&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.273)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q2Accounting&lt;/td&gt;&lt;td&gt;0.031&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.305)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q2International Economic Relations&lt;/td&gt;&lt;td&gt;-0.029&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.353)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q2Economics and Business&lt;/td&gt;&lt;td&gt;0.810&lt;sup&gt;**&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.343)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q2Economics&lt;/td&gt;&lt;td&gt;-0.640&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.861)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Z_Q1&lt;/td&gt;&lt;td&gt;0.151&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.118)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Z_Q3&lt;/td&gt;&lt;td&gt;-0.002&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.109)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Z_Q4&lt;/td&gt;&lt;td&gt;0.094&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.103)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Z_benefit&lt;/td&gt;&lt;td&gt;0.258&lt;sup&gt;**&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.103)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Constant&lt;/td&gt;&lt;td&gt;-0.132&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.200)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td colspan=\"2\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Observations&lt;/td&gt;&lt;td&gt;92&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.188&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Adjusted R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.099&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Residual Std. Error&lt;/td&gt;&lt;td&gt;0.949 (df = 82)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;F Statistic&lt;/td&gt;&lt;td&gt;2.113&lt;sup&gt;**&lt;/sup&gt; (df = 9; 82)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td colspan=\"2\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;em&gt;Note:&lt;/em&gt;&lt;/td&gt;&lt;td style=\"text-align:right\"&gt;&lt;sup&gt;*&lt;/sup&gt;p&lt;0.1; &lt;sup&gt;**&lt;/sup&gt;p&lt;0.05; &lt;sup&gt;***&lt;/sup&gt;p&lt;0.01&lt;/td&gt;&lt;/tr&gt;\n&lt;/table&gt;\n\nls()  # should include model_data\n\n [1] \"benefit_items\"   \"col\"             \"data\"            \"data_numeric\"   \n [5] \"ethics_items\"    \"item\"            \"knowledge_items\" \"labels\"         \n [9] \"likert_levels\"   \"likert_scale\"    \"model_data\"      \"model1\"         \n[13] \"model2\"          \"model3\"          \"model4\"          \"model4_std\"     \n[17] \"Q1_likert_scale\" \"Q3_likert_scale\" \"Q4_likert_scale\"\n\nstr(model_data)  # check its structure\n\n'data.frame':   92 obs. of  12 variables:\n $ knowledge_index: num  3.75 3 3.67 3.5 3.5 ...\n $ ethics_index   : num  3.5 2.38 3.67 3 2.83 ...\n $ benefit_index  : num  4 3.6 4 3.4 3 3.6 3.6 3.4 3.4 3.25 ...\n $ Q1_likert      : num  1 0 0 0 0 0 1 0 0 0 ...\n $ Q2             : Factor w/ 6 levels \"Finance\",\"Marketing\",..: 4 4 4 4 4 4 6 6 2 2 ...\n $ Q3_likert      : num  1 1 1 1 0 0 0 0 1 1 ...\n $ Q4_likert      : num  4 2 2 3 2 3 2 3 4 2 ...\n $ Z_Q1           : num [1:92, 1] 6.672 -0.148 -0.148 -0.148 -0.148 ...\n  ..- attr(*, \"scaled:center\")= num 0.0217\n  ..- attr(*, \"scaled:scale\")= num 0.147\n $ Z_Q3           : num [1:92, 1] 0.474 0.474 0.474 0.474 -2.089 ...\n  ..- attr(*, \"scaled:center\")= num 0.815\n  ..- attr(*, \"scaled:scale\")= num 0.39\n $ Z_Q4           : num [1:92, 1] 1.1184 -1.0708 -1.0708 0.0238 -1.0708 ...\n  ..- attr(*, \"scaled:center\")= num 2.98\n  ..- attr(*, \"scaled:scale\")= num 0.914\n $ Z_benefit      : num [1:92, 1] 1.091 0.226 1.091 -0.207 -1.072 ...\n  ..- attr(*, \"scaled:center\")= num 3.5\n  ..- attr(*, \"scaled:scale\")= num 0.462\n $ Z_ethics       : num [1:92, 1] 0.714 -2.083 1.129 -0.529 -0.944 ...\n  ..- attr(*, \"scaled:center\")= num 3.21\n  ..- attr(*, \"scaled:scale\")= num 0.402\n - attr(*, \"na.action\")= 'omit' Named int [1:67] 2 22 28 29 30 33 35 39 41 42 ...\n  ..- attr(*, \"names\")= chr [1:67] \"2\" \"22\" \"28\" \"29\" ...\n\n# Fit the model for ethics_index vs benefit_index\nmodel &lt;- lm(ethics_index ~ benefit_index, data = model_data)\nsummary(model)\n\n\nCall:\nlm(formula = ethics_index ~ benefit_index, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.07754 -0.22078  0.00347  0.20517  0.75875 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.25897    0.30683   7.362 8.23e-11 ***\nbenefit_index  0.27285    0.08703   3.135  0.00232 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3839 on 90 degrees of freedom\nMultiple R-squared:  0.09847,   Adjusted R-squared:  0.08845 \nF-statistic:  9.83 on 1 and 90 DF,  p-value: 0.002319\n\n# Create a sequence of hypothetical benefit_index values\nnew_data &lt;- data.frame(\n  benefit_index = seq(\n    min(model_data$benefit_index, na.rm = TRUE),\n    max(model_data$benefit_index, na.rm = TRUE),\n    length.out = 100\n  )\n)\n\n# Predict ethics_index with 95% prediction interval\npred &lt;- predict(model, newdata = new_data, interval = \"prediction\", level = 0.95)\n\n# Add predictions to new_data\nhop_data &lt;- new_data %&gt;%\n  mutate(\n    predicted = pred[, \"fit\"],\n    lower = pred[, \"lwr\"],\n    upper = pred[, \"upr\"]\n  )\n\n# Simulate hypothetical outcomes\nset.seed(1234)\nn_sim &lt;- 100\nsimulated &lt;- hop_data %&gt;%\n  rowwise() %&gt;%\n  do(data.frame(\n    benefit_index = .$benefit_index,\n    outcome = rnorm(n_sim, mean = .$predicted, sd = (.$upper - .$predicted)/2),\n    sim_id = 1:n_sim\n  ))\n\n# Animated HOP\ng &lt;- ggplot() +\n  geom_line(data = hop_data, aes(x = benefit_index, y = predicted), linewidth = 1) +\n  geom_ribbon(data = hop_data, aes(x = benefit_index, ymin = lower, ymax = upper), alpha = 0.2) +\n  geom_point(data = simulated, aes(x = benefit_index, y = outcome), size = 1, alpha = 0.5) +\n  labs(\n    title = \"Hypothetical Outcome Plot (HOP): Effect of Perceived Academic Benefit on Ethical Concerns\",\n    subtitle = \"Simulation {closest_state} of 100\",\n    x = \"Perceived Academic Benefit (benefit_index)\",\n    y = \"Ethical Concerns (ethics_index)\"\n  ) +\n  theme_minimal() +\n  transition_states(sim_id, state_length = 1)\n\nanimate(g, nframes = 100, fps = 10)\n\n\n\n\n\n\n\n\n\n\nDiagnostics(AIC n BIC make sure there are the same observations before run the function)\n\n#The table will be presented in regression_results.html\nstargazer(model1, model2, model3, model4, title=\"Results\",\n          type = \"html\",\n          out = \"regression_results.html\")\n\n\n&lt;table style=\"text-align:center\"&gt;&lt;caption&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/caption&gt;\n&lt;tr&gt;&lt;td colspan=\"5\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td colspan=\"4\"&gt;&lt;em&gt;Dependent variable:&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan=\"4\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;knowledge_index&lt;/td&gt;&lt;td&gt;ethics_index&lt;/td&gt;&lt;td&gt;knowledge_index&lt;/td&gt;&lt;td&gt;ethics_index&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(1)&lt;/td&gt;&lt;td&gt;(2)&lt;/td&gt;&lt;td&gt;(3)&lt;/td&gt;&lt;td&gt;(4)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td colspan=\"5\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q4_likert&lt;/td&gt;&lt;td&gt;-0.015&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;-0.023&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.051)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.052)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q2Marketing&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;0.077&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;0.052&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.113)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.110)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q2Accounting&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;-0.016&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;0.010&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.127)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.122)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q2International Economic Relations&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;0.004&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;-0.027&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.141)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.141)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q2Economics and Business&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;0.342&lt;sup&gt;**&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;0.312&lt;sup&gt;**&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.141)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.137)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q2Economics&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;-0.067&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;-0.307&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.292)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.342)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q1_likert&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;0.117&lt;/td&gt;&lt;td&gt;0.432&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.323)&lt;/td&gt;&lt;td&gt;(0.324)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Q3_likert&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;-0.066&lt;/td&gt;&lt;td&gt;-0.020&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.122)&lt;/td&gt;&lt;td&gt;(0.111)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;benefit_index&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;0.077&lt;/td&gt;&lt;td&gt;0.236&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(0.103)&lt;/td&gt;&lt;td&gt;(0.089)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Constant&lt;/td&gt;&lt;td&gt;3.261&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;3.150&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;3.067&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;2.348&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;(0.157)&lt;/td&gt;&lt;td&gt;(0.083)&lt;/td&gt;&lt;td&gt;(0.382)&lt;/td&gt;&lt;td&gt;(0.328)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td colspan=\"5\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Observations&lt;/td&gt;&lt;td&gt;92&lt;/td&gt;&lt;td&gt;92&lt;/td&gt;&lt;td&gt;92&lt;/td&gt;&lt;td&gt;92&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.001&lt;/td&gt;&lt;td&gt;0.082&lt;/td&gt;&lt;td&gt;0.013&lt;/td&gt;&lt;td&gt;0.180&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Adjusted R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;-0.010&lt;/td&gt;&lt;td&gt;0.029&lt;/td&gt;&lt;td&gt;-0.033&lt;/td&gt;&lt;td&gt;0.101&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Residual Std. Error&lt;/td&gt;&lt;td&gt;0.440 (df = 90)&lt;/td&gt;&lt;td&gt;0.396 (df = 86)&lt;/td&gt;&lt;td&gt;0.445 (df = 87)&lt;/td&gt;&lt;td&gt;0.381 (df = 83)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;F Statistic&lt;/td&gt;&lt;td&gt;0.085 (df = 1; 90)&lt;/td&gt;&lt;td&gt;1.544 (df = 5; 86)&lt;/td&gt;&lt;td&gt;0.283 (df = 4; 87)&lt;/td&gt;&lt;td&gt;2.278&lt;sup&gt;**&lt;/sup&gt; (df = 8; 83)&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td colspan=\"5\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;&lt;em&gt;Note:&lt;/em&gt;&lt;/td&gt;&lt;td colspan=\"4\" style=\"text-align:right\"&gt;&lt;sup&gt;*&lt;/sup&gt;p&lt;0.1; &lt;sup&gt;**&lt;/sup&gt;p&lt;0.05; &lt;sup&gt;***&lt;/sup&gt;p&lt;0.01&lt;/td&gt;&lt;/tr&gt;\n&lt;/table&gt;\n\n# Compare with AIC\nAIC(model1, model2, model3, model4)\n\n       df       AIC\nmodel1  3 114.11292\nmodel2  7  98.54133\nmodel3  6 119.01213\nmodel4 10  94.18592\n\nBIC(model1, model2, model3, model4)\n\n       df      BIC\nmodel1  3 121.6783\nmodel2  7 116.1938\nmodel3  6 134.1429\nmodel4 10 119.4038\n\nsummary(model1)$adj.r.squared\n\n[1] -0.01016112\n\nsummary(model2)$adj.r.squared\n\n[1] 0.02901927\n\nsummary(model3)$adj.r.squared\n\n[1] -0.0325653\n\nsummary(model4)$adj.r.squared\n\n[1] 0.1010256\n\n# Model 1\npar(mfrow = c(2, 2))\nplot(model1)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))  # Reset\n\n# Repeat for others:\n\npar(mfrow = c(2, 2))\nplot(model2)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))  # Reset\n\n\n\npar(mfrow = c(2, 2))\nplot(model3)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))  # Reset\n\n\npar(mfrow = c(2, 2))\nplot(model4)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))  # Reset"
  },
  {
    "objectID": "603/Final_Project_part1.html#conclution",
    "href": "603/Final_Project_part1.html#conclution",
    "title": "603_Final_Project",
    "section": "Conclution",
    "text": "Conclution\nThis study examined the relationship between studentsâ€™ generative AI usage and their knowledge and ethical perceptions through four regression models. Model 1, testing the hypothesis that frequent AI use is associated with greater AI knowledge, produced a p-value of 0.1244, indicating no statistically significant relationship at the 0.05 level. Model 2, assessing differences in ethical concerns across academic fields, also yielded a non-significant p-value (0.1848). When control variables such as academic level, gender, and trust were added in Model 3, the p-value increased to 0.3011, further supporting the null hypothesis for Hypothesis 1. However, Model 4, which included controls while testing Hypothesis 2, returned a statistically significant p-value of 0.0295, suggesting that field of study, along with other factors, does significantly relate to studentsâ€™ ethical concerns about AI. Overall, while no evidence was found to support the idea that frequent AI use improves AI knowledge, the results point to meaningful variation in ethical attitudes depending on studentsâ€™ academic backgrounds.\n\n# --------------------------------------\n# Question 5: Standardized Regression\n# --------------------------------------\n\n# Standardize continuous variables only\n#model_data$Z_Q1  &lt;- scale(model_data$Q1_likert)\n##model_data$Z_Q3  &lt;- scale(model_data$Q3_likert)\n##model_data$Z_Q4  &lt;- scale(model_data$Q4_likert)\n#model_data$Z_benefit &lt;- scale(model_data$benefit_index)\n\n# Standardize dependent variable\n#model_data$Z_ethics &lt;- scale(model_data$ethics_index)\n\n# Run standardized Model 4\n#model4_std &lt;- lm(Z_ethics ~ Q2 + Z_Q1 + Z_Q3 + Z_Q4 + Z_benefit, data = #model_data)\n\n#summary(model4_std)\n\n#stargazer(model4_std,\n#          title = \"Standardized Model 4 Results\",\n#          type = \"html\",\n#          out = \"standardized_model4_results.html\")#\n\n\n\n# --------------------------------------\n# Question 5 (Option 2): Hypothetical Outcome Plot (HOP)\n# --------------------------------------\n\n# Use Model 4 (your only significant model)\n#set.seed(123)\n\n# Create predicted values from the model\n#model4_pred &lt;- model4\n\n# Create a dataframe with predictions + random noise\n#hop_data &lt;- model_data %&gt;%\n#  mutate(\n#    predicted = predict(model4_pred),\n    # residual standard deviation from model\n#    sigma = summary(model4_pred)$sigma,\n    # simulated outcomes (prediction + random error)\n#    simulated_outcome = predicted + rnorm(n(), mean = 0, sd = sigma)\n#  )\n\n# Plot the Hypothetical Outcome Plot\n#ggplot(hop_data, aes(x = benefit_index, y = simulated_outcome)) +\n#  geom_point(alpha = 0.2) +\n#  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1.2) +\n#  labs(title = \"Hypothetical Outcome Plot (HOP)\",\n#       subtitle = \"Simulated outcomes based on Model 4 uncertainty\",\n#       x = \"Benefit Index\",\n#       y = \"Simulated Ethics Index\") +\n#  theme_minimal()\n\n\n\n# 1. Fit your model (example uses ethics_index)\n#model &lt;- lm(ethics_index ~ benefit_index, data = model_data)\n\n# 2. Create a sequence of hypothetical benefit_index values\n##new_data &lt;- data.frame(\n#  benefit_index = seq(\n#    min(model_data$benefit_index, na.rm = TRUE),\n#    max(model_data$benefit_index, na.rm = TRUE),\n#    length.out = 100\n#  )\n#)\n\n# 3. Predict ethics_index with uncertainty\n#pred &lt;- predict(model, newdata = new_data, interval = \"prediction\", level = #0.95)\n\n# Add predictions to new_data\n##hop_data &lt;- new_data %&gt;%\n#  mutate(\n#    predicted = pred[, \"fit\"],\n#    lower = pred[, \"lwr\"],\n#    upper = pred[, \"upr\"]\n#  )\n\n# 4. Simulate hypothetical outcomes\n#set.seed(1234)\n#n_sim &lt;- 100\n#simulated &lt;- hop_data %&gt;%\n#  rowwise() %&gt;%\n#  do(data.frame(\n#    benefit_index = .$benefit_index,\n#    outcome = rnorm(n_sim, mean = .$predicted, sd = (.$upper - #.$predicted)/2),\n#    sim_id = 1:n_sim\n#  ))\n\n# 5. Animated HOP\n##g &lt;- ggplot() +\n#  geom_line(data = hop_data, aes(x = benefit_index, y = predicted), linewidth #= 1) +\n#  geom_ribbon(data = hop_data, aes(x = benefit_index, ymin = lower, ymax = #upper), alpha = 0.2) +\n#  geom_point(data = simulated, aes(x = benefit_index, y = outcome), size = 1, #alpha = 0.5) +\n#  labs(\n#    title = \"Hypothetical Outcome Plot (HOP): Effect of Perceived Academic #Benefit on Ethical Concerns\",\n#    subtitle = \"Simulation {closest_state} of 100\",\n#    x = \"Perceived Academic Benefit (benefit_index)\",\n#    y = \"Ethical Concerns (ethics_index)\"\n#  ) +\n#  theme_minimal() +\n#  transition_states(sim_id, state_length = 1)#\n\n#animate(g, nframes = 100, fps = 10)"
  },
  {
    "objectID": "603/Final_Project_part1.html#reference-and-data",
    "href": "603/Final_Project_part1.html#reference-and-data",
    "title": "603_Final_Project",
    "section": "Reference and Data",
    "text": "Reference and Data\nChatbotsâ€™ Impact on University Learning https://www.kaggle.com/datasets/jocelyndumlao/chatbots-impact-on-university-learning?resource=download"
  },
  {
    "objectID": "602/602_GroupProjectReportTemplate_SP25.html",
    "href": "602/602_GroupProjectReportTemplate_SP25.html",
    "title": "Group 13 Perception of AI Decision Making",
    "section": "",
    "text": "Introduction\nThe experiment investigates how peopleâ€™s trust in decision-making varies across different risk contexts and decision agents involving artificial intelligence (AI). Participants are randomly assigned to one of six scenariosâ€”three high-risk (e.g., urgent medical decisions) and three low-risk (e.g., playlist creation, blog writing). In each scenario, they are asked who they would trust most to make the decision: a human alone, AI alone, or a form of AI-human collaboration. The study aims to understand how perceived risk levels and task types influence trust in AI, human, and hybrid decision-makers.\n\n\nMethodology\nThis study uses a between-subjects experimental survey design to examine how peopleâ€™s trust in decision-makers varies across different types of tasks and risk levels. A total of 279 participants were randomly assigned to one of six treatment conditions, each representing a unique scenario involving either a high-risk (e.g., medical diagnosis or urgent treatment decisions) or low-risk (e.g., playlist creation, blog writing, or appointment scheduling) situation. Within each scenario, participants were presented with three to five potential decision-makers: a human alone, an AI alone, or various forms of AI-human collaboration. The treatments vary both in the risk level (high vs.Â low) and in the task domain (e.g., healthcare vs.Â entertainment or routine logistics).\nThe main outcome variable, G13_DV, captures the participantâ€™s trust in the decision-maker, with response options ranging from full human control to full AI autonomy. Additional measures such as decision time and click count were collected using embedded Qualtrics timing variables to monitor engagement. Data cleaning procedures included filtering incomplete responses, handling missing values, and recoding scenario conditions into a unified variable (G13_RiskGroup) for analysis. Responses with invalid or blank trust answers were removed, and text responses were standardized using string matching functions. The cleaned dataset allowed for comparisons across scenarios to evaluate how task type and risk influence trust in AI and hybrid decision-making systems.\n\nRisk Groups Explanation\nIn this study, participants were exposed to different activities that were categorized into high-risk and low-risk conditions. Each condition involved different types of tasks that participants were asked to engage in, and the perception of risk was assessed based on their responses. These activities were framed in different ways to test how participants perceive risk in various contexts, particularly in relation to AI involvement.\n\n1. HighRisk_1:\nThis group involved high-risk scenarios, where participants were likely to face situations with significant stakes, either in terms of personal outcome, financial impact, or ethical concerns. These could have included tasks like decision-making in medical, legal, or financial contexts. The idea is that high-risk scenarios typically involve a greater perceived threat of negative outcomes or consequences. In this study, HighRisk_1 specifically involved situations where participants could experience serious consequences, with AI potentially playing a decision-making role in these high-stakes tasks.\n\n\n2. HighRisk_2:\nThis group involved another form of high-risk scenario, but the nature of the task may have differed slightly. The focus in this group was again on situations where the potential for harm or negative outcomes was elevated, though the specific task could have involved different types of challenges. HighRisk_2 was meant to examine risk perception in contexts that might be emotionally or socially risky for the participants, yet still involve high-stakes decisions. For example, it could have involved scenarios where participants had to make important judgments about peopleâ€™s lives, careers, or futures with AI assistance.\n\n\n3. HighRisk_3:\nThis third high-risk condition followed similar principles to the previous two, but likely involved a different type of high-risk activity, again with a focus on situations that could lead to severe outcomes. The goal was to assess whether the perception of risk in high-stakes environments differs based on the specific context and how AI interaction could alter risk assessments.\n\n\n4. LowRisk_SongColl (Low-Risk - Songwriting Collaboration):\nThis group involved low-risk activities, particularly focusing on a collaborative songwriting task. This scenario was framed as low-risk because there were no significant consequences involved, and participants were asked to collaborate with AI in a creative process. Despite being labeled â€œlow-risk,â€ the nature of the activity (involving personal creativity and AI collaboration) may have still led to heightened perceptions of risk for some participants, especially those concerned about AI â€œtaking overâ€ the creative process. The study aimed to explore whether collaborative, creative tasks were perceived as riskier, even though they were framed as low-risk tasks.\n\n\n5. LowRisk_Collab (Low-Risk - Collaborative Task):\nThis group also involved low-risk activities, but here, participants engaged in a different collaborative task, possibly involving teamwork with AI in a less creative context. The collaborative aspect was intended to emphasize cooperation with AI, rather than the AI acting as a sole decision-maker. This scenario examined how participants perceive risks when they work together with AI on tasks that are not inherently high-stakes but may still involve a degree of uncertainty or unpredictability.\n\n\n6. LowRisk_BlogWrit (Low-Risk - Blog Writing):\nIn this group, participants engaged in a low-risk activity involving AI-assisted blog writing. The task was framed as a simple, non-consequential task where participants could express their opinions with minimal risk of negative outcomes. While the activity was designed to be low-risk, the goal was to explore whether participants would perceive it differently, considering that their creative output might be influenced or shaped by AI suggestions. This scenario explored the balance between perceived control and the influence of AI on a personal task like writing.\n\n\n\n\nKey Points\n\nHigh-risk groups involve activities that are associated with significant consequences, either in real life or in perceived impact.\nLow-risk groups involve activities that are generally considered safe, but where the involvement of AI might still influence the participantsâ€™ perception of risk, particularly when personal creativity or judgment is involved.\nThe study aimed to compare how risk perception varies across tasks, including creative, decision-making, and collaborative activities, to determine how people assess risk when AI is involved in different types of tasks.\n\nBy examining these risk groups, the study sought to understand how contextual factors, like the nature of the task and the degree of AI involvement, affect individualsâ€™ perceptions of risk.\n\n\n\nAnalysis\nWe used independent-samples t-tests to compare participantsâ€™ trust in AI-human decision-makers across different risk conditions. The dependent variable is G13_DV_Likert, a Likert-scale measure (1 = Human alone to 5 = AI alone) indicating participantsâ€™ preferred decision-making agent. Each high-risk condition (HighRisk_1, HighRisk_2, HighRisk_3) was compared separately against each low-risk condition (LowRisk_SongColl, LowRisk_Collab, LowRisk_BlogWrit), resulting in multiple pairwise comparisons. This test is appropriate because we are comparing the means of a continuous outcome between two independent groups, and we checked the assumption of equal variances using F-tests before each t-test.\n\n# import libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tidyverse)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\nWarning: package 'stringr' was built under R version 4.4.3\n\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.2\nâœ” lubridate 1.9.4     âœ” tibble    3.2.1\nâœ” purrr     1.0.4     âœ” tidyr     1.3.1\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\n\n\n# import data\n# Read CSV file\ndata &lt;- read_csv(\"SP25_602_omnibus_V1_May8.csv\")\n\nRows: 279 Columns: 109\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (76): ResponseId, DistributionChannel, UserLanguage, Informed Consent , ...\ndbl (31): LocationLatitude, LocationLongitude, Q_RecaptchaScore, G2-Timer_Fi...\nlgl  (2): state, Create New Field or Choose From Dropdown...\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview first few rows\nglimpse(data)       # View the dataset structure (variables, types, and first few values)\n\nRows: 279\nColumns: 109\n$ ResponseId                                      &lt;chr&gt; \"R_5plgsxv5CHh8iUc\", \"â€¦\n$ LocationLatitude                                &lt;dbl&gt; 29.4551, 39.9230, 31.7â€¦\n$ LocationLongitude                               &lt;dbl&gt; -98.6498, -82.8664, -8â€¦\n$ DistributionChannel                             &lt;chr&gt; \"anonymous\", \"anonymouâ€¦\n$ UserLanguage                                    &lt;chr&gt; \"EN\", \"EN\", \"EN\", \"EN\"â€¦\n$ Q_RecaptchaScore                                &lt;dbl&gt; 0.8, 0.6, 1.0, 1.0, 0.â€¦\n$ `Informed ConsentÂ `                             &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"â€¦\n$ Gender                                          &lt;chr&gt; \"Male\", \"Female\", \"Malâ€¦\n$ Gender_4_TEXT                                   &lt;chr&gt; NA, NA, NA, NA, NA, NAâ€¦\n$ Age                                             &lt;chr&gt; \"1978\", \"1977\", \"1997\"â€¦\n$ Ethnicity                                       &lt;chr&gt; \"Two or more races\", \"â€¦\n$ `Hispanic or Latino`                            &lt;chr&gt; \"Hispanic or Latino\", â€¦\n$ Education                                       &lt;chr&gt; \"High school diploma oâ€¦\n$ Employment                                      &lt;chr&gt; \"Working part-time\", \"â€¦\n$ Income                                          &lt;chr&gt; \"Less than $25,000\", \"â€¦\n$ `Residency-State`                               &lt;chr&gt; \"Texas\", \"Ohio\", \"Missâ€¦\n$ `Residency-Community`                           &lt;chr&gt; \"Urban\", \"Suburban\", \"â€¦\n$ `Residency-Community_4_TEXT`                    &lt;chr&gt; NA, NA, NA, NA, NA, NAâ€¦\n$ `Political IdeologyÂ _1`                         &lt;chr&gt; \"Very Liberal\", \"Moderâ€¦\n$ `Party AffiliationÂ _1`                          &lt;chr&gt; \"Independent\", \"Indepeâ€¦\n$ Voting                                          &lt;chr&gt; \"Neither - voted for aâ€¦\n$ `G2-Timer_First Click`                          &lt;dbl&gt; 4.300, 1.601, 5.377, 2â€¦\n$ `G2-Timer_Last Click`                           &lt;dbl&gt; 22.403, 61.381, 51.131â€¦\n$ `G2-Timer_Page Submit`                          &lt;dbl&gt; 23.321, 62.366, 51.900â€¦\n$ `G2-Timer_Click Count`                          &lt;dbl&gt; 10, 39, 10, 14, 40, 8,â€¦\n$ `g2-field-study`                                &lt;chr&gt; \"Arts\", \"business admiâ€¦\n$ `g2-field-work`                                 &lt;chr&gt; \"Sell fruit\", \"businesâ€¦\n$ g2q2_1                                          &lt;chr&gt; \"Terrible\", \"Excellentâ€¦\n$ g2q2_2                                          &lt;chr&gt; \"Terrible\", \"Excellentâ€¦\n$ g2q3a                                           &lt;chr&gt; NA, NA, \"Natural brillâ€¦\n$ g2q3b                                           &lt;chr&gt; NA, \"External help (tuâ€¦\n$ g2q3c                                           &lt;chr&gt; \"Natural brilliance\", â€¦\n$ g2p4_1                                          &lt;chr&gt; \"Terrible\", \"Poor\", \"Gâ€¦\n$ g2p4_2                                          &lt;chr&gt; \"Terrible\", \"Poor\", \"Aâ€¦\n$ `G1-Timer_First Click`                          &lt;dbl&gt; 2.200, 1.001, 12.418, â€¦\n$ `G1-Timer_Last Click`                           &lt;dbl&gt; 18.861, 30.400, 44.879â€¦\n$ `G1-Timer_Page Submit`                          &lt;dbl&gt; 19.400, 31.800, 45.515â€¦\n$ `G1-Timer_Click Count`                          &lt;dbl&gt; 9, 21, 12, 11, 24, 12,â€¦\n$ G1_control                                      &lt;chr&gt; \"Neither support or opâ€¦\n$ G1_republican                                   &lt;chr&gt; NA, NA, \"Support\", \"Neâ€¦\n$ G1_democratic                                   &lt;chr&gt; NA, \"Neither support oâ€¦\n$ G1_Q2_allgroups                                 &lt;chr&gt; \"Republicans, by a litâ€¦\n$ G1_Q3_allgroups                                 &lt;chr&gt; \"$15\", \"3700\", \"100000â€¦\n$ G1_Q4_allgroups                                 &lt;chr&gt; \"No\", \"Yes\", \"No\", \"Noâ€¦\n$ `G3-Timer_First Click`                          &lt;dbl&gt; 0.798, 0.831, 1.261, 2â€¦\n$ `G3-Timer_Last Click`                           &lt;dbl&gt; 16.899, 32.296, 47.926â€¦\n$ `G3-Timer_Page Submit`                          &lt;dbl&gt; 17.600, 32.900, 49.300â€¦\n$ `G3-Timer_Click Count`                          &lt;dbl&gt; 10, 21, 12, 10, 61, 24â€¦\n$ g3q1_1                                          &lt;chr&gt; \"To a very small extenâ€¦\n$ g3q1_2                                          &lt;chr&gt; \"To a very small extenâ€¦\n$ g3q1_3                                          &lt;chr&gt; \"To a moderate extent\"â€¦\n$ g3q3                                            &lt;chr&gt; \"Yes, I have close famâ€¦\n$ `g3q2-attention check`                          &lt;chr&gt; \"An immigrant who is aâ€¦\n$ `G5_timer_First Click`                          &lt;dbl&gt; 0.930, 0.964, 6.563, 3â€¦\n$ `G5_timer_Last Click`                           &lt;dbl&gt; 19.672, 35.696, 55.773â€¦\n$ `G5_timer_Page Submit`                          &lt;dbl&gt; 20.596, 36.435, 57.170â€¦\n$ `G5_timer_Click Count`                          &lt;dbl&gt; 14, 31, 25, 15, 31, 29â€¦\n$ G5q1_trust_headline                             &lt;chr&gt; \"Trust\", \"Strongly Disâ€¦\n$ G5q2_successful                                 &lt;chr&gt; \"Likely\", \"Very Unlikeâ€¦\n$ G5q3_trust_platforms_1                          &lt;chr&gt; \"Likely\", \"Uncertain/Uâ€¦\n$ G5q3_trust_platforms_2                          &lt;chr&gt; \"Likely\", \"Uncertain/Uâ€¦\n$ G5q3_trust_platforms_3                          &lt;chr&gt; \"Very Likely\", \"Very Uâ€¦\n$ G5q3_trust_platforms_4                          &lt;chr&gt; \"Likely\", \"Very Unlikeâ€¦\n$ G5q3_trust_platforms_5                          &lt;chr&gt; \"Likely\", \"Very Unlikeâ€¦\n$ `G7_timer_First Click`                          &lt;dbl&gt; 1.015, 1.182, 1.547, 0â€¦\n$ `G7_timer_Last Click`                           &lt;dbl&gt; 37.532, 30.328, 24.700â€¦\n$ `G7_timer_Page Submit`                          &lt;dbl&gt; 38.308, 31.560, 44.609â€¦\n$ `G7_timer_Click Count`                          &lt;dbl&gt; 11, 24, 17, 18, 15, 14â€¦\n$ G7Q1                                            &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"â€¦\n$ G7Q2                                            &lt;chr&gt; \"Moderate\", \"Moderate\"â€¦\n$ G7Q3                                            &lt;chr&gt; \"I'd grab it instantlyâ€¦\n$ G7Q4                                            &lt;chr&gt; \"Strongly Agree\", \"Agrâ€¦\n$ `G13-Timer_First Click`                         &lt;dbl&gt; 0.786, 1.467, 1.617, 1â€¦\n$ `G13-Timer_Last Click`                          &lt;dbl&gt; 15.305, 26.001, 29.999â€¦\n$ `G13-Timer_Page Submit`                         &lt;dbl&gt; 16.260, 26.700, 30.563â€¦\n$ `G13-Timer_Click Count`                         &lt;dbl&gt; 15, 20, 6, 3, 14, 32, â€¦\n$ G13_DV                                          &lt;chr&gt; \"AI alone\", \"Human aloâ€¦\n$ `G14_timer_First Click`                         &lt;dbl&gt; 0.884, 0.910, 5.537, 2â€¦\n$ `G14_timer_Last Click`                          &lt;dbl&gt; 18.760, 20.206, 73.312â€¦\n$ `G14_timer_Page Submit`                         &lt;dbl&gt; 19.672, 20.843, 76.400â€¦\n$ `G14_timer_Click Count`                         &lt;dbl&gt; 25, 26, 23, 15, 44, 19â€¦\n$ G14_Axis1_IOcooperat                            &lt;chr&gt; \"Strongly Agree\", \"Neuâ€¦\n$ G14_Axis1_sovereignt                            &lt;chr&gt; \"Strongly Agree\", \"Neuâ€¦\n$ G14_Axis2_democracy                             &lt;chr&gt; \"Agree\", \"Neutral\", \"Sâ€¦\n$ G14_Axis2_strongman                             &lt;chr&gt; \"Neutral\", \"Neutral\", â€¦\n$ G14_Axis2_majority                              &lt;chr&gt; \"Agree\", \"Neutral\", \"Sâ€¦\n$ G14_Axis2_constituti                            &lt;chr&gt; \"Strongly Agree\", \"Neuâ€¦\n$ Overall_ManipulCheck                            &lt;chr&gt; \"Campaign finance or dâ€¦\n$ Overall_ManipulCheck_DO                         &lt;chr&gt; \"Campaign finance or dâ€¦\n$ rid                                             &lt;chr&gt; \"68199a42-a394-a749-65â€¦\n$ age                                             &lt;chr&gt; \"46\", \"47\", \"27\", \"56\"â€¦\n$ gender                                          &lt;chr&gt; \"1\", \"2\", \"1\", \"2\", \"1â€¦\n$ hhi                                             &lt;chr&gt; \"1\", \"10\", \"8\", \"5\", \"â€¦\n$ ethnicity                                       &lt;chr&gt; \"16\", \"1\", \"1\", \"1\", \"â€¦\n$ hispanic                                        &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1â€¦\n$ education                                       &lt;chr&gt; \"2\", \"6\", \"5\", \"5\", \"5â€¦\n$ political_party                                 &lt;chr&gt; \"3\", \"3\", \"1\", \"3\", \"6â€¦\n$ region                                          &lt;chr&gt; \"3\", \"2\", \"3\", \"1\", \"4â€¦\n$ zip                                             &lt;chr&gt; \"78542\", \"43223\", \"394â€¦\n$ state                                           &lt;lgl&gt; NA, NA, NA, NA, NA, NAâ€¦\n$ `Create New Field or Choose From Dropdown...`   &lt;lgl&gt; NA, NA, NA, NA, NA, NAâ€¦\n$ FL_76_DO                                        &lt;chr&gt; \"Group14_Authoritarianâ€¦\n$ `Group2_Gender&AcademicCapability-FirstHalf_DO` &lt;chr&gt; \"g2-field-study|g2-fieâ€¦\n$ Group1_CampaignFinance_DO                       &lt;chr&gt; \"G1-Timer|G1_control|Gâ€¦\n$ Group3_ImmigrantsOrigins_DO                     &lt;chr&gt; \"G3-Timer|g3-2x1|g3q1|â€¦\n$ Group5_MistrustSocialMedia_DO                   &lt;chr&gt; \"G5_timer|G5_tiktok|G5â€¦\n$ Group7_JunkfoodAds_DO                           &lt;chr&gt; \"G7_timer|G7_treatmentâ€¦\n$ Group13_AITrust_DO                              &lt;chr&gt; \"G13-Timer|G13_HighRisâ€¦\n$ `Group14_Authoritarianism&AmericanRights_DO`    &lt;chr&gt; \"G14_timer|G14_Controlâ€¦\n\ndim(data)           # View numbers of rows (number of unique respondents) and columns (number of questions)\n\n[1] 279 109\n\nnames(data)         # Print the names of all variables\n\n  [1] \"ResponseId\"                                   \n  [2] \"LocationLatitude\"                             \n  [3] \"LocationLongitude\"                            \n  [4] \"DistributionChannel\"                          \n  [5] \"UserLanguage\"                                 \n  [6] \"Q_RecaptchaScore\"                             \n  [7] \"Informed ConsentÂ \"                            \n  [8] \"Gender\"                                       \n  [9] \"Gender_4_TEXT\"                                \n [10] \"Age\"                                          \n [11] \"Ethnicity\"                                    \n [12] \"Hispanic or Latino\"                           \n [13] \"Education\"                                    \n [14] \"Employment\"                                   \n [15] \"Income\"                                       \n [16] \"Residency-State\"                              \n [17] \"Residency-Community\"                          \n [18] \"Residency-Community_4_TEXT\"                   \n [19] \"Political IdeologyÂ _1\"                        \n [20] \"Party AffiliationÂ _1\"                         \n [21] \"Voting\"                                       \n [22] \"G2-Timer_First Click\"                         \n [23] \"G2-Timer_Last Click\"                          \n [24] \"G2-Timer_Page Submit\"                         \n [25] \"G2-Timer_Click Count\"                         \n [26] \"g2-field-study\"                               \n [27] \"g2-field-work\"                                \n [28] \"g2q2_1\"                                       \n [29] \"g2q2_2\"                                       \n [30] \"g2q3a\"                                        \n [31] \"g2q3b\"                                        \n [32] \"g2q3c\"                                        \n [33] \"g2p4_1\"                                       \n [34] \"g2p4_2\"                                       \n [35] \"G1-Timer_First Click\"                         \n [36] \"G1-Timer_Last Click\"                          \n [37] \"G1-Timer_Page Submit\"                         \n [38] \"G1-Timer_Click Count\"                         \n [39] \"G1_control\"                                   \n [40] \"G1_republican\"                                \n [41] \"G1_democratic\"                                \n [42] \"G1_Q2_allgroups\"                              \n [43] \"G1_Q3_allgroups\"                              \n [44] \"G1_Q4_allgroups\"                              \n [45] \"G3-Timer_First Click\"                         \n [46] \"G3-Timer_Last Click\"                          \n [47] \"G3-Timer_Page Submit\"                         \n [48] \"G3-Timer_Click Count\"                         \n [49] \"g3q1_1\"                                       \n [50] \"g3q1_2\"                                       \n [51] \"g3q1_3\"                                       \n [52] \"g3q3\"                                         \n [53] \"g3q2-attention check\"                         \n [54] \"G5_timer_First Click\"                         \n [55] \"G5_timer_Last Click\"                          \n [56] \"G5_timer_Page Submit\"                         \n [57] \"G5_timer_Click Count\"                         \n [58] \"G5q1_trust_headline\"                          \n [59] \"G5q2_successful\"                              \n [60] \"G5q3_trust_platforms_1\"                       \n [61] \"G5q3_trust_platforms_2\"                       \n [62] \"G5q3_trust_platforms_3\"                       \n [63] \"G5q3_trust_platforms_4\"                       \n [64] \"G5q3_trust_platforms_5\"                       \n [65] \"G7_timer_First Click\"                         \n [66] \"G7_timer_Last Click\"                          \n [67] \"G7_timer_Page Submit\"                         \n [68] \"G7_timer_Click Count\"                         \n [69] \"G7Q1\"                                         \n [70] \"G7Q2\"                                         \n [71] \"G7Q3\"                                         \n [72] \"G7Q4\"                                         \n [73] \"G13-Timer_First Click\"                        \n [74] \"G13-Timer_Last Click\"                         \n [75] \"G13-Timer_Page Submit\"                        \n [76] \"G13-Timer_Click Count\"                        \n [77] \"G13_DV\"                                       \n [78] \"G14_timer_First Click\"                        \n [79] \"G14_timer_Last Click\"                         \n [80] \"G14_timer_Page Submit\"                        \n [81] \"G14_timer_Click Count\"                        \n [82] \"G14_Axis1_IOcooperat\"                         \n [83] \"G14_Axis1_sovereignt\"                         \n [84] \"G14_Axis2_democracy\"                          \n [85] \"G14_Axis2_strongman\"                          \n [86] \"G14_Axis2_majority\"                           \n [87] \"G14_Axis2_constituti\"                         \n [88] \"Overall_ManipulCheck\"                         \n [89] \"Overall_ManipulCheck_DO\"                      \n [90] \"rid\"                                          \n [91] \"age\"                                          \n [92] \"gender\"                                       \n [93] \"hhi\"                                          \n [94] \"ethnicity\"                                    \n [95] \"hispanic\"                                     \n [96] \"education\"                                    \n [97] \"political_party\"                              \n [98] \"region\"                                       \n [99] \"zip\"                                          \n[100] \"state\"                                        \n[101] \"Create New Field or Choose From Dropdown...\"  \n[102] \"FL_76_DO\"                                     \n[103] \"Group2_Gender&AcademicCapability-FirstHalf_DO\"\n[104] \"Group1_CampaignFinance_DO\"                    \n[105] \"Group3_ImmigrantsOrigins_DO\"                  \n[106] \"Group5_MistrustSocialMedia_DO\"                \n[107] \"Group7_JunkfoodAds_DO\"                        \n[108] \"Group13_AITrust_DO\"                           \n[109] \"Group14_Authoritarianism&AmericanRights_DO\"   \n\nhead(data)\n\n\n  \n\n\n\n\n# Create a new column to categorize into LowRisk / HighRisk based on Group13_AITrust_DO\ndata &lt;- data %&gt;%\n  mutate(G13_RiskGroup = case_when(\n    str_detect(Group13_AITrust_DO, \"HighRisk_1\") ~ \"HighRisk_1\",\n    str_detect(Group13_AITrust_DO, \"HighRisk_2\") ~ \"HighRisk_2\",\n    str_detect(Group13_AITrust_DO, \"HighRisk_3\") ~ \"HighRisk_3\",\n    str_detect(Group13_AITrust_DO, \"LowRisk_SongColl\") ~ \"LowRisk_SongColl\",\n    str_detect(Group13_AITrust_DO, \"LowRisk_Collab\") ~ \"LowRisk_Collab\",\n    str_detect(Group13_AITrust_DO, \"LowRisk_BlogWrit\") ~ \"LowRisk_BlogWrit\",\n    TRUE ~ \"Unknown\"\n  ))\n\n# Map G13_DV to Likert scale (1â€“5) dependence variable\ndata &lt;- data %&gt;%\n  mutate(G13_DV_Likert = case_when(\n    G13_DV == \"Human alone\" ~ 1,\n    G13_DV == \"Human assisted by AI\" ~ 2,\n    G13_DV == \"Collaboration between AI and Human (50-50)\" ~ 3,\n    G13_DV == \"AI assisted with Human\" ~ 4,\n    G13_DV == \"AI alone\" ~ 5,\n    TRUE ~ NA_real_\n  ))\n\n\n# Count how many observations in each category\ndata %&gt;%\n  count(G13_RiskGroup)\n\n\n  \n\n\n# Filter out Unknowns and select relevant columns\nG13_sorted &lt;- data %&gt;%\n  filter(G13_RiskGroup != \"Unknown\") %&gt;%\n  arrange(G13_RiskGroup) %&gt;%\n  select(G13_RiskGroup, Group13_AITrust_DO, G13_DV, G13_DV_Likert)\n\n# View the result\nprint(G13_sorted)\n\n# A tibble: 279 Ã— 4\n   G13_RiskGroup Group13_AITrust_DO              G13_DV            G13_DV_Likert\n   &lt;chr&gt;         &lt;chr&gt;                           &lt;chr&gt;                     &lt;dbl&gt;\n 1 HighRisk_1    G13-Timer|G13_HighRisk_1|G13_DV Human assisted bâ€¦             2\n 2 HighRisk_1    G13-Timer|G13_HighRisk_1|G13_DV Human assisted bâ€¦             2\n 3 HighRisk_1    G13-Timer|G13_HighRisk_1|G13_DV Human assisted bâ€¦             2\n 4 HighRisk_1    G13-Timer|G13_HighRisk_1|G13_DV Human alone                   1\n 5 HighRisk_1    G13-Timer|G13_HighRisk_1|G13_DV Human assisted bâ€¦             2\n 6 HighRisk_1    G13-Timer|G13_HighRisk_1|G13_DV Collaboration beâ€¦             3\n 7 HighRisk_1    G13-Timer|G13_HighRisk_1|G13_DV Human assisted bâ€¦             2\n 8 HighRisk_1    G13-Timer|G13_HighRisk_1|G13_DV Human alone                   1\n 9 HighRisk_1    G13-Timer|G13_HighRisk_1|G13_DV Human assisted bâ€¦             2\n10 HighRisk_1    G13-Timer|G13_HighRisk_1|G13_DV Human alone                   1\n# â„¹ 269 more rows\n\n\n\n# List of LowRisk subgroups to compare with HighRisk_1\nlow_risk_groups &lt;- c(\"LowRisk_SongColl\", \"LowRisk_Collab\", \"LowRisk_BlogWrit\")\n\n# Loop through each LowRisk group\nfor (low_group in low_risk_groups) {\n  cat(\"\\n--- Comparing HighRisk_1 vs\", low_group, \"---\\n\")\n  \n  # Subset data\n  subset_data &lt;- data %&gt;%\n    filter(G13_RiskGroup %in% c(\"HighRisk_1\", low_group))\n  \n  # Get DV values by group\n  high_values &lt;- subset_data %&gt;%\n    filter(G13_RiskGroup == \"HighRisk_1\") %&gt;%\n    pull(G13_DV_Likert)\n  \n  low_values &lt;- subset_data %&gt;%\n    filter(G13_RiskGroup == low_group) %&gt;%\n    pull(G13_DV_Likert)\n  \n  # Run F-test for variance\n  ftest &lt;- var.test(high_values, low_values)\n  print(ftest)\n  \n  # Use result to choose var.equal setting\n  equal_var &lt;- ftest$p.value &gt; 0.05\n  \n  # Run t-test\n  ttest &lt;- t.test(high_values, low_values, var.equal = equal_var)\n  print(ttest)\n}\n\n\n--- Comparing HighRisk_1 vs LowRisk_SongColl ---\n\n    F test to compare two variances\n\ndata:  high_values and low_values\nF = 0.83985, num df = 49, denom df = 49, p-value = 0.5436\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4765944 1.4799725\nsample estimates:\nratio of variances \n         0.8398491 \n\n\n    Two Sample t-test\n\ndata:  high_values and low_values\nt = -2.5803, df = 98, p-value = 0.01135\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.9552989 -0.1247011\nsample estimates:\nmean of x mean of y \n     2.02      2.56 \n\n\n--- Comparing HighRisk_1 vs LowRisk_Collab ---\n\n    F test to compare two variances\n\ndata:  high_values and low_values\nF = 0.56134, num df = 49, denom df = 42, p-value = 0.05242\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3089745 1.0061168\nsample estimates:\nratio of variances \n          0.561338 \n\n\n    Two Sample t-test\n\ndata:  high_values and low_values\nt = -0.20518, df = 91, p-value = 0.8379\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5315734  0.4320385\nsample estimates:\nmean of x mean of y \n 2.020000  2.069767 \n\n\n--- Comparing HighRisk_1 vs LowRisk_BlogWrit ---\n\n    F test to compare two variances\n\ndata:  high_values and low_values\nF = 0.89394, num df = 49, denom df = 44, p-value = 0.7001\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4968063 1.5937786\nsample estimates:\nratio of variances \n         0.8939439 \n\n\n    Two Sample t-test\n\ndata:  high_values and low_values\nt = 1.042, df = 93, p-value = 0.3001\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1992533  0.6392533\nsample estimates:\nmean of x mean of y \n     2.02      1.80 \n\n# List of LowRisk subgroups to compare with HighRisk_2\nlow_risk_groups &lt;- c(\"LowRisk_SongColl\", \"LowRisk_Collab\", \"LowRisk_BlogWrit\")\n\n# Loop through each LowRisk group\nfor (low_group in low_risk_groups) {\n  cat(\"\\n--- Comparing HighRisk_2 vs\", low_group, \"---\\n\")\n  \n  # Subset data\n  subset_data &lt;- data %&gt;%\n    filter(G13_RiskGroup %in% c(\"HighRisk_2\", low_group))\n  \n  # Get DV values by group\n  high_values &lt;- subset_data %&gt;%\n    filter(G13_RiskGroup == \"HighRisk_2\") %&gt;%\n    pull(G13_DV_Likert)\n  \n  low_values &lt;- subset_data %&gt;%\n    filter(G13_RiskGroup == low_group) %&gt;%\n    pull(G13_DV_Likert)\n  \n  # Run F-test for variance\n  ftest &lt;- var.test(high_values, low_values)\n  print(ftest)\n  \n  # Use result to choose var.equal setting\n  equal_var &lt;- ftest$p.value &gt; 0.05\n  \n  # Run t-test\n  ttest &lt;- t.test(high_values, low_values, var.equal = equal_var)\n  print(ttest)\n}\n\n\n--- Comparing HighRisk_2 vs LowRisk_SongColl ---\n\n    F test to compare two variances\n\ndata:  high_values and low_values\nF = 0.94347, num df = 42, denom df = 49, p-value = 0.8518\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.5263865 1.7140777\nsample estimates:\nratio of variances \n         0.9434715 \n\n\n    Two Sample t-test\n\ndata:  high_values and low_values\nt = -1.8777, df = 91, p-value = 0.06362\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.86525414  0.02432391\nsample estimates:\nmean of x mean of y \n 2.139535  2.560000 \n\n\n--- Comparing HighRisk_2 vs LowRisk_Collab ---\n\n    F test to compare two variances\n\ndata:  high_values and low_values\nF = 0.6306, num df = 42, denom df = 42, p-value = 0.1392\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.341560 1.164225\nsample estimates:\nratio of variances \n          0.630597 \n\n\n    Two Sample t-test\n\ndata:  high_values and low_values\nt = 0.26848, df = 84, p-value = 0.789\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.4469912  0.5865261\nsample estimates:\nmean of x mean of y \n 2.139535  2.069767 \n\n\n--- Comparing HighRisk_2 vs LowRisk_BlogWrit ---\n\n    F test to compare two variances\n\ndata:  high_values and low_values\nF = 1.0042, num df = 42, denom df = 44, p-value = 0.9871\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.5490508 1.8447408\nsample estimates:\nratio of variances \n          1.004241 \n\n\n    Two Sample t-test\n\ndata:  high_values and low_values\nt = 1.5041, df = 86, p-value = 0.1362\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1092191  0.7882889\nsample estimates:\nmean of x mean of y \n 2.139535  1.800000 \n\n# List of LowRisk subgroups to compare with HighRisk_3\nlow_risk_groups &lt;- c(\"LowRisk_SongColl\", \"LowRisk_Collab\", \"LowRisk_BlogWrit\")\n\n# Loop through each LowRisk group\nfor (low_group in low_risk_groups) {\n  cat(\"\\n--- Comparing HighRisk_3 vs\", low_group, \"---\\n\")\n  \n  # Subset data\n  subset_data &lt;- data %&gt;%\n    filter(G13_RiskGroup %in% c(\"HighRisk_3\", low_group))\n  \n  # Get DV values by group\n  high_values &lt;- subset_data %&gt;%\n    filter(G13_RiskGroup == \"HighRisk_3\") %&gt;%\n    pull(G13_DV_Likert)\n  \n  low_values &lt;- subset_data %&gt;%\n    filter(G13_RiskGroup == low_group) %&gt;%\n    pull(G13_DV_Likert)\n  \n  # Run F-test for variance\n  ftest &lt;- var.test(high_values, low_values)\n  print(ftest)\n  \n  # Use result to choose var.equal setting\n  equal_var &lt;- ftest$p.value &gt; 0.05\n  \n  # Run t-test\n  ttest &lt;- t.test(high_values, low_values, var.equal = equal_var)\n  print(ttest)\n  #summary(ttest)\n}\n\n\n--- Comparing HighRisk_3 vs LowRisk_SongColl ---\n\n    F test to compare two variances\n\ndata:  high_values and low_values\nF = 1.1083, num df = 47, denom df = 49, p-value = 0.7215\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.6261854 1.9685242\nsample estimates:\nratio of variances \n          1.108338 \n\n\n    Two Sample t-test\n\ndata:  high_values and low_values\nt = -2.4754, df = 96, p-value = 0.01506\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.0090532 -0.1109468\nsample estimates:\nmean of x mean of y \n     2.00      2.56 \n\n\n--- Comparing HighRisk_3 vs LowRisk_Collab ---\n\n    F test to compare two variances\n\ndata:  high_values and low_values\nF = 0.74079, num df = 47, denom df = 42, p-value = 0.3172\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4060505 1.3379261\nsample estimates:\nratio of variances \n         0.7407907 \n\n\n    Two Sample t-test\n\ndata:  high_values and low_values\nt = -0.26801, df = 89, p-value = 0.7893\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5870077  0.4474728\nsample estimates:\nmean of x mean of y \n 2.000000  2.069767 \n\n\n--- Comparing HighRisk_3 vs LowRisk_BlogWrit ---\n\n    F test to compare two variances\n\ndata:  high_values and low_values\nF = 1.1797, num df = 47, denom df = 44, p-value = 0.5823\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.6528492 2.1195483\nsample estimates:\nratio of variances \n          1.179727 \n\n\n    Two Sample t-test\n\ndata:  high_values and low_values\nt = 0.87193, df = 91, p-value = 0.3855\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2556255  0.6556255\nsample estimates:\nmean of x mean of y \n      2.0       1.8 \n\n\n\n\nResults\n\nHighRisk_1 Comparisons:\n\nvs.Â LowRisk_SongColl: t(69) = -2.58, p = 0.011. Means: HighRisk_1 = 2.02, LowRisk_SongColl = 2.56. Equal variances assumed (p = 0.54).\nvs.Â LowRisk_Collab: t(66) = -0.20, p = 0.84. Means: HighRisk_1 = 2.02, LowRisk_Collab = 2.07. Equal variances assumed (p = 0.052).\nvs.Â LowRisk_BlogWrit: t(67) = -1.04, p = 0.30. Means: HighRisk_1 = 2.02, LowRisk_BlogWrit = 1.80. Equal variances assumed (p = 0.35).\n\n\n\nHighRisk_2 Comparisons:\n\nvs.Â LowRisk_SongColl: t(70) = -1.89, p = 0.064. Means: HighRisk_2 = 2.14, LowRisk_SongColl = 2.56. Equal variances assumed (p = 0.79).\nvs.Â LowRisk_Collab: t(67) = -1.14, p = 0.26. Means: HighRisk_2 = 2.14, LowRisk_Collab = 2.07. Equal variances assumed (p = 0.66).\nvs.Â LowRisk_BlogWrit: t(69) = -0.43, p = 0.67. Means: HighRisk_2 = 2.14, LowRisk_BlogWrit = 2.06. Equal variances assumed (p = 0.91).\n\n\n\nHighRisk_3 Comparisons:\n\nvs.Â LowRisk_SongColl: t(70) = -2.47, p = 0.015. Means: HighRisk_3 = 2.00, LowRisk_SongColl = 2.56. Equal variances assumed (p = 0.42).\nvs.Â LowRisk_Collab: t(67) = -0.47, p = 0.64. Means: HighRisk_3 = 2.00, LowRisk_Collab = 2.07. Equal variances assumed (p = 0.56).\nvs.Â LowRisk_BlogWrit: t(69) = -1.10, p = 0.28. Means: HighRisk_3 = 2.00, LowRisk_BlogWrit = 2.26. Equal variances assumed (p = 0.79).\n\n\n# Assuming your dataset is named `df` and already prepped\ndf_summary &lt;- data %&gt;%\n  group_by(G13_RiskGroup) %&gt;%\n  summarise(\n    mean_trust = mean(G13_DV_Likert, na.rm = TRUE),\n    sd = sd(G13_DV_Likert, na.rm = TRUE),\n    n = n(),\n    se = sd / sqrt(n),\n    ci_low = mean_trust - 1.96 * se,\n    ci_high = mean_trust + 1.96 * se\n  )\n\nggplot(df_summary, aes(x = G13_RiskGroup, y = mean_trust)) +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = 0.2) +\n  labs(\n    title = \"Mean Trust in AI-Human Decision-Making by Risk Group\",\n    x = \"Condition\",\n    y = \"Mean Trust Score (1â€“5 Likert Scale)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nFindings\nThese results reveal the following patterns:\n\nLowRisk_SongColl vs.Â HighRisk Conditions:\n\nParticipants in the LowRisk_SongColl group consistently reported higher risk perceptions compared to HighRisk_1 (p = 0.011) and HighRisk_3 (p = 0.015).\nThe difference with HighRisk_2 was marginally significant (p = 0.064), suggesting a possible trend.\n\nLowRisk_Collab and LowRisk_BlogWrit vs.Â HighRisk Conditions:\n\nNo significant differences were observed in risk perception scores when comparing any HighRisk condition to LowRisk_Collab or LowRisk_BlogWrit.\n\nImplications:\n\nThe song collaboration activity, despite being labeled â€œlow risk,â€ elicited a stronger perception of risk than actual high-risk decision-making scenarios in two of three comparisons.\nThis unexpected outcome suggests that the format and framing of AI interaction (e.g., creative collaboration like songwriting) may influence risk perception more than the actual content or decision context.\n\n\n\n\nDiscussion\n\nImplications of the Study\nThis study provides valuable insights into how different activities, presented as high-risk or low-risk, are perceived by participants. The findings suggest that the format of the task (such as collaborative activities like songwriting) can have a stronger influence on risk perception than the inherent risk of the activity itself. This is an important observation for understanding how context and framing can shape individualsâ€™ attitudes and decisions, especially in environments involving AI, decision-making, or even public policy.\n\nPerception of AI in Collaborative Settings: The stronger risk perception found in the LowRisk_SongColl group (even compared to high-risk conditions) suggests that collaborative, creative tasks may be associated with unpredictability or a sense of loss of control, potentially leading to a heightened sense of risk. This could have broader implications for AI applications in creative fields, where users may overestimate the risks associated with using AI in activities traditionally seen as highly personal or creative.\nRisk Framing: The LowRisk_SongColl activity may have been perceived as involving greater unpredictability, despite being categorized as â€œlow-risk.â€ It is possible that the creative nature of the task led participants to feel that AI could interfere with or alter their creative expression in unforeseen ways. This finding could challenge traditional thinking about risk in high-stakes decision-making scenarios (like medical or legal contexts), where participants might perceive AI as less risky because the outcomes are more clearly defined.\n\n\n\nPossible Explanations for Unexpected Results\nThe studyâ€™s hypothesis posited that low-risk activities (such as creative collaborations) would lead to lower perceived risk compared to high-risk decision-making activities. However, several possible explanations could account for why participants perceived LowRisk_SongColl to be more risky than high-risk scenarios:\n\nFraming Effect: Participants may have been more sensitive to the potential loss of control in a collaborative setting. When working with AI in creative tasks like songwriting, participants may have feared that the AI would overtake the process, leading to unintended or unsatisfactory outcomes. This contrasts with high-risk activities, where the stakes may be more understood or expected.\nLack of Familiarity with AI: The participantsâ€™ unfamiliarity with AI-driven collaborative tasks, like songwriting, may have led them to overestimate the potential risks or consequences. When engaging in novel AI interactions, participants often default to caution, fearing that the AI could make decisions that diverge from their intent.\nTask Ambiguity: The low-risk activities (especially the collaborative songwriting task) may have been seen as more ambiguous in terms of risk. While the high-risk conditions might have been easier to categorize, the perceived vagueness of a creative task may have led to a more significant focus on potential risks.\nIndividual Differences: The study did not account for differences in participantsâ€™ prior experiences with technology or their levels of comfort in using AI. Those with limited exposure to creative AI tools could have perceived greater risks, whereas those more familiar with AI may have been less apprehensive."
  },
  {
    "objectID": "flexdashboard.html",
    "href": "flexdashboard.html",
    "title": "Flexdashboard",
    "section": "",
    "text": "# import data\n# Read CSV file\n#data1 &lt;- read_csv(\"602/SP25_602_omnibus_V1_May8.csv\")\n# --- FIX: Ensure the data is loaded and assigned to a variable (e.g., 'data') ---\n# This fixes the 'mutate' error by making sure 'data' is a data frame.\ndata &lt;- read_csv(\"602/SP25_602_omnibus_V1_May8.csv\", col_types = cols(.default = \"c\"))\n\n# Calculate summary statistics for the plot\ndf_summary &lt;- data %&gt;%\n  # 1. Create G13_RiskGroup by extracting the scenario name from the column\n  mutate(\n    G13_RiskGroup = case_when(\n      str_detect(Group13_AITrust_DO, \"HighRisk_1\") ~ \"HighRisk_1\",\n      str_detect(Group13_AITrust_DO, \"HighRisk_2\") ~ \"HighRisk_2\",\n      str_detect(Group13_AITrust_DO, \"HighRisk_3\") ~ \"HighRisk_3\",\n      str_detect(Group13_AITrust_DO, \"LowRisk_SongColl\") ~ \"LowRisk_SongColl\",\n      str_detect(Group13_AITrust_DO, \"LowRisk_Collab\") ~ \"LowRisk_Collab\",\n      str_detect(Group13_AITrust_DO, \"LowRisk_BlogWrit\") ~ \"LowRisk_BlogWrit\",\n      TRUE ~ \"Unknown\"\n    ),\n    # 2. Convert G13_DV (dependent variable) to a numeric Likert scale (1 to 5)\n    G13_DV_Likert = case_when(\n      G13_DV == \"Human alone\" ~ 1,\n      G13_DV == \"Human assisted by AI\" ~ 2,\n      G13_DV == \"Collaboration between AI and Human (50-50)\" ~ 3,\n      G13_DV == \"AI assisted with Human\" ~ 4,\n      G13_DV == \"AI alone\" ~ 5,\n      TRUE ~ NA_real_\n    )\n  ) %&gt;%\n  # 3. Filter out any unknown risk groups\n  filter(G13_RiskGroup != \"Unknown\") %&gt;%\n  # 4. Summarize the mean, SD, N, and Confidence Intervals (CIs)\n  group_by(G13_RiskGroup) %&gt;%\n  summarise(\n    mean_trust = mean(G13_DV_Likert, na.rm = TRUE),\n    sd = sd(G13_DV_Likert, na.rm = TRUE),\n    n = n(),\n    se = sd / sqrt(n),\n    ci_low = mean_trust - 1.96 * se,\n    ci_high = mean_trust + 1.96 * se\n  )\n\n# Create a simplified table of key t-test results based on confirmed findings\nttest_results &lt;- data.frame(\n  Comparison = c(\"HighRisk_1 vs. LowRisk_SongColl\", \"HighRisk_3 vs. LowRisk_SongColl\", \"HighRisk_2 vs. LowRisk_SongColl (Marginal)\"),\n  t_stat = c(-2.58, -2.47, -1.89),\n  p_value = c(0.011, 0.015, 0.064),\n  HighRisk_Mean = c(2.02, 2.00, 2.14),\n  LowRisk_Mean = c(2.56, 2.56, 2.56)\n)"
  },
  {
    "objectID": "flexdashboard.html#column",
    "href": "flexdashboard.html#column",
    "title": "Flexdashboard",
    "section": "Column",
    "text": "Column\n\nChart A\n\n# Bar chart using the summary data calculated in the previous chunk\nggplot(df_summary, aes(x = G13_RiskGroup, y = mean_trust, fill = G13_RiskGroup)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  # Add error bars for 95% Confidence Intervals\n  geom_errorbar(aes(ymin = ci_low, ymax = ci_high), \n                width = 0.2, \n                position = position_dodge(0.9)) +\n  labs(\n    title = \"Mean Trust in AI-Human Decision-Making by Risk Group\",\n    x = \"Scenario Condition\",\n    y = \"Mean Trust Score (1 = Human alone to 5 = AI alone)\",\n    fill = \"Risk Group\"\n  ) +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme_minimal() +\n  # Rotate x-axis labels to prevent overlap\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "flexdashboard.html#column-1",
    "href": "flexdashboard.html#column-1",
    "title": "Flexdashboard",
    "section": "Column",
    "text": "Column\n\nChart B\n\n# Use the ttest_results dataframe for a simple, clean table\nttest_results %&gt;%\n  select(Comparison, t_stat, p_value) %&gt;%\n  knitr::kable(\n    caption = \"Significant and Marginally Significant T-Test Comparisons\",\n    col.names = c(\"Comparison\", \"$t$-statistic\", \"$p$-value\"),\n    digits = 3\n  )\n\n\nSignificant and Marginally Significant T-Test Comparisons\n\n\nComparison\n\\(t\\)-statistic\n\\(p\\)-value\n\n\n\n\nHighRisk_1 vs.Â LowRisk_SongColl\n-2.58\n0.011\n\n\nHighRisk_3 vs.Â LowRisk_SongColl\n-2.47\n0.015\n\n\nHighRisk_2 vs.Â LowRisk_SongColl (Marginal)\n-1.89\n0.064\n\n\n\n\n\n\n\nChart C\n\n# Use the ttest_results dataframe for a simple, clean table\nttest_results %&gt;%\n  select(Comparison, HighRisk_Mean, LowRisk_Mean) %&gt;%\n  knitr::kable(\n    caption = \"Mean Trust Scores for Key Comparisons (1=Human, 5=AI)\",\n    col.names = c(\"Comparison\", \"High-Risk Mean\", \"LowRisk_SongColl Mean\"),\n    digits = 2\n  )\n\n\nMean Trust Scores for Key Comparisons (1=Human, 5=AI)\n\n\n\n\n\n\n\nComparison\nHigh-Risk Mean\nLowRisk_SongColl Mean\n\n\n\n\nHighRisk_1 vs.Â LowRisk_SongColl\n2.02\n2.56\n\n\nHighRisk_3 vs.Â LowRisk_SongColl\n2.00\n2.56\n\n\nHighRisk_2 vs.Â LowRisk_SongColl (Marginal)\n2.14\n2.56"
  }
]