[
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "My primary professional and academic goals are currently focused on two key areas:\n\nAcademic Completion: Successfully completing my Masterâ€™s program in the Data Analytics and Computational Social Science (DACSS) program at the University of Massachusetts Amherst (UMass Amherst).\nCareer Transition: Securing a professional position focused on Data Analysis or Data Science, where I can apply the quantitative, computational, and analytical skills gained during my studies.\n\n\n\n\n\n\nMaster of Science (M.S.), Data Analytics and Computational Social Science (DACSS) * Expected Graduation: [Insert Expected Graduation Date, e.g., May 2026] * Key Areas of Study: [Briefly list 2-3 key skills or domains, e.g., Statistical Modeling, Machine Learning, Data Visualization, Causal Inference]\n\nView My Full Curriculum Vitae (CV)\nThanks for checking out my web site!"
  },
  {
    "objectID": "604_Final_check-in_2.html",
    "href": "604_Final_check-in_2.html",
    "title": "DACSS604_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "604_Final_check-in_2.html#research-question-and-hypothesis",
    "href": "604_Final_check-in_2.html#research-question-and-hypothesis",
    "title": "DACSS604_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "604_Final_check-in_2.html#data-collection",
    "href": "604_Final_check-in_2.html#data-collection",
    "title": "DACSS604_Final_Project",
    "section": "Data Collection",
    "text": "Data Collection\nTo explore the concept of â€œsimping,â€ I collected YouTube comments from six relevant videos for textual analysis. I utilized an R scraping script to extract approximately 8,000 comments in total. Following a cleaning and filtering process, a dataset of around 7,000 practical comments was retained for analysis.\nI specifically focused on the case study known as the â€œèƒ–è²“è·³æ±Ÿäº‹ä»¶â€ (Suicide of Fat Cat). This event, which occurred in Mainland China, provides a particularly rich and relevant dataset because it was a well-documented news story officially reported by the Chinese court. This official documentation makes it a real and verifiable event, distinguishing it from mere rumors or social media anecdotes. Furthermore, the use of Chinese-language videos as the reference source is critical, as the Chinese-speaking audience possesses extensive background knowledge and cultural context directly related to the local details of this incident.\n\nSIMP001 - é™ªæ‰“éŠæˆ²è³ºç™¾è¬é¤Šå¥³å‹æ…˜é­åˆ†æ‰‹ï¼ã€Œèƒ–è²“äº‹ä»¶ã€å¼•çˆ†ä¸­åœ‹æ€§åˆ¥æˆ°çˆ­ï¼Ÿã€Œæ’ˆå¥³ã€æ»¿è¡—è·‘çš„èƒŒå¾ŒåŸå› ï¼Ÿã€TODAY çœ‹ä¸–ç•Œã€‘(https://www.youtube.com/watch?v=o5TfkwlthWU&t=13s) 1952 comments from 11/04/2025\nSIMP002 - å½“èƒ–çŒ«é‡åˆ°æå¥³ï¼Œä¸€ä¸ªå¹´è½»äººå¦‚ä½•èµ°ä¸Šä¸å½’è·¯ï¼Ÿï½œå¥³æƒï½œæå¥³ï½œèƒ–çŒ«ï½œç‹è€…è£è€€ï½œç”·å¥³å¹³æƒï½œæ—¥æœ¬ï½œæ¢…å¤§é«˜é€Ÿï½œèˆ†è®ºæ§åˆ¶ï½œç‹å±€æ‹æ¡ˆ20240507 (https://www.youtube.com/watch?v=39Gq_eOPuDY&t=1s) 3731 comments from 11/04/2025\nSIMP003 - è€æ¢ï¼šç»™â€œèƒ–çŒ«â€å¤šæ¡é€‰æ‹© é‡åº†â€œèƒ–çŒ«äº‹ä»¶â€ä¸æ˜¯æ€§åˆ«å¤§æˆ˜ å¦‚ä½•é¿å…æˆä¸ºâ€œèƒ–çŒ«â€(https://www.youtube.com/watch?v=mjcgg0wFpfE) 997 comments from 11/04/2025\nSIMP004 - å°ä¼™ç‚ºæ„›è·³æ±Ÿï¼Œæ‹œé‡‘çš„å¥³å‹ï¼Œå¸è¡€çš„è¦ªå§ï¼Œç„¡è‰¯çš„å•†å®¶ï¼Œç˜‹ç‹‚çš„ç¶²æ°‘ï¼Œèª°æ‰æ˜¯åŠ å®³è€…ï¼Ÿç‚ºä½•è­¦å¯Ÿèªå®šå¥³å‹ç„¡ç½ªï¼Œåè€Œæ˜¯è¦ªå§é•äº†æ³•ï¼Ÿä¸€å£æ°£çœ‹å®Œèƒ–è²“äº‹ä»¶å§‹æœ«ï¼| Wayneèª¿æŸ¥(https://www.youtube.com/watch?v=igs7GoIU4MU) 615 comments from 11/04/2025\nSIMP005 - ç¥ç´šé™ªç©ã€Œèƒ–è²“ã€é­è©ä¹¾227è¬äº¡ å¥³å‹é“æ­‰ï½œ20240506 ETåˆé–“æ–°è (https://www.youtube.com/watch?v=tAE83zZEcOY) 402 comments from 11/04/2025\nSIMP006 - è¢«æ’ˆå¥³é¨™å…‰50è¬ï¼ŒéŠæˆ²å®…ç”·è·³æ±Ÿè‡ªæ®ºï¼Œè½Ÿå‹•å…¨ç¶²ï¼æ’ˆå¥³è­šç«¹æ¦¨ä¹¾èƒ–è²“äº‹ä»¶çœŸç›¸ï¼ã€æ–°é—»æœ€å˜²ç‚¹ å§œå…‰å®‡ã€2024.0508(https://www.youtube.com/watch?v=YYngd2Yt3zk) 271 comments from 11/04/2025\n\n\nlibrary(plyr)\n\nWarning: package 'plyr' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.4.3\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(purrr)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:plyr':\n\n    compact\n\nlibrary(chromote)\n\nWarning: package 'chromote' was built under R version 4.4.3\n\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tibble)\nlibrary(nnet)\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.4.3\n\n\ncorrplot 0.95 loaded\n\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.4.3\n\n\nLoading required package: NLP\n\n\nWarning: package 'NLP' was built under R version 4.4.2\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.4.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.4.3\n\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.4.3\n\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.4.3\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(\"quanteda.textplots\")\n\nWarning: package 'quanteda.textplots' was built under R version 4.4.3\n\nlibrary(httr)\n\nWarning: package 'httr' was built under R version 4.4.3\n\n\n\nAttaching package: 'httr'\n\n\nThe following object is masked from 'package:NLP':\n\n    content\n\nlibrary(RColorBrewer)\nlibrary(RedditExtractoR)\n\nWarning: package 'RedditExtractoR' was built under R version 4.4.3\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.4.3\n\nlibrary(tidyr)\n\n\ndata1 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\ndata2 &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments.csv\")\ndata3 &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments.csv\")\ndata4 &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments.csv\")\ndata5 &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments.csv\")\ndata6 &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments.csv\")"
  },
  {
    "objectID": "604_Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "href": "604_Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "title": "DACSS604_Final_Project",
    "section": "Data Cleaning (info for the poster",
    "text": "Data Cleaning (info for the poster\nALSO GUIDE IT TO THE PART WHICH I WANT PLUS PRESENT THE CLEAN FORMAT IN THE POSTER\nThe raw dataset, collected as several CSV files, initially contained detailed comment metadata. The original structure included columns such as: videoId, commentId, parentId, author, text, likeCount, publishedAt, updatedAt, viewerRating, canRate, and reply.\nFor data cleaning, all CSV files in the Final_project_data folder were systematically processed using the R environment. The initial step was to streamline the dataset by retaining only the essential variables for textual and engagement analysis: text, likeCount, and reply.\nI utilized the R packages dplyr and stringr to focus on standardizing the text column. This involved a series of cleaning operations: normalization of whitespace (removing line breaks, tabs, and extra spaces, and trimming leading/trailing whitespace) and character filtering. Crucially, I removed non-essential symbols and unusual characters while meticulously preserving all Chinese characters to ensure the comments remained culturally authentic and meaningful for subsequent analysis.\nFinally, each cleaned and standardized dataset was saved as a new CSV file, appended with the suffix _cleaned. UTF-8 encoding was explicitly used to guarantee the accurate representation of the Chinese characters. This systematic workflow ensures the comment data are tidy, standardized, and immediately ready for downstream procedures, such as tokenization and sentiment or frequency analysis.\n\nsource(\"data_cleaning_CN.R\")\n\ndata cleaning complete!.\n\nSIMP001 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\n\n#Present the eample of the result\nhead(SIMP001)\n\n      videoId                  commentId parentId        author\n1 o5TfkwlthWU UgyekRC230MDXREkdeN4AaABAg     &lt;NA&gt;  @DanjonMeshi\n2 o5TfkwlthWU UgxangSP0zjJm6_gHfV4AaABAg     &lt;NA&gt;  @paullee4451\n3 o5TfkwlthWU UgwZdLtl6Eb2wgDWaDV4AaABAg     &lt;NA&gt;      @urikora\n4 o5TfkwlthWU UgwSmeqUyHYUXGD6l3l4AaABAg     &lt;NA&gt; @fayechen1928\n5 o5TfkwlthWU UgwpotCAmJmn2wWU7u54AaABAg     &lt;NA&gt; @running_goat\n6 o5TfkwlthWU UgxQSyQbnLT9r7I1faB4AaABAg     &lt;NA&gt;  @Jack2006103\n                                                                                                                       text\n1                                                             å•†å®¶é›†é«”çµ¦ç©ºè¢‹çœŸçš„ç¬‘æ­»ï¼Œä¸æ­¢ç”Ÿæ´»åœ¨ä¸­åœ‹ï¼Œé€£æ­»åœ¨ä¸­åœ‹éƒ½è¦å·è‘—æ¨‚ğŸ˜†\n2                                                                                                                      é ­é¦™\n3                       æ›´æ…˜çš„æ˜¯ï¼Œäººéƒ½èµ°äº†ä¸€å€‹æœˆ çµæœå°±åœ¨é€™æ™‚æ©Ÿé»è¢«æŠ“ä¾†æ“‹æ”¿åºœåšçš„é†œäº‹ï¼ˆè·¯å´©è¯ç‚ºè»Šè¡é€²å»å‘æ´ç„¶å¾Œå¿«é€Ÿç‡ƒèµ·ä¾†ï¼‰\n4                                                                                      æ¯æ¬¡çœ‹åˆ°ä¸­åœ‹é€™ç¨®æ‚²åŠ‡éƒ½è¦ºå¾—ä¸å¯æ€è­°ğŸ˜¨ğŸ˜­\n5 æˆ‘çœŸçš„å¿…é ˆå¾—èªªå°å²¸ç”·å¥³æˆ°çˆ­çœŸçš„è¶Šä¾†è¶Šåš´é‡== å•éå¥½å¹¾å€‹å°å²¸çš„å¥³ç”Ÿéƒ½èªç‚ºç”·ç”Ÿå°±æ˜¯æ‡‰è©²è¦çµ¦å½©ç¦® åƒé£¯å°±æ˜¯è¦å¹«å¥³ç”Ÿä»˜éŒ¢ç­‰ç­‰ å¾ˆå¯æ€•\n6                                                                                                é€™äº›åº—å®¶åƒäººè¡€é¥…é ­ï¼Œè¶…å™çˆ›\n  likeCount          publishedAt            updatedAt viewerRating canRate\n1        58 2024-05-09T16:01:52Z 2024-05-09T16:01:52Z         none    TRUE\n2         0 2024-05-09T16:02:23Z 2024-05-09T16:02:23Z         none    TRUE\n3       345 2024-05-09T16:05:05Z 2024-05-09T16:05:05Z         none    TRUE\n4         1 2024-05-09T16:05:57Z 2024-05-09T16:05:57Z         none    TRUE\n5       139 2024-05-09T16:06:47Z 2024-05-09T16:06:47Z         none    TRUE\n6         4 2024-05-09T16:07:29Z 2024-05-09T16:07:29Z         none    TRUE\n  reply\n1 FALSE\n2 FALSE\n3 FALSE\n4 FALSE\n5 FALSE\n6 FALSE\n\ndata1_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_cleaned.csv\")\ndata2_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments_cleaned.csv\")\ndata3_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments_cleaned.csv\")\ndata4_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments_cleaned.csv\")\ndata5_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments_cleaned.csv\")\ndata6_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments_cleaned.csv\")"
  },
  {
    "objectID": "604_Final_check-in_2.html#preprocess-the-data",
    "href": "604_Final_check-in_2.html#preprocess-the-data",
    "title": "DACSS604_Final_Project",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nFor visualizing the dominant linguistic patterns within the comment data, I employed two complementary approaches. First, a Word Cloud visualization (generated using the Word_cloud_visualization.R script) provided an intuitive, qualitative representation of high-frequency words, instantly highlighting the most common terms associated with discussions of â€œSIMPâ€ behavior.\nSecond, I conducted a quantitative rank-frequency analysis by applying Zipfâ€™s Law to the word corpus. After arranging all unique words by descending frequency and assigning a rank, I plotted the resulting distribution using the ggplot2 package. The resulting visualization confirmed that the comment discourse adheres to a Zipfian distribution, where a few words account for a disproportionate share of the total vocabulary.\nThe key terms driving the discourse were clearly identifiable:\nThese visualizations collectively offer both quantitative validation (Zipfâ€™s Law distribution) and qualitative insight (Word Cloud/Top Terms) into how the audience discusses and perceives the central event and the related concept of â€œSIMPâ€ behavior in this context. The high frequency of questioning and uncertainty (ç‚ºä»€éº¼, ä¸çŸ¥é“, æ˜¯ä¸æ˜¯) coupled with terms of exploitation (pua) and suffering (å—å®³è€…) reveals a key focus on moral judgment and accountability in the discussion.\n\nsource(\"TOKENIZATION.R\")\n\nTokenization complete!\n\nSIMP001_comments_tokens &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_tokens.csv\")\n#Present the eample of the result\nhead(SIMP001_comments_tokens)\n\n  likeCount reply     word\n1       345 FALSE   è¡é€²å»\n2         1 FALSE ä¸å¯æ€è­°\n3       139 FALSE   è¶Šä¾†è¶Š\n4       139 FALSE   å¥½å¹¾å€‹\n5       141 FALSE   éº¥ç•¶å‹\n6         3 FALSE   æœ‰äººèªª\n\n\n\nsource(\"Word_frequency.R\")\n\nWord Frequency Calculation Complete!\n\nSIMP001_wordfreq &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_wordfreq.csv\")\n\n#Present the eample of the result\nhead(SIMP001_wordfreq)\n\n    word  n rank\n1 å—å®³è€… 75    1\n2 ç‚ºä»€éº¼ 67    2\n3 ä¸çŸ¥é“ 53    3\n4    pua 42    4\n5 é€™ä»¶äº‹ 34    5\n6 ä¸€å€‹äºº 32    6\n\n\n\nsource(\"Same_Word.R\")\n\nCommon words saved to: Final_project_data/common_words_across_files.csv \n\ncommon_words &lt;- read.csv(\"Final_project_data/common_words_across_files.csv\")\n\n#Present the eample of the result\nhead(common_words)\n\n    word total_count\n1 ä¸çŸ¥é“         172\n2 ä¸ºä»€ä¹ˆ         154\n3 å—å®³è€…         118\n4    pua         115\n5 æ˜¯ä¸æ˜¯         101\n6 ç‚ºä»€éº¼         100\n\n\n\nsource(\"Change_to_Traditional_Chinese.R\")\n\nWarning: package 'textstem' was built under R version 4.4.3\n\n\nLoading required package: koRpus.lang.en\n\n\nWarning: package 'koRpus.lang.en' was built under R version 4.4.3\n\n\nLoading required package: koRpus\n\n\nWarning: package 'koRpus' was built under R version 4.4.3\n\n\nLoading required package: sylly\n\n\nWarning: package 'sylly' was built under R version 4.4.3\n\n\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\n\nAttaching package: 'koRpus'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    tokens, types\n\n\nThe following object is masked from 'package:tm':\n\n    readTagged\n\n\nThe following object is masked from 'package:readr':\n\n    tokenize\n\n\nWarning: package 'tmcn' was built under R version 4.4.3\n\n\n# tmcn Version: 0.2-13\n\n\n[1] \"girl\"    \"woman\"   \"simping\" \"lover\"  \nTranslation complete! Output saved to 'your_output_file.csv'\n\nTraditional_Chinese_data_cleaned &lt;- read.csv(\"Final_project_data/traditional_common_words_combined.csv\")\n\n#Present the data after cleaning\nhead(Traditional_Chinese_data_cleaned)\n\n  traditional_text total_count\n1           ç‚ºä»€éº¼         254\n2           ä¸çŸ¥é“         172\n3           å—å®³è€…         118\n4              pua         116\n5           æ˜¯ä¸æ˜¯         101\n6           å¥³æœ‹å‹          95"
  },
  {
    "objectID": "604_Final_check-in_2.html#visualization",
    "href": "604_Final_check-in_2.html#visualization",
    "title": "DACSS604_Final_Project",
    "section": "Visualization",
    "text": "Visualization\nFor visualizing patterns in the comments, I used two approaches. First, the Word_cloud_visualization.R script generated word clouds to highlight high-frequency words, providing a clear and intuitive view of the most common terms associated with discussions of â€œSIMPâ€ behavior. Second, I applied Zipfâ€™s Law to examine the relationship between word rank and frequency. After arranging words by descending frequency and assigning ranks, I plotted all words using ggplot2, labeling only the top five most frequent words to emphasize the key terms in the discourse. The resulting visualizations offer both quantitative and qualitative insight into how people discuss and perceive â€œSIMPâ€ behavior in YouTube comments.\n\nsource(\"Word_cloud_visualization.R\")\n\n\n\n\n\n\n\n\nWord Cloud generated for: traditional_common_words_combined.csv\n\n\n\n# Sort by frequency and assign ranks\nzipf_data_ranked &lt;- Traditional_Chinese_data_cleaned %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  mutate(rank = row_number())\n\n# Print the top 5 ranked words to confirm the data structure\nprint(head(zipf_data_ranked, 5))\n\n  traditional_text total_count rank\n1           ç‚ºä»€éº¼         254    1\n2           ä¸çŸ¥é“         172    2\n3           å—å®³è€…         118    3\n4              pua         116    4\n5           æ˜¯ä¸æ˜¯         101    5\n\n# --- Linear Scale (As Requested) ---\n\nggplot(zipf_data_ranked, aes(x = rank, y = total_count)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"darkorange\", size = 1.5) +\n  geom_text(\n    # Label the top 8 words\n    aes(label = ifelse(rank &lt;= 6, traditional_text, \"\")),\n    vjust = -0.8,\n    size = 3.5,\n    check_overlap = TRUE # Prevents overlapping labels\n  ) +\n  labs(\n    title = \"Zipfâ€™s Law: Word Rank vs Frequency\",\n    x = \"Rank of Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nTranslation\n\n\n\nRank\nWord\nTranslate\n\n\n1\nç‚ºä»€éº¼\nâ€œWhy / Why is it thatâ€¦â€\n\n\n2\nä¸çŸ¥é“\nâ€œDonâ€™t knowâ€\n\n\n3\nå—å®³è€…\nâ€œVictimâ€\n\n\n4\npua\nâ€œPUAâ€\n\n\n5\næ˜¯ä¸æ˜¯\nâ€œIs it / Is it not?â€\n\n\n6\nå¥³æœ‹å‹\nâ€œGirlfriendâ€"
  },
  {
    "objectID": "604_Final_check-in_2.html#word-embedding",
    "href": "604_Final_check-in_2.html#word-embedding",
    "title": "DACSS604_Final_Project",
    "section": "Word Embedding",
    "text": "Word Embedding\nFor semantic analysis, I applied Word2Vec using a pseudo-document approach to capture relationships between words in the comments. Each word was repeated according to its frequency (total_count) to create co-occurrence information, which is essential for small datasets where natural co-occurrences are limited. The repeated words were then combined into a single space-separated pseudo-document and used to train a skip-gram Word2Vec model with a vector dimension of 50, window size of 5, and 50 iterations, setting min_count = 1 to include all words.\nThe resulting word vectors allow calculation of cosine similarity to examine semantic relationships between words, as well as clustering and downstream supervised learning tasks. For example, the vector for a keyword such as â€œç‚ºä»€éº¼â€ can be compared with all other word vectors to identify the top semantically similar words, revealing patterns in how concepts related to â€œSIMPâ€ behavior are discussed in YouTube comments. This approach provides a robust representation of word meaning in the context of the dataset while accommodating the limited co-occurrence information inherent in smaller comment datasets.\n\n# Word2Vec can be the best option for the word embeding.\n\nsource(\"Word Embeddings.R\")\n\nWarning: package 'word2vec' was built under R version 4.4.3\n\n\nWarning: package 'text2vec' was built under R version 4.4.3"
  },
  {
    "objectID": "604_Final_check-in_2.html#sentiment-analysis",
    "href": "604_Final_check-in_2.html#sentiment-analysis",
    "title": "DACSS604_Final_Project",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nFor sentiment analysis, I applied a custom Chinese sentiment dictionary tailored to the context of â€œSIMPâ€ behavior. The dictionary categorizes words into three groups: positive (supportive or relationship-related words such as â€œå¥³æœ‹å‹â€ and â€œé—œå¿ƒâ€), negative (critical or unfairness-related words such as â€œä¸å€¼å¾—â€ and â€œä¸å…¬å¹³â€), and behavior (attention-seeking or â€œsimpâ€ behavior words such as â€œpuaâ€ and â€œè¿½æ±‚â€). Using R, I computed sentiment scores for each word in the dataset by summing occurrences in these categories. A raw polarity score was calculated as the sum of positive and behavior counts minus negative counts, then normalized by the total occurrences of all dictionary words to produce a relative polarity measure.\nThe analysis revealed that the current positive and negative categories do not fully capture the sentiment expressed in the comments. Some words were misclassified or contextually ambiguous, highlighting that the dictionary needs further adjustment and refinement to improve accuracy. Polarity distributions were visualized using a histogram, providing an overview of how positive, negative, and behavior-related language appears in discussions of â€œSIMPâ€ behavior. This approach provides a preliminary sentiment assessment while acknowledging the limitations of the existing dictionary.\n\nsource(\"Sentiment Analysis.R\")\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” forcats   1.0.0     âœ” lubridate 1.9.4\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– NLP::annotate()         masks ggplot2::annotate()\nâœ– httr::content()         masks NLP::content()\nâœ– dplyr::filter()         masks stats::filter()\nâœ– jsonlite::flatten()     masks purrr::flatten()\nâœ– rvest::guess_encoding() masks readr::guess_encoding()\nâœ– dplyr::lag()            masks stats::lag()\nâœ– koRpus::tokenize()      masks readr::tokenize()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'quanteda.textmodels' was built under R version 4.4.3\n\n\nWarning: package 'stopwords' was built under R version 4.4.3\n\n\n\nAttaching package: 'stopwords'\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nWarning: package 'caret' was built under R version 4.4.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:httr':\n\n    progress\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n[1] \"DFM after Custom Simp Dictionary Lookup (Counts per category):\"\n\n### SUPERVISED LEARNING (Naive Bayes Classification)\n[1] \"Confusion Matrix:\"\n\n### LEXICON-BASED NTUSD SENTIMENT RESULTS\n[1] \"Mean Sentiment Score by Simping Label (NTUSD):\"\n# A tibble: 2 Ã— 2\n  contains_simp_factor mean_sentiment\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 FALSE                       -0.0990\n2 TRUE                        -0.477 \n\n\n\n\n\n\n\n\n\nIn this graph:\nX-Axis: Simping Label:\n\nFALSE: Comments that do not contain any of the words from your custom â€œsimpâ€ dictionary (e.g., â€œèˆ”ç‹—,â€ â€œå·¥å…·äºº,â€ â€œä¸€å»‚æƒ…é¡˜,â€ etc.).\nTRUE: Comments that do contain at least one word from your custom â€œsimpâ€ dictionary.\n\nY-Axis: Comment Sentiment Score:\n\nPositive Scores (above 0): Indicate a more positive overall sentiment.\nZero (0): Indicates a neutral or balanced sentiment.\nNegative Scores (below 0): Indicate a more negative overall sentiment."
  },
  {
    "objectID": "604_Final_check-in_2.html#sentiment-analysis-summary",
    "href": "604_Final_check-in_2.html#sentiment-analysis-summary",
    "title": "DACSS604_Final_Project",
    "section": "Sentiment Analysis Summary",
    "text": "Sentiment Analysis Summary\nThe lexicon-based sentiment analysis, utilizing the NTUSD dictionary, reveals a pronounced negative emotional shift in texts discussing the â€œsimpâ€ phenomenon. Specifically, content that contains terms from the custom dictionaryâ€”which targets themes like â€œsimp behaviorâ€ (e.g., èˆ”ç‹—, simp), â€œvictim positionâ€ (e.g., å—å®³è€…, pua), and â€œrelationship imbalanceâ€â€”shows a highly negative mean sentiment score of -0.477. This score is significantly more negative than the average score of -0.0990 found in texts that do not contain these specific terms. This sharp difference (a nearly five-fold increase in negative sentiment magnitude) indicates that conversations about excessive one-sided effort, perceived exploitation, and unequal relationshipsâ€”the core of the â€œsimpâ€ conceptâ€”are strongly associated with negative emotional discourse within the corpus."
  },
  {
    "objectID": "604_Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "href": "604_Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "title": "DACSS604_Final_Project",
    "section": "Supervised Learning Analysis (Naive Bayes Classification)",
    "text": "Supervised Learning Analysis (Naive Bayes Classification)\nThe Naive Bayes model was employed to classify comments based on whether they contained the â€œsimpâ€ factor, using a cleaned feature set that excluded all words from the custom â€œsimpâ€ dictionary to prevent data leakage. The model achieved an overall Accuracy of 81.69%, which is slightly higher than the No Information Rate (NIR) of 80.36%, indicating its performance is marginally better than random guessing based on class prevalence.\nHowever, a closer look at the results reveals a significant class imbalance issue and skewed performance:\n\nHigh Sensitivity (Recall): The model is excellent at correctly identifying comments that do NOT contain the simp factor (the majority class, FALSE), with a high Sensitivity of 95.72%.\nLow Specificity: Conversely, the model is very poor at correctly identifying comments that DO contain the simp factor (the minority class, TRUE), with a low Specificity of 24.29%.\nKappa Value: The Kappa statistic of 0.2565 suggests only a fair level of agreement beyond chance.\n\nIn summary, the high overall accuracy is largely driven by correctly classifying the prevalent negative class (FALSE). The model struggles to reliably identify actual â€œsimpâ€ comments (TRUE), suggesting that the remaining general vocabulary in the comments lacks sufficient predictive power to consistently distinguish between the two categories without the core dictionary terms.\n\nsource(\"Supervised_Learning.R\")\n\n\n--- Solving Data Leakage: Remove the word which exist in Simp dictionary ---\n[1] \"Original (matrix_main): 2703\"\n[1] \"Remove the word in Simp dictionary (X_cleaned): 2674\"\n\n--- 6. Naive Bayes Training ---\n\n--- Naive Bayes Prediction ---\n[1] \"Confusion Matrix:\"\nConfusion Matrix and Statistics\n\n                 y_test\npredicted_cleaned FALSE TRUE\n            FALSE   693  134\n            TRUE     31   43\n                                        \n               Accuracy : 0.8169        \n                 95% CI : (0.79, 0.8416)\n    No Information Rate : 0.8036        \n    P-Value [Acc &gt; NIR] : 0.1676        \n                                        \n                  Kappa : 0.2565        \n                                        \n Mcnemar's Test P-Value : 2.011e-15     \n                                        \n            Sensitivity : 0.9572        \n            Specificity : 0.2429        \n         Pos Pred Value : 0.8380        \n         Neg Pred Value : 0.5811        \n              Precision : 0.8380        \n                 Recall : 0.9572        \n                     F1 : 0.8936        \n             Prevalence : 0.8036        \n         Detection Rate : 0.7691        \n   Detection Prevalence : 0.9179        \n      Balanced Accuracy : 0.6001        \n                                        \n       'Positive' Class : FALSE"
  },
  {
    "objectID": "604_Final_check-in_2.html#topic-modeling-lda",
    "href": "604_Final_check-in_2.html#topic-modeling-lda",
    "title": "DACSS604_Final_Project",
    "section": "Topic Modeling (LDA)",
    "text": "Topic Modeling (LDA)\nThe script follows a standard text mining workflow using the tidyverse and text2vec packages:\n\nData Preparation: It reads the individual tokenized comment files, reconstructs the full comments by assigning and aggregating tokens by a unique document ID (doc_id), and then combines the tokens back into complete text strings.\nFeature Engineering: It creates an iterator from the aggregated text and builds a vocabulary. Crucially, it prunes the vocabulary by removing words that occur less than three times (term_count_min = 3), which helps reduce noise and improves the quality of the derived topics.\nDTM Creation: The processed tokens are converted into a Document-Term Matrix (DTM), which is the input required for LDA.\nModel Training: The script initializes and trains an LDA model with a predefined number of topics (K=8) and 500 iterations.\nOutput: Finally, the code extracts and saves two key results: the Topic-Word distribution (the top 10 most characteristic words for each of the 8 topics) and the Document-Topic distribution (the probability that each comment belongs to each topic), storing both as CSV files for subsequent qualitative analysis.\n\n\nsource(\"Topic_Model.R\")\n\n[1] \"Starting LDA Topic Modeling with K = 8\"\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======================================================================| 100%\n[1] \"LDA Training Complete.\"\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nâ„¹ Using compatibility `.name_repair`.\n\n\n[1] \"Top 10 Words for Each Topic:\"\n# A tibble: 10 Ã— 9\n   Topic_Word_Rank Topic_0 Topic_1 Topic_2 Topic_3  Topic_4  Topic_5 Topic_6\n   &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1 Word_1          pua     ä¸çŸ¥é“  å—å®³è€…  æ˜¯ä¸æ˜¯   ç‚ºä»€éº¼   ä¸çŸ¥é“  è€Œä¸æ˜¯ \n 2 Word_2          ä¸ºä»€ä¹ˆ  ä¸‡ä½™å…ƒ  ä¸ºä»€ä¹ˆ  å¥³æœ‹å‹   åœ¨ä¸€èµ·   è¶Šæ¥è¶Š  é€™ä»¶äº‹ \n 3 Word_3          ä¸­å›½äºº  pua     pua     ä¸ºä»€ä¹ˆ   ä¹Ÿå¯ä»¥   å¤§éƒ¨åˆ†  ä¸çŸ¥é“ \n 4 Word_4          ç”·å­©å­  å¯èƒ½æ˜¯  çœŸçš„æ˜¯  ä¸çŸ¥é“   ä¸å¯èƒ½   éƒ½ä¸æ˜¯  ä¸€å®šæ˜¯ \n 5 Word_5          å¥³å­©å­  ä¸€å€‹äºº  éº¦å½“åŠ³  ä¸éœ€è¦   éƒ½æ²’æœ‰   çœŸçš„æ˜¯  å¥³æœ‹å‹ \n 6 Word_6          ä¹Ÿä¸æ˜¯  é€™ä»¶äº‹  å®¶åº­çš„  ä¹Ÿæ²¡æœ‰   é€™å°±æ˜¯   å—å®³è€…  èƒ½ä¸èƒ½ \n 7 Word_7          æ˜¯ä¸æ˜¯  ä¸­å›½äºº  å…¨ä¸–ç•Œ  å¤§éƒ¨åˆ†   ä¹Ÿæ²’æœ‰   å¥³æœ‹å‹  äººæ°‘å¹£ \n 8 Word_8          æ²¡ä»€ä¹ˆ  å°¤å…¶æ˜¯  å…¶å®æ˜¯  ç”·æœ‹å‹   ç”·å°Šå¥³å‘ æˆ‘çŸ¥é“  åƒåœ¾æ¡¶ \n 9 Word_9          å¯èƒ½æ˜¯  çœŸçš„æ˜¯  å¥½åƒæ˜¯  ç”·å¥³å¹³ç­‰ æ²’ä»€éº¼   æ˜¯ä¸æ˜¯  æœ‰äº›äºº \n10 Word_10         ä¸å€¼å¾—  ç‚ºä»€éº¼  è‚¯å®šæ˜¯  æ‰€è°“çš„   ä¸å­˜åœ¨   ä¹Ÿä¸æ˜¯  ä¸ºä»€ä¹ˆ \n# â„¹ 1 more variable: Topic_7 &lt;chr&gt;\n[1] \"Saved topic words to lda_topic_words.csv\"\n[1] \"Document-Topic Distribution (Head):\"\n# A tibble: 6 Ã— 9\n  doc_id    V1    V2    V3    V4    V5    V6    V7    V8\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1     0  0      0    0      0     0       0     0\n2      2     0  0      0    0      0     0       0     0\n3      3     0  0.05   0    0.15   0.6   0.2     0     0\n4      4     0  0.1    0.2  0      0.4   0.3     0     0\n5      5     0  0      0.8  0      0.2   0       0     0\n6      6     0  0      0    0      0     0       1     0\n[1] \"Saved document-topic distribution to lda_doc_topic_distr.csv\"\n\n\nTranslation for the every words in the topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic_Word_Rank\nTopic_0\nTopic_1\nTopic_2\nTopic_3\nTopic_4\nTopic_5\nTopic_6\nTopic_7\n\n\nWord_1\nPUA\nDonâ€™t know\nVictim\nIs it?\nWhy\nDonâ€™t know\nAnd not\nBuffet (Resource)\n\n\nWord_2\nWhy\nOver 10k yuan\nWhy\nGirlfriend\nBe together\nMore and more\nThis matter\nHighway\n\n\nWord_3\nChinese people\nPUA\nPUA\nWhy\nAlso can\nMajority\nDonâ€™t know\nToo good\n\n\nWord_4\nBoy / Male\nPossibly\nTruly is\nDonâ€™t know\nImpossible\nAre not all\nMust be\nWhy\n\n\nWord_5\nGirl / Female\nA person\nMcDonaldâ€™s\nDonâ€™t need\nDonâ€™t have at all\nTruly is\nGirlfriend\nMainly is\n\n\nWord_6\nAlso is not\nThis matter\nFamilyâ€™s\nAlso donâ€™t have\nThis is\nVictim\nCan or cannot\nSome people\n\n\nWord_7\nIs it?\nChinese people\nWhole world\nMajority\nAlso donâ€™t have\nGirlfriend\nRMB (Money)\nInequality\n\n\nWord_8\nNothing much\nEspecially\nActually is\nBoyfriend\nMale Superiority\nI know\nTrash Can (Worthless)\nShould be\n\n\nWord_9\nPossibly\nTruly is\nSeems like\nGender Equality\nNothing much\nIs it?\nSome people\nA person\n\n\nWord_10\nNot worth it\nWhy\nDefinitely is\nSo-called\nDoes not exist\nAlso is not\nWhy\nGender Equality\n\n\n\nThe interpretation for each topic\n\n\n\n\n\n\n\n\nTopic\nCore Keywords & Interpretation\nSuggested Topic Label\n\n\nTopic 0\nThis topic strongly links the PUA phenomenon with discussions about specific gender roles and identities within the Chinese context. The presence of â€œNot worth itâ€ suggests this cluster is focused on evaluating the value of actions/relationships under the PUA framework.\nPUA & Gender Dynamics in China\n\n\nTopic 1\nThis topic mixes uncertainty and specific financial figures (Over 10k yuan), directly linked to PUA. It suggests discussions about high-stakes financial loss or investment by an individual in a relationship where the outcome or reality is unclear.\nFinancial Dimension of PUA & Uncertainty\n\n\nTopic 2\nThe simultaneous presence of â€œVictim,â€ â€œPUA,â€ and â€œTruly isâ€ indicates a core discussion cluster dedicated to validating the existence and reality of being exploited. â€œMcDonaldâ€™sâ€ implies cheap/casual provision, while â€œFamilyâ€™sâ€ suggests the conversation may touch on the origins or impact of these dynamics within a family unit.\nValidating Victimhood & Low-Cost Exploitation\n\n\nTopic 3\nThis topic is saturated with questioning terms (â€œIs it?â€, â€œWhy?â€, â€œDonâ€™t knowâ€), applied directly to boyfriend/girlfriend roles and the concept of gender equality. It represents a pervasive atmosphere of skepticism and critical discussion about expected behavior in modern relationships.\nSkepticism & Questioning of Relationship Roles\n\n\nTopic 4\nA highly polarized topic that denies (ä¸å¯èƒ½, ä¸å­˜åœ¨) the relevance or existence of Male Superiority (ç”·å°Šå¥³å‘). It focuses on the possibility of being together (åœ¨ä¸€èµ·), suggesting a desire for modern, equal partnerships and a strong rejection of patriarchal norms.\nDenial of Traditional Patriarchy in Relationships\n\n\nTopic 5\nTerms like â€œMore and moreâ€ and â€œMajorityâ€ point to a discussion of social trends and scale. When combined with â€œVictimâ€ and â€œGirlfriend,â€ it indicates a conversation about whether victimhood is becoming increasingly common or if the perception of victimhood is changing within the female partner role.\nDiscussing Shifting Social Norms & Victim Pool\n\n\nTopic 6\nThis is the most explicitly transactional topic. It discusses financial payment (RMB) and the concept of a person being reduced to a â€œTrash Canâ€ (worthless/emotional dumping ground). The use of â€œAnd notâ€ suggests a debate over what a relationship should be versus what it currently is (i.e., not a transaction, but one of money/exploitation).\nMonetary Value vs.Â Emotional Worth (The Price of Simping)\n\n\nTopic 7\nThis topic links resource provision (implied by â€œBuffetâ€ and â€œHighway,â€ often used as metaphors for free/easy access) with discussions of Inequality and Gender Equality. It debates whether resources should be provided freely, who is responsible for providing them, and the resulting fairness in the relationship structure.\nResource Provision & Equality Debate\n\n\n\n\n\n\nDTM\nTopic_Word_Rank\n\n\nV1\nTopic_0\n\n\nV2\nTopic_1\n\n\nV3\nTopic_2\n\n\nV4\nTopic_3\n\n\nV5\nTopic_4\n\n\nV6\nTopic_5\n\n\nV7\nTopic_6\n\n\nV8\nTopic_7"
  },
  {
    "objectID": "604_Final_check-in_2.html#causal-inference",
    "href": "604_Final_check-in_2.html#causal-inference",
    "title": "DACSS604_Final_Project",
    "section": "Causal Inference",
    "text": "Causal Inference\nIn the casual inference part, I present the Ordinary Least Squares (OLS) regression model to analyze how the probability of eight LDA topics influences the number of likes received by a comment (likeCount). Topic V8 (PUA/Victim) was set as the reference group (Reference Topic) in the model. Overall, the modelâ€™s explanatory power is extremely low (\\(\\text{Adjusted R-squared} = 0.00025\\)), suggesting that the variation in likeCount is primarily influenced by factors outside the model, rather than the topics themselves. However, the coefficient tests for individual topics revealed that Topic V2 demonstrated a statistically significant positive influence. After controlling for the effects of other topics, an increase of 1 unit in the probability of Topic V2 (Financial/Money II), relative to the reference group V8, is expected to increase the number of likes by approximately 9.66 (\\(p = 0.038^{*}\\)). This suggests that specific discussion content related to money or finance is more likely to garner attention and agreement within the community. Apart from the intercept, the remaining topics (V1, V3, V4, V5, V6, and V7) did not show a statistically significant relationship with the number of likes.\n\nsource(\"Causal_Inference.R\")\n\nRows: 3006 Columns: 9\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (9): doc_id, V1, V2, V3, V4, V5, V6, V7, V8\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n[1] \"--- OLS Regression Results (Outcome: likeCount) ---\"\n[1] \"Reference Topic: V8 (PUA/Victim)\"\n\nCall:\nlm(formula = likeCount ~ V1 + V2 + V3 + V4 + V5 + V6 + V7, data = merged_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -18.59   -8.93   -6.76   -5.12 1763.41 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.934      2.301   3.882 0.000106 ***\nV1            -2.424      4.144  -0.585 0.558600    \nV2             9.655      4.656   2.074 0.038172 *  \nV3            -2.701      4.312  -0.626 0.531107    \nV4            -2.380      4.266  -0.558 0.576978    \nV5            -2.815      4.399  -0.640 0.522298    \nV6             1.729      4.351   0.398 0.691027    \nV7            -0.953      4.363  -0.218 0.827099    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.08 on 2998 degrees of freedom\nMultiple R-squared:  0.002579,  Adjusted R-squared:  0.00025 \nF-statistic: 1.107 on 7 and 2998 DF,  p-value: 0.3553"
  },
  {
    "objectID": "604_Final_check-in_2.html#conclusion",
    "href": "604_Final_check-in_2.html#conclusion",
    "title": "DACSS604_Final_Project",
    "section": "Conclusion",
    "text": "Conclusion\n(PUT THESE TWO POINT INTO THE FUTURE WORK ALSO NEED TO EXPLAIN ABOUT DIFFICULTY TO COLLECT THE ACADEMIC DATA THERE ** SIMP DOES NOT HAVE A FORMAL DEFINITION**\nâ€œSynthesizing the projectâ€™s findings, the primary discovery is that community engagement (\\(\\text{likeCount}\\)) on YouTube comments is not driven by broad emotional tone or general topics, but rather by a specific, critical narrative focused on â€˜financial exploitation and victimhoodâ€™ (\\(\\text{Topic}\\) \\(\\text{V2}\\)). This specific form of critical discussion related to â€˜SIMPâ€™ behavior is extremely negative in sentiment (\\(\\text{mean}\\) \\(\\text{sentiment} = -0.477\\)) and is so unique in its linguistic pattern that its occurrence can be accurately predicted by the supervised learning model. Consequently, the communityâ€™s response to â€˜SIMPâ€™ behavior is highly concentrated and emotionally charged, with its online visibility predominantly stemming from comments that link the behavior directly to concrete financial inequality and victim scenarios.â€"
  },
  {
    "objectID": "604_Final_check-in_2.html#future",
    "href": "604_Final_check-in_2.html#future",
    "title": "DACSS604_Final_Project",
    "section": "Future",
    "text": "Future\nFuture work should prioritize addressing the observed limitations in both the supervised classification model and the initial data preprocessing pipeline to enhance the robustness and explanatory power of the analysis. Firstly, while the initial Naive Bayes classifier provided baseline insights, its predictive performance should be critically re-evaluated. Improving the accuracy of the automated simp classification label requires exploring more sophisticated machine learning techniques, such as Support Vector Machines (SVMs), Gradient Boosting, or even Transformer-based deep learning models. Concurrently, enhancing the feature set by refining or expanding the custom dictionariesâ€”perhaps incorporating sentiment scores or incorporating word embeddingsâ€”could significantly boost the modelâ€™s ability to discriminate between classes, moving beyond simple bag-of-words approaches. Secondly, a crucial area for improvement lies in the token filtering and data processing stage. Despite standard removal procedures, the presence of numerous contextually irrelevant tokens, such as specific objects (â€œé«˜é€Ÿå…¬è·¯â€) and brands (â€œéº¥ç•¶å‹â€), confirms the necessity of a more rigorous, domain-specific cleanup. Future efforts must focus on constructing an expanded, domain-aware stop word list or implementing Named Entity Recognition (NER) to systematically identify and remove non-topical, low-information tokens, ensuring the remaining features are highly predictive and representative of the core concepts being discussed."
  },
  {
    "objectID": "604_Final_check-in_2.html#reference",
    "href": "604_Final_check-in_2.html#reference",
    "title": "DACSS604_Final_Project",
    "section": "Reference",
    "text": "Reference\nHO, Daniel. The (simp)le truth about excessive & obsessive romantic behaviors in men. (2023). https://ink.library.smu.edu.sg/etd_coll/516\nKrishnamurthy, V., & Duan, Y. (2017). Dependence Structure Analysis Of Meta-level Metrics in YouTube Videos: A Vine Copula Approach. arXiv preprint arXiv:1712.10232. â€œto explain the comment and the view of the video are relatedâ€\nLun-Wei Ku and Hsin-Hsi Chen (2007). Mining Opinions from the Web: Beyond Relevance Retrieval. Journal of American Society for Information Science and Technology, Special Issue on Mining Web Resources for Enhancing Information Retrieval, 58(12), pages 1838-1850.\nPew Research Center. (2020). Many Americans get news on YouTube, where news organizations and independent producers thrive side by side. https://www.pewresearch.org/journalism/2020/09/28/many-americans-get-news-on-youtube-where-news-organizations-and-independent-producers-thrive-side-by-side/\nZhou, W. (2024). é‡åº†è­¦æ–¹å‘å¸ƒâ€œèƒ–çŒ«â€äº‹ä»¶è­¦æƒ…é€šæŠ¥ [Chongqing police issue incident report on the â€œPangmaoâ€ incident]. Xinhua Net. http://www.news.cn/politics/20240519/fb56352660c94810a58e79bc18459a3e/c.html"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "View Full Analysis & Report (HTML)"
  },
  {
    "objectID": "projects.html#final-project-check-in-2",
    "href": "projects.html#final-project-check-in-2",
    "title": "Projects",
    "section": "",
    "text": "604_Final_check-in_2.qmd ## Project 2"
  },
  {
    "objectID": "projects.html#project-4",
    "href": "projects.html#project-4",
    "title": "Projects",
    "section": "Project 4",
    "text": "Project 4"
  },
  {
    "objectID": "Final_check-in_2.html",
    "href": "Final_check-in_2.html",
    "title": "DACSS785_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "Final_check-in_2.html#research-question-and-hypothesis",
    "href": "Final_check-in_2.html#research-question-and-hypothesis",
    "title": "DACSS785_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "Final_check-in_2.html#data-collection",
    "href": "Final_check-in_2.html#data-collection",
    "title": "DACSS785_Final_Project",
    "section": "Data Collection",
    "text": "Data Collection\nTo explore the concept of â€œsimping,â€ I collected YouTube comments from six relevant videos for textual analysis. I utilized an R scraping script to extract approximately 8,000 comments in total. Following a cleaning and filtering process, a dataset of around 7,000 practical comments was retained for analysis.\nI specifically focused on the case study known as the â€œèƒ–è²“è·³æ±Ÿäº‹ä»¶â€ (Suicide of Fat Cat). This event, which occurred in Mainland China, provides a particularly rich and relevant dataset because it was a well-documented news story officially reported by the Chinese court. This official documentation makes it a real and verifiable event, distinguishing it from mere rumors or social media anecdotes. Furthermore, the use of Chinese-language videos as the reference source is critical, as the Chinese-speaking audience possesses extensive background knowledge and cultural context directly related to the local details of this incident.\n\nSIMP001 - é™ªæ‰“éŠæˆ²è³ºç™¾è¬é¤Šå¥³å‹æ…˜é­åˆ†æ‰‹ï¼ã€Œèƒ–è²“äº‹ä»¶ã€å¼•çˆ†ä¸­åœ‹æ€§åˆ¥æˆ°çˆ­ï¼Ÿã€Œæ’ˆå¥³ã€æ»¿è¡—è·‘çš„èƒŒå¾ŒåŸå› ï¼Ÿã€TODAY çœ‹ä¸–ç•Œã€‘(https://www.youtube.com/watch?v=o5TfkwlthWU&t=13s) 1952 comments from 11/04/2025\nSIMP002 - å½“èƒ–çŒ«é‡åˆ°æå¥³ï¼Œä¸€ä¸ªå¹´è½»äººå¦‚ä½•èµ°ä¸Šä¸å½’è·¯ï¼Ÿï½œå¥³æƒï½œæå¥³ï½œèƒ–çŒ«ï½œç‹è€…è£è€€ï½œç”·å¥³å¹³æƒï½œæ—¥æœ¬ï½œæ¢…å¤§é«˜é€Ÿï½œèˆ†è®ºæ§åˆ¶ï½œç‹å±€æ‹æ¡ˆ20240507 (https://www.youtube.com/watch?v=39Gq_eOPuDY&t=1s) 3731 comments from 11/04/2025\nSIMP003 - è€æ¢ï¼šç»™â€œèƒ–çŒ«â€å¤šæ¡é€‰æ‹© é‡åº†â€œèƒ–çŒ«äº‹ä»¶â€ä¸æ˜¯æ€§åˆ«å¤§æˆ˜ å¦‚ä½•é¿å…æˆä¸ºâ€œèƒ–çŒ«â€(https://www.youtube.com/watch?v=mjcgg0wFpfE) 997 comments from 11/04/2025\nSIMP004 - å°ä¼™ç‚ºæ„›è·³æ±Ÿï¼Œæ‹œé‡‘çš„å¥³å‹ï¼Œå¸è¡€çš„è¦ªå§ï¼Œç„¡è‰¯çš„å•†å®¶ï¼Œç˜‹ç‹‚çš„ç¶²æ°‘ï¼Œèª°æ‰æ˜¯åŠ å®³è€…ï¼Ÿç‚ºä½•è­¦å¯Ÿèªå®šå¥³å‹ç„¡ç½ªï¼Œåè€Œæ˜¯è¦ªå§é•äº†æ³•ï¼Ÿä¸€å£æ°£çœ‹å®Œèƒ–è²“äº‹ä»¶å§‹æœ«ï¼| Wayneèª¿æŸ¥(https://www.youtube.com/watch?v=igs7GoIU4MU) 615 comments from 11/04/2025\nSIMP005 - ç¥ç´šé™ªç©ã€Œèƒ–è²“ã€é­è©ä¹¾227è¬äº¡ å¥³å‹é“æ­‰ï½œ20240506 ETåˆé–“æ–°è (https://www.youtube.com/watch?v=tAE83zZEcOY) 402 comments from 11/04/2025\nSIMP006 - è¢«æ’ˆå¥³é¨™å…‰50è¬ï¼ŒéŠæˆ²å®…ç”·è·³æ±Ÿè‡ªæ®ºï¼Œè½Ÿå‹•å…¨ç¶²ï¼æ’ˆå¥³è­šç«¹æ¦¨ä¹¾èƒ–è²“äº‹ä»¶çœŸç›¸ï¼ã€æ–°é—»æœ€å˜²ç‚¹ å§œå…‰å®‡ã€2024.0508(https://www.youtube.com/watch?v=YYngd2Yt3zk) 271 comments from 11/04/2025\n\n\nlibrary(plyr)\n\nWarning: package 'plyr' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.4.3\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(purrr)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:plyr':\n\n    compact\n\nlibrary(chromote)\n\nWarning: package 'chromote' was built under R version 4.4.3\n\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tibble)\nlibrary(nnet)\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.4.3\n\n\ncorrplot 0.95 loaded\n\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.4.3\n\n\nLoading required package: NLP\n\n\nWarning: package 'NLP' was built under R version 4.4.2\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.4.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.4.3\n\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.4.3\n\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.4.3\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(\"quanteda.textplots\")\n\nWarning: package 'quanteda.textplots' was built under R version 4.4.3\n\nlibrary(httr)\n\nWarning: package 'httr' was built under R version 4.4.3\n\n\n\nAttaching package: 'httr'\n\n\nThe following object is masked from 'package:NLP':\n\n    content\n\nlibrary(RColorBrewer)\nlibrary(RedditExtractoR)\n\nWarning: package 'RedditExtractoR' was built under R version 4.4.3\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.4.3\n\nlibrary(tidyr)\n\n\ndata1 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\ndata2 &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments.csv\")\ndata3 &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments.csv\")\ndata4 &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments.csv\")\ndata5 &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments.csv\")\ndata6 &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments.csv\")"
  },
  {
    "objectID": "Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "href": "Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "title": "DACSS785_Final_Project",
    "section": "Data Cleaning (info for the poster",
    "text": "Data Cleaning (info for the poster\nALSO GUIDE IT TO THE PART WHICH I WANT PLUS PRESENT THE CLEAN FORMAT IN THE POSTER\nThe raw dataset, collected as several CSV files, initially contained detailed comment metadata. The original structure included columns such as: videoId, commentId, parentId, author, text, likeCount, publishedAt, updatedAt, viewerRating, canRate, and reply.\nFor data cleaning, all CSV files in the Final_project_data folder were systematically processed using the R environment. The initial step was to streamline the dataset by retaining only the essential variables for textual and engagement analysis: text, likeCount, and reply.\nI utilized the R packages dplyr and stringr to focus on standardizing the text column. This involved a series of cleaning operations: normalization of whitespace (removing line breaks, tabs, and extra spaces, and trimming leading/trailing whitespace) and character filtering. Crucially, I removed non-essential symbols and unusual characters while meticulously preserving all Chinese characters to ensure the comments remained culturally authentic and meaningful for subsequent analysis.\nFinally, each cleaned and standardized dataset was saved as a new CSV file, appended with the suffix _cleaned. UTF-8 encoding was explicitly used to guarantee the accurate representation of the Chinese characters. This systematic workflow ensures the comment data are tidy, standardized, and immediately ready for downstream procedures, such as tokenization and sentiment or frequency analysis.\n\nsource(\"data_cleaning_CN.R\")\n\ndata cleaning complete!.\n\nSIMP001 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\n\n#Present the eample of the result\nhead(SIMP001)\n\n      videoId                  commentId parentId        author\n1 o5TfkwlthWU UgyekRC230MDXREkdeN4AaABAg     &lt;NA&gt;  @DanjonMeshi\n2 o5TfkwlthWU UgxangSP0zjJm6_gHfV4AaABAg     &lt;NA&gt;  @paullee4451\n3 o5TfkwlthWU UgwZdLtl6Eb2wgDWaDV4AaABAg     &lt;NA&gt;      @urikora\n4 o5TfkwlthWU UgwSmeqUyHYUXGD6l3l4AaABAg     &lt;NA&gt; @fayechen1928\n5 o5TfkwlthWU UgwpotCAmJmn2wWU7u54AaABAg     &lt;NA&gt; @running_goat\n6 o5TfkwlthWU UgxQSyQbnLT9r7I1faB4AaABAg     &lt;NA&gt;  @Jack2006103\n                                                                                                                       text\n1                                                             å•†å®¶é›†é«”çµ¦ç©ºè¢‹çœŸçš„ç¬‘æ­»ï¼Œä¸æ­¢ç”Ÿæ´»åœ¨ä¸­åœ‹ï¼Œé€£æ­»åœ¨ä¸­åœ‹éƒ½è¦å·è‘—æ¨‚ğŸ˜†\n2                                                                                                                      é ­é¦™\n3                       æ›´æ…˜çš„æ˜¯ï¼Œäººéƒ½èµ°äº†ä¸€å€‹æœˆ çµæœå°±åœ¨é€™æ™‚æ©Ÿé»è¢«æŠ“ä¾†æ“‹æ”¿åºœåšçš„é†œäº‹ï¼ˆè·¯å´©è¯ç‚ºè»Šè¡é€²å»å‘æ´ç„¶å¾Œå¿«é€Ÿç‡ƒèµ·ä¾†ï¼‰\n4                                                                                      æ¯æ¬¡çœ‹åˆ°ä¸­åœ‹é€™ç¨®æ‚²åŠ‡éƒ½è¦ºå¾—ä¸å¯æ€è­°ğŸ˜¨ğŸ˜­\n5 æˆ‘çœŸçš„å¿…é ˆå¾—èªªå°å²¸ç”·å¥³æˆ°çˆ­çœŸçš„è¶Šä¾†è¶Šåš´é‡== å•éå¥½å¹¾å€‹å°å²¸çš„å¥³ç”Ÿéƒ½èªç‚ºç”·ç”Ÿå°±æ˜¯æ‡‰è©²è¦çµ¦å½©ç¦® åƒé£¯å°±æ˜¯è¦å¹«å¥³ç”Ÿä»˜éŒ¢ç­‰ç­‰ å¾ˆå¯æ€•\n6                                                                                                é€™äº›åº—å®¶åƒäººè¡€é¥…é ­ï¼Œè¶…å™çˆ›\n  likeCount          publishedAt            updatedAt viewerRating canRate\n1        58 2024-05-09T16:01:52Z 2024-05-09T16:01:52Z         none    TRUE\n2         0 2024-05-09T16:02:23Z 2024-05-09T16:02:23Z         none    TRUE\n3       345 2024-05-09T16:05:05Z 2024-05-09T16:05:05Z         none    TRUE\n4         1 2024-05-09T16:05:57Z 2024-05-09T16:05:57Z         none    TRUE\n5       139 2024-05-09T16:06:47Z 2024-05-09T16:06:47Z         none    TRUE\n6         4 2024-05-09T16:07:29Z 2024-05-09T16:07:29Z         none    TRUE\n  reply\n1 FALSE\n2 FALSE\n3 FALSE\n4 FALSE\n5 FALSE\n6 FALSE\n\ndata1_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_cleaned.csv\")\ndata2_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments_cleaned.csv\")\ndata3_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments_cleaned.csv\")\ndata4_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments_cleaned.csv\")\ndata5_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments_cleaned.csv\")\ndata6_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments_cleaned.csv\")"
  },
  {
    "objectID": "Final_check-in_2.html#preprocess-the-data",
    "href": "Final_check-in_2.html#preprocess-the-data",
    "title": "DACSS785_Final_Project",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nFor visualizing the dominant linguistic patterns within the comment data, I employed two complementary approaches. First, a Word Cloud visualization (generated using the Word_cloud_visualization.R script) provided an intuitive, qualitative representation of high-frequency words, instantly highlighting the most common terms associated with discussions of â€œSIMPâ€ behavior.\nSecond, I conducted a quantitative rank-frequency analysis by applying Zipfâ€™s Law to the word corpus. After arranging all unique words by descending frequency and assigning a rank, I plotted the resulting distribution using the ggplot2 package. The resulting visualization confirmed that the comment discourse adheres to a Zipfian distribution, where a few words account for a disproportionate share of the total vocabulary.\nThe key terms driving the discourse were clearly identifiable:\nThese visualizations collectively offer both quantitative validation (Zipfâ€™s Law distribution) and qualitative insight (Word Cloud/Top Terms) into how the audience discusses and perceives the central event and the related concept of â€œSIMPâ€ behavior in this context. The high frequency of questioning and uncertainty (ç‚ºä»€éº¼, ä¸çŸ¥é“, æ˜¯ä¸æ˜¯) coupled with terms of exploitation (pua) and suffering (å—å®³è€…) reveals a key focus on moral judgment and accountability in the discussion.\n\nsource(\"TOKENIZATION.R\")\n\nTokenization complete!\n\nSIMP001_comments_tokens &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_tokens.csv\")\n#Present the eample of the result\nhead(SIMP001_comments_tokens)\n\n  likeCount reply     word\n1       345 FALSE   è¡é€²å»\n2         1 FALSE ä¸å¯æ€è­°\n3       139 FALSE   è¶Šä¾†è¶Š\n4       139 FALSE   å¥½å¹¾å€‹\n5       141 FALSE   éº¥ç•¶å‹\n6         3 FALSE   æœ‰äººèªª\n\n\n\nsource(\"Word_frequency.R\")\n\nWord Frequency Calculation Complete!\n\nSIMP001_wordfreq &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_wordfreq.csv\")\n\n#Present the eample of the result\nhead(SIMP001_wordfreq)\n\n    word  n rank\n1 å—å®³è€… 75    1\n2 ç‚ºä»€éº¼ 67    2\n3 ä¸çŸ¥é“ 53    3\n4    pua 42    4\n5 é€™ä»¶äº‹ 34    5\n6 ä¸€å€‹äºº 32    6\n\n\n\nsource(\"Same_Word.R\")\n\nCommon words saved to: Final_project_data/common_words_across_files.csv \n\ncommon_words &lt;- read.csv(\"Final_project_data/common_words_across_files.csv\")\n\n#Present the eample of the result\nhead(common_words)\n\n    word total_count\n1 ä¸çŸ¥é“         172\n2 ä¸ºä»€ä¹ˆ         154\n3 å—å®³è€…         118\n4    pua         115\n5 æ˜¯ä¸æ˜¯         101\n6 ç‚ºä»€éº¼         100\n\n\n\nsource(\"Change_to_Traditional_Chinese.R\")\n\nWarning: package 'textstem' was built under R version 4.4.3\n\n\nLoading required package: koRpus.lang.en\n\n\nWarning: package 'koRpus.lang.en' was built under R version 4.4.3\n\n\nLoading required package: koRpus\n\n\nWarning: package 'koRpus' was built under R version 4.4.3\n\n\nLoading required package: sylly\n\n\nWarning: package 'sylly' was built under R version 4.4.3\n\n\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\n\nAttaching package: 'koRpus'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    tokens, types\n\n\nThe following object is masked from 'package:tm':\n\n    readTagged\n\n\nThe following object is masked from 'package:readr':\n\n    tokenize\n\n\nWarning: package 'tmcn' was built under R version 4.4.3\n\n\n# tmcn Version: 0.2-13\n\n\n[1] \"girl\"    \"woman\"   \"simping\" \"lover\"  \nTranslation complete! Output saved to 'your_output_file.csv'\n\nTraditional_Chinese_data_cleaned &lt;- read.csv(\"Final_project_data/traditional_common_words_combined.csv\")\n\n#Present the data after cleaning\nhead(Traditional_Chinese_data_cleaned)\n\n  traditional_text total_count\n1           ç‚ºä»€éº¼         254\n2           ä¸çŸ¥é“         172\n3           å—å®³è€…         118\n4              pua         116\n5           æ˜¯ä¸æ˜¯         101\n6           å¥³æœ‹å‹          95"
  },
  {
    "objectID": "Final_check-in_2.html#visualization",
    "href": "Final_check-in_2.html#visualization",
    "title": "DACSS785_Final_Project",
    "section": "Visualization",
    "text": "Visualization\nFor visualizing patterns in the comments, I used two approaches. First, the Word_cloud_visualization.R script generated word clouds to highlight high-frequency words, providing a clear and intuitive view of the most common terms associated with discussions of â€œSIMPâ€ behavior. Second, I applied Zipfâ€™s Law to examine the relationship between word rank and frequency. After arranging words by descending frequency and assigning ranks, I plotted all words using ggplot2, labeling only the top five most frequent words to emphasize the key terms in the discourse. The resulting visualizations offer both quantitative and qualitative insight into how people discuss and perceive â€œSIMPâ€ behavior in YouTube comments.\n\nsource(\"Word_cloud_visualization.R\")\n\n\n\n\n\n\n\n\nWord Cloud generated for: traditional_common_words_combined.csv\n\n\n\n# Sort by frequency and assign ranks\nzipf_data_ranked &lt;- Traditional_Chinese_data_cleaned %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  mutate(rank = row_number())\n\n# Print the top 5 ranked words to confirm the data structure\nprint(head(zipf_data_ranked, 5))\n\n  traditional_text total_count rank\n1           ç‚ºä»€éº¼         254    1\n2           ä¸çŸ¥é“         172    2\n3           å—å®³è€…         118    3\n4              pua         116    4\n5           æ˜¯ä¸æ˜¯         101    5\n\n# --- Linear Scale (As Requested) ---\n\nggplot(zipf_data_ranked, aes(x = rank, y = total_count)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"darkorange\", size = 1.5) +\n  geom_text(\n    # Label the top 8 words\n    aes(label = ifelse(rank &lt;= 6, traditional_text, \"\")),\n    vjust = -0.8,\n    size = 3.5,\n    check_overlap = TRUE # Prevents overlapping labels\n  ) +\n  labs(\n    title = \"Zipfâ€™s Law: Word Rank vs Frequency\",\n    x = \"Rank of Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nTranslation\n\n\n\nRank\nWord\nTranslate\n\n\n1\nç‚ºä»€éº¼\nâ€œWhy / Why is it thatâ€¦â€\n\n\n2\nä¸çŸ¥é“\nâ€œDonâ€™t knowâ€\n\n\n3\nå—å®³è€…\nâ€œVictimâ€\n\n\n4\npua\nâ€œPUAâ€\n\n\n5\næ˜¯ä¸æ˜¯\nâ€œIs it / Is it not?â€\n\n\n6\nå¥³æœ‹å‹\nâ€œGirlfriendâ€"
  },
  {
    "objectID": "Final_check-in_2.html#word-embedding",
    "href": "Final_check-in_2.html#word-embedding",
    "title": "DACSS785_Final_Project",
    "section": "Word Embedding",
    "text": "Word Embedding\nFor semantic analysis, I applied Word2Vec using a pseudo-document approach to capture relationships between words in the comments. Each word was repeated according to its frequency (total_count) to create co-occurrence information, which is essential for small datasets where natural co-occurrences are limited. The repeated words were then combined into a single space-separated pseudo-document and used to train a skip-gram Word2Vec model with a vector dimension of 50, window size of 5, and 50 iterations, setting min_count = 1 to include all words.\nThe resulting word vectors allow calculation of cosine similarity to examine semantic relationships between words, as well as clustering and downstream supervised learning tasks. For example, the vector for a keyword such as â€œç‚ºä»€éº¼â€ can be compared with all other word vectors to identify the top semantically similar words, revealing patterns in how concepts related to â€œSIMPâ€ behavior are discussed in YouTube comments. This approach provides a robust representation of word meaning in the context of the dataset while accommodating the limited co-occurrence information inherent in smaller comment datasets.\n\n# Word2Vec can be the best option for the word embeding.\n\nsource(\"Word Embeddings.R\")\n\nWarning: package 'word2vec' was built under R version 4.4.3\n\n\nWarning: package 'text2vec' was built under R version 4.4.3"
  },
  {
    "objectID": "Final_check-in_2.html#sentiment-analysis",
    "href": "Final_check-in_2.html#sentiment-analysis",
    "title": "DACSS785_Final_Project",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nFor sentiment analysis, I applied a custom Chinese sentiment dictionary tailored to the context of â€œSIMPâ€ behavior. The dictionary categorizes words into three groups: positive (supportive or relationship-related words such as â€œå¥³æœ‹å‹â€ and â€œé—œå¿ƒâ€), negative (critical or unfairness-related words such as â€œä¸å€¼å¾—â€ and â€œä¸å…¬å¹³â€), and behavior (attention-seeking or â€œsimpâ€ behavior words such as â€œpuaâ€ and â€œè¿½æ±‚â€). Using R, I computed sentiment scores for each word in the dataset by summing occurrences in these categories. A raw polarity score was calculated as the sum of positive and behavior counts minus negative counts, then normalized by the total occurrences of all dictionary words to produce a relative polarity measure.\nThe analysis revealed that the current positive and negative categories do not fully capture the sentiment expressed in the comments. Some words were misclassified or contextually ambiguous, highlighting that the dictionary needs further adjustment and refinement to improve accuracy. Polarity distributions were visualized using a histogram, providing an overview of how positive, negative, and behavior-related language appears in discussions of â€œSIMPâ€ behavior. This approach provides a preliminary sentiment assessment while acknowledging the limitations of the existing dictionary.\n\nsource(\"Sentiment Analysis.R\")\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” forcats   1.0.0     âœ” lubridate 1.9.4\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– NLP::annotate()         masks ggplot2::annotate()\nâœ– httr::content()         masks NLP::content()\nâœ– dplyr::filter()         masks stats::filter()\nâœ– jsonlite::flatten()     masks purrr::flatten()\nâœ– rvest::guess_encoding() masks readr::guess_encoding()\nâœ– dplyr::lag()            masks stats::lag()\nâœ– koRpus::tokenize()      masks readr::tokenize()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'quanteda.textmodels' was built under R version 4.4.3\n\n\nWarning: package 'stopwords' was built under R version 4.4.3\n\n\n\nAttaching package: 'stopwords'\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nWarning: package 'caret' was built under R version 4.4.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:httr':\n\n    progress\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n[1] \"DFM after Custom Simp Dictionary Lookup (Counts per category):\"\n\n### SUPERVISED LEARNING (Naive Bayes Classification)\n[1] \"Confusion Matrix:\"\n\n### LEXICON-BASED NTUSD SENTIMENT RESULTS\n[1] \"Mean Sentiment Score by Simping Label (NTUSD):\"\n# A tibble: 2 Ã— 2\n  contains_simp_factor mean_sentiment\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 FALSE                       -0.0990\n2 TRUE                        -0.477 \n\n\n\n\n\n\n\n\n\nIn this graph:\nX-Axis: Simping Label:\n\nFALSE: Comments that do not contain any of the words from your custom â€œsimpâ€ dictionary (e.g., â€œèˆ”ç‹—,â€ â€œå·¥å…·äºº,â€ â€œä¸€å»‚æƒ…é¡˜,â€ etc.).\nTRUE: Comments that do contain at least one word from your custom â€œsimpâ€ dictionary.\n\nY-Axis: Comment Sentiment Score:\n\nPositive Scores (above 0): Indicate a more positive overall sentiment.\nZero (0): Indicates a neutral or balanced sentiment.\nNegative Scores (below 0): Indicate a more negative overall sentiment."
  },
  {
    "objectID": "Final_check-in_2.html#sentiment-analysis-summary",
    "href": "Final_check-in_2.html#sentiment-analysis-summary",
    "title": "DACSS785_Final_Project",
    "section": "Sentiment Analysis Summary",
    "text": "Sentiment Analysis Summary\nThe lexicon-based sentiment analysis, utilizing the NTUSD dictionary, reveals a pronounced negative emotional shift in texts discussing the â€œsimpâ€ phenomenon. Specifically, content that contains terms from the custom dictionaryâ€”which targets themes like â€œsimp behaviorâ€ (e.g., èˆ”ç‹—, simp), â€œvictim positionâ€ (e.g., å—å®³è€…, pua), and â€œrelationship imbalanceâ€â€”shows a highly negative mean sentiment score of -0.477. This score is significantly more negative than the average score of -0.0990 found in texts that do not contain these specific terms. This sharp difference (a nearly five-fold increase in negative sentiment magnitude) indicates that conversations about excessive one-sided effort, perceived exploitation, and unequal relationshipsâ€”the core of the â€œsimpâ€ conceptâ€”are strongly associated with negative emotional discourse within the corpus."
  },
  {
    "objectID": "Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "href": "Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "title": "DACSS785_Final_Project",
    "section": "Supervised Learning Analysis (Naive Bayes Classification)",
    "text": "Supervised Learning Analysis (Naive Bayes Classification)\nThe Naive Bayes model was employed to classify comments based on whether they contained the â€œsimpâ€ factor, using a cleaned feature set that excluded all words from the custom â€œsimpâ€ dictionary to prevent data leakage. The model achieved an overall Accuracy of 81.69%, which is slightly higher than the No Information Rate (NIR) of 80.36%, indicating its performance is marginally better than random guessing based on class prevalence.\nHowever, a closer look at the results reveals a significant class imbalance issue and skewed performance:\n\nHigh Sensitivity (Recall): The model is excellent at correctly identifying comments that do NOT contain the simp factor (the majority class, FALSE), with a high Sensitivity of 95.72%.\nLow Specificity: Conversely, the model is very poor at correctly identifying comments that DO contain the simp factor (the minority class, TRUE), with a low Specificity of 24.29%.\nKappa Value: The Kappa statistic of 0.2565 suggests only a fair level of agreement beyond chance.\n\nIn summary, the high overall accuracy is largely driven by correctly classifying the prevalent negative class (FALSE). The model struggles to reliably identify actual â€œsimpâ€ comments (TRUE), suggesting that the remaining general vocabulary in the comments lacks sufficient predictive power to consistently distinguish between the two categories without the core dictionary terms.\n\nsource(\"Supervised_Learning.R\")\n\n\n--- Solving Data Leakage: Remove the word which exist in Simp dictionary ---\n[1] \"Original (matrix_main): 2703\"\n[1] \"Remove the word in Simp dictionary (X_cleaned): 2674\"\n\n--- 6. Naive Bayes Training ---\n\n--- Naive Bayes Prediction ---\n[1] \"Confusion Matrix:\"\nConfusion Matrix and Statistics\n\n                 y_test\npredicted_cleaned FALSE TRUE\n            FALSE   693  134\n            TRUE     31   43\n                                        \n               Accuracy : 0.8169        \n                 95% CI : (0.79, 0.8416)\n    No Information Rate : 0.8036        \n    P-Value [Acc &gt; NIR] : 0.1676        \n                                        \n                  Kappa : 0.2565        \n                                        \n Mcnemar's Test P-Value : 2.011e-15     \n                                        \n            Sensitivity : 0.9572        \n            Specificity : 0.2429        \n         Pos Pred Value : 0.8380        \n         Neg Pred Value : 0.5811        \n              Precision : 0.8380        \n                 Recall : 0.9572        \n                     F1 : 0.8936        \n             Prevalence : 0.8036        \n         Detection Rate : 0.7691        \n   Detection Prevalence : 0.9179        \n      Balanced Accuracy : 0.6001        \n                                        \n       'Positive' Class : FALSE"
  },
  {
    "objectID": "Final_check-in_2.html#topic-modeling-lda",
    "href": "Final_check-in_2.html#topic-modeling-lda",
    "title": "DACSS785_Final_Project",
    "section": "Topic Modeling (LDA)",
    "text": "Topic Modeling (LDA)\nThe script follows a standard text mining workflow using the tidyverse and text2vec packages:\n\nData Preparation: It reads the individual tokenized comment files, reconstructs the full comments by assigning and aggregating tokens by a unique document ID (doc_id), and then combines the tokens back into complete text strings.\nFeature Engineering: It creates an iterator from the aggregated text and builds a vocabulary. Crucially, it prunes the vocabulary by removing words that occur less than three times (term_count_min = 3), which helps reduce noise and improves the quality of the derived topics.\nDTM Creation: The processed tokens are converted into a Document-Term Matrix (DTM), which is the input required for LDA.\nModel Training: The script initializes and trains an LDA model with a predefined number of topics (K=8) and 500 iterations.\nOutput: Finally, the code extracts and saves two key results: the Topic-Word distribution (the top 10 most characteristic words for each of the 8 topics) and the Document-Topic distribution (the probability that each comment belongs to each topic), storing both as CSV files for subsequent qualitative analysis.\n\n\nsource(\"Topic_Model.R\")\n\n[1] \"Starting LDA Topic Modeling with K = 8\"\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======================================================================| 100%\n[1] \"LDA Training Complete.\"\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nâ„¹ Using compatibility `.name_repair`.\n\n\n[1] \"Top 10 Words for Each Topic:\"\n# A tibble: 10 Ã— 9\n   Topic_Word_Rank Topic_0 Topic_1 Topic_2 Topic_3  Topic_4  Topic_5 Topic_6\n   &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1 Word_1          pua     ä¸çŸ¥é“  å—å®³è€…  æ˜¯ä¸æ˜¯   ç‚ºä»€éº¼   ä¸çŸ¥é“  è€Œä¸æ˜¯ \n 2 Word_2          ä¸ºä»€ä¹ˆ  ä¸‡ä½™å…ƒ  ä¸ºä»€ä¹ˆ  å¥³æœ‹å‹   åœ¨ä¸€èµ·   è¶Šæ¥è¶Š  é€™ä»¶äº‹ \n 3 Word_3          ä¸­å›½äºº  pua     pua     ä¸ºä»€ä¹ˆ   ä¹Ÿå¯ä»¥   å¤§éƒ¨åˆ†  ä¸çŸ¥é“ \n 4 Word_4          ç”·å­©å­  å¯èƒ½æ˜¯  çœŸçš„æ˜¯  ä¸çŸ¥é“   ä¸å¯èƒ½   éƒ½ä¸æ˜¯  ä¸€å®šæ˜¯ \n 5 Word_5          å¥³å­©å­  ä¸€å€‹äºº  éº¦å½“åŠ³  ä¸éœ€è¦   éƒ½æ²’æœ‰   çœŸçš„æ˜¯  å¥³æœ‹å‹ \n 6 Word_6          ä¹Ÿä¸æ˜¯  é€™ä»¶äº‹  å®¶åº­çš„  ä¹Ÿæ²¡æœ‰   é€™å°±æ˜¯   å—å®³è€…  èƒ½ä¸èƒ½ \n 7 Word_7          æ˜¯ä¸æ˜¯  ä¸­å›½äºº  å…¨ä¸–ç•Œ  å¤§éƒ¨åˆ†   ä¹Ÿæ²’æœ‰   å¥³æœ‹å‹  äººæ°‘å¹£ \n 8 Word_8          æ²¡ä»€ä¹ˆ  å°¤å…¶æ˜¯  å…¶å®æ˜¯  ç”·æœ‹å‹   ç”·å°Šå¥³å‘ æˆ‘çŸ¥é“  åƒåœ¾æ¡¶ \n 9 Word_9          å¯èƒ½æ˜¯  çœŸçš„æ˜¯  å¥½åƒæ˜¯  ç”·å¥³å¹³ç­‰ æ²’ä»€éº¼   æ˜¯ä¸æ˜¯  æœ‰äº›äºº \n10 Word_10         ä¸å€¼å¾—  ç‚ºä»€éº¼  è‚¯å®šæ˜¯  æ‰€è°“çš„   ä¸å­˜åœ¨   ä¹Ÿä¸æ˜¯  ä¸ºä»€ä¹ˆ \n# â„¹ 1 more variable: Topic_7 &lt;chr&gt;\n[1] \"Saved topic words to lda_topic_words.csv\"\n[1] \"Document-Topic Distribution (Head):\"\n# A tibble: 6 Ã— 9\n  doc_id    V1    V2    V3    V4    V5    V6    V7    V8\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1     0  0      0    0      0     0       0     0\n2      2     0  0      0    0      0     0       0     0\n3      3     0  0.05   0    0.15   0.6   0.2     0     0\n4      4     0  0.1    0.2  0      0.4   0.3     0     0\n5      5     0  0      0.8  0      0.2   0       0     0\n6      6     0  0      0    0      0     0       1     0\n[1] \"Saved document-topic distribution to lda_doc_topic_distr.csv\"\n\n\nTranslation for the every words in the topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic_Word_Rank\nTopic_0\nTopic_1\nTopic_2\nTopic_3\nTopic_4\nTopic_5\nTopic_6\nTopic_7\n\n\nWord_1\nPUA\nDonâ€™t know\nVictim\nIs it?\nWhy\nDonâ€™t know\nAnd not\nBuffet (Resource)\n\n\nWord_2\nWhy\nOver 10k yuan\nWhy\nGirlfriend\nBe together\nMore and more\nThis matter\nHighway\n\n\nWord_3\nChinese people\nPUA\nPUA\nWhy\nAlso can\nMajority\nDonâ€™t know\nToo good\n\n\nWord_4\nBoy / Male\nPossibly\nTruly is\nDonâ€™t know\nImpossible\nAre not all\nMust be\nWhy\n\n\nWord_5\nGirl / Female\nA person\nMcDonaldâ€™s\nDonâ€™t need\nDonâ€™t have at all\nTruly is\nGirlfriend\nMainly is\n\n\nWord_6\nAlso is not\nThis matter\nFamilyâ€™s\nAlso donâ€™t have\nThis is\nVictim\nCan or cannot\nSome people\n\n\nWord_7\nIs it?\nChinese people\nWhole world\nMajority\nAlso donâ€™t have\nGirlfriend\nRMB (Money)\nInequality\n\n\nWord_8\nNothing much\nEspecially\nActually is\nBoyfriend\nMale Superiority\nI know\nTrash Can (Worthless)\nShould be\n\n\nWord_9\nPossibly\nTruly is\nSeems like\nGender Equality\nNothing much\nIs it?\nSome people\nA person\n\n\nWord_10\nNot worth it\nWhy\nDefinitely is\nSo-called\nDoes not exist\nAlso is not\nWhy\nGender Equality\n\n\n\nThe interpretation for each topic\n\n\n\n\n\n\n\n\nTopic\nCore Keywords & Interpretation\nSuggested Topic Label\n\n\nTopic 0\nThis topic strongly links the PUA phenomenon with discussions about specific gender roles and identities within the Chinese context. The presence of â€œNot worth itâ€ suggests this cluster is focused on evaluating the value of actions/relationships under the PUA framework.\nPUA & Gender Dynamics in China\n\n\nTopic 1\nThis topic mixes uncertainty and specific financial figures (Over 10k yuan), directly linked to PUA. It suggests discussions about high-stakes financial loss or investment by an individual in a relationship where the outcome or reality is unclear.\nFinancial Dimension of PUA & Uncertainty\n\n\nTopic 2\nThe simultaneous presence of â€œVictim,â€ â€œPUA,â€ and â€œTruly isâ€ indicates a core discussion cluster dedicated to validating the existence and reality of being exploited. â€œMcDonaldâ€™sâ€ implies cheap/casual provision, while â€œFamilyâ€™sâ€ suggests the conversation may touch on the origins or impact of these dynamics within a family unit.\nValidating Victimhood & Low-Cost Exploitation\n\n\nTopic 3\nThis topic is saturated with questioning terms (â€œIs it?â€, â€œWhy?â€, â€œDonâ€™t knowâ€), applied directly to boyfriend/girlfriend roles and the concept of gender equality. It represents a pervasive atmosphere of skepticism and critical discussion about expected behavior in modern relationships.\nSkepticism & Questioning of Relationship Roles\n\n\nTopic 4\nA highly polarized topic that denies (ä¸å¯èƒ½, ä¸å­˜åœ¨) the relevance or existence of Male Superiority (ç”·å°Šå¥³å‘). It focuses on the possibility of being together (åœ¨ä¸€èµ·), suggesting a desire for modern, equal partnerships and a strong rejection of patriarchal norms.\nDenial of Traditional Patriarchy in Relationships\n\n\nTopic 5\nTerms like â€œMore and moreâ€ and â€œMajorityâ€ point to a discussion of social trends and scale. When combined with â€œVictimâ€ and â€œGirlfriend,â€ it indicates a conversation about whether victimhood is becoming increasingly common or if the perception of victimhood is changing within the female partner role.\nDiscussing Shifting Social Norms & Victim Pool\n\n\nTopic 6\nThis is the most explicitly transactional topic. It discusses financial payment (RMB) and the concept of a person being reduced to a â€œTrash Canâ€ (worthless/emotional dumping ground). The use of â€œAnd notâ€ suggests a debate over what a relationship should be versus what it currently is (i.e., not a transaction, but one of money/exploitation).\nMonetary Value vs.Â Emotional Worth (The Price of Simping)\n\n\nTopic 7\nThis topic links resource provision (implied by â€œBuffetâ€ and â€œHighway,â€ often used as metaphors for free/easy access) with discussions of Inequality and Gender Equality. It debates whether resources should be provided freely, who is responsible for providing them, and the resulting fairness in the relationship structure.\nResource Provision & Equality Debate\n\n\n\n\n\n\nDTM\nTopic_Word_Rank\n\n\nV1\nTopic_0\n\n\nV2\nTopic_1\n\n\nV3\nTopic_2\n\n\nV4\nTopic_3\n\n\nV5\nTopic_4\n\n\nV6\nTopic_5\n\n\nV7\nTopic_6\n\n\nV8\nTopic_7"
  },
  {
    "objectID": "Final_check-in_2.html#causal-inference",
    "href": "Final_check-in_2.html#causal-inference",
    "title": "DACSS785_Final_Project",
    "section": "Causal Inference",
    "text": "Causal Inference\nIn the casual inference part, I present the Ordinary Least Squares (OLS) regression model to analyze how the probability of eight LDA topics influences the number of likes received by a comment (likeCount). Topic V8 (PUA/Victim) was set as the reference group (Reference Topic) in the model. Overall, the modelâ€™s explanatory power is extremely low (\\(\\text{Adjusted R-squared} = 0.00025\\)), suggesting that the variation in likeCount is primarily influenced by factors outside the model, rather than the topics themselves. However, the coefficient tests for individual topics revealed that Topic V2 demonstrated a statistically significant positive influence. After controlling for the effects of other topics, an increase of 1 unit in the probability of Topic V2 (Financial/Money II), relative to the reference group V8, is expected to increase the number of likes by approximately 9.66 (\\(p = 0.038^{*}\\)). This suggests that specific discussion content related to money or finance is more likely to garner attention and agreement within the community. Apart from the intercept, the remaining topics (V1, V3, V4, V5, V6, and V7) did not show a statistically significant relationship with the number of likes.\n\nsource(\"Causal_Inference.R\")\n\nRows: 3006 Columns: 9\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (9): doc_id, V1, V2, V3, V4, V5, V6, V7, V8\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n[1] \"--- OLS Regression Results (Outcome: likeCount) ---\"\n[1] \"Reference Topic: V8 (PUA/Victim)\"\n\nCall:\nlm(formula = likeCount ~ V1 + V2 + V3 + V4 + V5 + V6 + V7, data = merged_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -18.59   -8.93   -6.76   -5.12 1763.41 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.934      2.301   3.882 0.000106 ***\nV1            -2.424      4.144  -0.585 0.558600    \nV2             9.655      4.656   2.074 0.038172 *  \nV3            -2.701      4.312  -0.626 0.531107    \nV4            -2.380      4.266  -0.558 0.576978    \nV5            -2.815      4.399  -0.640 0.522298    \nV6             1.729      4.351   0.398 0.691027    \nV7            -0.953      4.363  -0.218 0.827099    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.08 on 2998 degrees of freedom\nMultiple R-squared:  0.002579,  Adjusted R-squared:  0.00025 \nF-statistic: 1.107 on 7 and 2998 DF,  p-value: 0.3553"
  },
  {
    "objectID": "Final_check-in_2.html#conclusion",
    "href": "Final_check-in_2.html#conclusion",
    "title": "DACSS785_Final_Project",
    "section": "Conclusion",
    "text": "Conclusion\n(PUT THESE TWO POINT INTO THE FUTURE WORK ALSO NEED TO EXPLAIN ABOUT DIFFICULTY TO COLLECT THE ACADEMIC DATA THERE ** SIMP DOES NOT HAVE A FORMAL DEFINITION**\nâ€œSynthesizing the projectâ€™s findings, the primary discovery is that community engagement (\\(\\text{likeCount}\\)) on YouTube comments is not driven by broad emotional tone or general topics, but rather by a specific, critical narrative focused on â€˜financial exploitation and victimhoodâ€™ (\\(\\text{Topic}\\) \\(\\text{V2}\\)). This specific form of critical discussion related to â€˜SIMPâ€™ behavior is extremely negative in sentiment (\\(\\text{mean}\\) \\(\\text{sentiment} = -0.477\\)) and is so unique in its linguistic pattern that its occurrence can be accurately predicted by the supervised learning model. Consequently, the communityâ€™s response to â€˜SIMPâ€™ behavior is highly concentrated and emotionally charged, with its online visibility predominantly stemming from comments that link the behavior directly to concrete financial inequality and victim scenarios.â€"
  },
  {
    "objectID": "Final_check-in_2.html#future",
    "href": "Final_check-in_2.html#future",
    "title": "DACSS785_Final_Project",
    "section": "Future",
    "text": "Future\nFuture work should prioritize addressing the observed limitations in both the supervised classification model and the initial data preprocessing pipeline to enhance the robustness and explanatory power of the analysis. Firstly, while the initial Naive Bayes classifier provided baseline insights, its predictive performance should be critically re-evaluated. Improving the accuracy of the automated simp classification label requires exploring more sophisticated machine learning techniques, such as Support Vector Machines (SVMs), Gradient Boosting, or even Transformer-based deep learning models. Concurrently, enhancing the feature set by refining or expanding the custom dictionariesâ€”perhaps incorporating sentiment scores or incorporating word embeddingsâ€”could significantly boost the modelâ€™s ability to discriminate between classes, moving beyond simple bag-of-words approaches. Secondly, a crucial area for improvement lies in the token filtering and data processing stage. Despite standard removal procedures, the presence of numerous contextually irrelevant tokens, such as specific objects (â€œé«˜é€Ÿå…¬è·¯â€) and brands (â€œéº¥ç•¶å‹â€), confirms the necessity of a more rigorous, domain-specific cleanup. Future efforts must focus on constructing an expanded, domain-aware stop word list or implementing Named Entity Recognition (NER) to systematically identify and remove non-topical, low-information tokens, ensuring the remaining features are highly predictive and representative of the core concepts being discussed."
  },
  {
    "objectID": "Final_check-in_2.html#reference",
    "href": "Final_check-in_2.html#reference",
    "title": "DACSS785_Final_Project",
    "section": "Reference",
    "text": "Reference\nHO, Daniel. The (simp)le truth about excessive & obsessive romantic behaviors in men. (2023). https://ink.library.smu.edu.sg/etd_coll/516\nKrishnamurthy, V., & Duan, Y. (2017). Dependence Structure Analysis Of Meta-level Metrics in YouTube Videos: A Vine Copula Approach. arXiv preprint arXiv:1712.10232. â€œto explain the comment and the view of the video are relatedâ€\nLun-Wei Ku and Hsin-Hsi Chen (2007). Mining Opinions from the Web: Beyond Relevance Retrieval. Journal of American Society for Information Science and Technology, Special Issue on Mining Web Resources for Enhancing Information Retrieval, 58(12), pages 1838-1850.\nPew Research Center. (2020). Many Americans get news on YouTube, where news organizations and independent producers thrive side by side. https://www.pewresearch.org/journalism/2020/09/28/many-americans-get-news-on-youtube-where-news-organizations-and-independent-producers-thrive-side-by-side/\nZhou, W. (2024). é‡åº†è­¦æ–¹å‘å¸ƒâ€œèƒ–çŒ«â€äº‹ä»¶è­¦æƒ…é€šæŠ¥ [Chongqing police issue incident report on the â€œPangmaoâ€ incident]. Xinhua Net. http://www.news.cn/politics/20240519/fb56352660c94810a58e79bc18459a3e/c.html"
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "About Me",
    "section": "",
    "text": "My primary professional and academic goals are currently focused on two key areas:\n\nAcademic Completion: Successfully completing my Masterâ€™s program in the Data Analytics and Computational Social Science (DACSS) program at the University of Massachusetts Amherst (UMass Amherst).\nCareer Transition: Securing a professional position focused on Data Analysis or Data Science, where I can apply the quantitative, computational, and analytical skills gained during my studies."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "",
    "text": "Master of Science (M.S.), Data Analytics and Computational Social Science (DACSS) * Expected Graduation: [Insert Expected Graduation Date, e.g., May 2026] * Key Areas of Study: [Briefly list 2-3 key skills or domains, e.g., Statistical Modeling, Machine Learning, Data Visualization, Causal Inference]\n\nView My Full Curriculum Vitae (CV)"
  },
  {
    "objectID": "projects.html#final-project",
    "href": "projects.html#final-project",
    "title": "Projects",
    "section": "",
    "text": "View Full Analysis & Report (HTML)"
  },
  {
    "objectID": "projects.html#project-5",
    "href": "projects.html#project-5",
    "title": "Projects",
    "section": "Project 5",
    "text": "Project 5"
  },
  {
    "objectID": "758/Final_check-in_2.html",
    "href": "758/Final_check-in_2.html",
    "title": "DACSS785_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "758/Final_check-in_2.html#research-question-and-hypothesis",
    "href": "758/Final_check-in_2.html#research-question-and-hypothesis",
    "title": "DACSS785_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "758/Final_check-in_2.html#data-collection",
    "href": "758/Final_check-in_2.html#data-collection",
    "title": "DACSS785_Final_Project",
    "section": "Data Collection",
    "text": "Data Collection\nTo explore the concept of â€œsimping,â€ I collected YouTube comments from six relevant videos for textual analysis. I utilized an R scraping script to extract approximately 8,000 comments in total. Following a cleaning and filtering process, a dataset of around 7,000 practical comments was retained for analysis.\nI specifically focused on the case study known as the â€œèƒ–è²“è·³æ±Ÿäº‹ä»¶â€ (Suicide of Fat Cat). This event, which occurred in Mainland China, provides a particularly rich and relevant dataset because it was a well-documented news story officially reported by the Chinese court. This official documentation makes it a real and verifiable event, distinguishing it from mere rumors or social media anecdotes. Furthermore, the use of Chinese-language videos as the reference source is critical, as the Chinese-speaking audience possesses extensive background knowledge and cultural context directly related to the local details of this incident.\n\nSIMP001 - é™ªæ‰“éŠæˆ²è³ºç™¾è¬é¤Šå¥³å‹æ…˜é­åˆ†æ‰‹ï¼ã€Œèƒ–è²“äº‹ä»¶ã€å¼•çˆ†ä¸­åœ‹æ€§åˆ¥æˆ°çˆ­ï¼Ÿã€Œæ’ˆå¥³ã€æ»¿è¡—è·‘çš„èƒŒå¾ŒåŸå› ï¼Ÿã€TODAY çœ‹ä¸–ç•Œã€‘(https://www.youtube.com/watch?v=o5TfkwlthWU&t=13s) 1952 comments from 11/04/2025\nSIMP002 - å½“èƒ–çŒ«é‡åˆ°æå¥³ï¼Œä¸€ä¸ªå¹´è½»äººå¦‚ä½•èµ°ä¸Šä¸å½’è·¯ï¼Ÿï½œå¥³æƒï½œæå¥³ï½œèƒ–çŒ«ï½œç‹è€…è£è€€ï½œç”·å¥³å¹³æƒï½œæ—¥æœ¬ï½œæ¢…å¤§é«˜é€Ÿï½œèˆ†è®ºæ§åˆ¶ï½œç‹å±€æ‹æ¡ˆ20240507 (https://www.youtube.com/watch?v=39Gq_eOPuDY&t=1s) 3731 comments from 11/04/2025\nSIMP003 - è€æ¢ï¼šç»™â€œèƒ–çŒ«â€å¤šæ¡é€‰æ‹© é‡åº†â€œèƒ–çŒ«äº‹ä»¶â€ä¸æ˜¯æ€§åˆ«å¤§æˆ˜ å¦‚ä½•é¿å…æˆä¸ºâ€œèƒ–çŒ«â€(https://www.youtube.com/watch?v=mjcgg0wFpfE) 997 comments from 11/04/2025\nSIMP004 - å°ä¼™ç‚ºæ„›è·³æ±Ÿï¼Œæ‹œé‡‘çš„å¥³å‹ï¼Œå¸è¡€çš„è¦ªå§ï¼Œç„¡è‰¯çš„å•†å®¶ï¼Œç˜‹ç‹‚çš„ç¶²æ°‘ï¼Œèª°æ‰æ˜¯åŠ å®³è€…ï¼Ÿç‚ºä½•è­¦å¯Ÿèªå®šå¥³å‹ç„¡ç½ªï¼Œåè€Œæ˜¯è¦ªå§é•äº†æ³•ï¼Ÿä¸€å£æ°£çœ‹å®Œèƒ–è²“äº‹ä»¶å§‹æœ«ï¼| Wayneèª¿æŸ¥(https://www.youtube.com/watch?v=igs7GoIU4MU) 615 comments from 11/04/2025\nSIMP005 - ç¥ç´šé™ªç©ã€Œèƒ–è²“ã€é­è©ä¹¾227è¬äº¡ å¥³å‹é“æ­‰ï½œ20240506 ETåˆé–“æ–°è (https://www.youtube.com/watch?v=tAE83zZEcOY) 402 comments from 11/04/2025\nSIMP006 - è¢«æ’ˆå¥³é¨™å…‰50è¬ï¼ŒéŠæˆ²å®…ç”·è·³æ±Ÿè‡ªæ®ºï¼Œè½Ÿå‹•å…¨ç¶²ï¼æ’ˆå¥³è­šç«¹æ¦¨ä¹¾èƒ–è²“äº‹ä»¶çœŸç›¸ï¼ã€æ–°é—»æœ€å˜²ç‚¹ å§œå…‰å®‡ã€2024.0508(https://www.youtube.com/watch?v=YYngd2Yt3zk) 271 comments from 11/04/2025\n\n\nlibrary(plyr)\n\nWarning: package 'plyr' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.4.3\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(purrr)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:plyr':\n\n    compact\n\nlibrary(chromote)\n\nWarning: package 'chromote' was built under R version 4.4.3\n\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tibble)\nlibrary(nnet)\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.4.3\n\n\ncorrplot 0.95 loaded\n\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.4.3\n\n\nLoading required package: NLP\n\n\nWarning: package 'NLP' was built under R version 4.4.2\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.4.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.4.3\n\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.4.3\n\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.4.3\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(\"quanteda.textplots\")\n\nWarning: package 'quanteda.textplots' was built under R version 4.4.3\n\nlibrary(httr)\n\nWarning: package 'httr' was built under R version 4.4.3\n\n\n\nAttaching package: 'httr'\n\n\nThe following object is masked from 'package:NLP':\n\n    content\n\nlibrary(RColorBrewer)\nlibrary(RedditExtractoR)\n\nWarning: package 'RedditExtractoR' was built under R version 4.4.3\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.4.3\n\nlibrary(tidyr)\n\n\ndata1 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\ndata2 &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments.csv\")\ndata3 &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments.csv\")\ndata4 &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments.csv\")\ndata5 &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments.csv\")\ndata6 &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments.csv\")"
  },
  {
    "objectID": "758/Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "href": "758/Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "title": "DACSS785_Final_Project",
    "section": "Data Cleaning (info for the poster",
    "text": "Data Cleaning (info for the poster\nALSO GUIDE IT TO THE PART WHICH I WANT PLUS PRESENT THE CLEAN FORMAT IN THE POSTER\nThe raw dataset, collected as several CSV files, initially contained detailed comment metadata. The original structure included columns such as: videoId, commentId, parentId, author, text, likeCount, publishedAt, updatedAt, viewerRating, canRate, and reply.\nFor data cleaning, all CSV files in the Final_project_data folder were systematically processed using the R environment. The initial step was to streamline the dataset by retaining only the essential variables for textual and engagement analysis: text, likeCount, and reply.\nI utilized the R packages dplyr and stringr to focus on standardizing the text column. This involved a series of cleaning operations: normalization of whitespace (removing line breaks, tabs, and extra spaces, and trimming leading/trailing whitespace) and character filtering. Crucially, I removed non-essential symbols and unusual characters while meticulously preserving all Chinese characters to ensure the comments remained culturally authentic and meaningful for subsequent analysis.\nFinally, each cleaned and standardized dataset was saved as a new CSV file, appended with the suffix _cleaned. UTF-8 encoding was explicitly used to guarantee the accurate representation of the Chinese characters. This systematic workflow ensures the comment data are tidy, standardized, and immediately ready for downstream procedures, such as tokenization and sentiment or frequency analysis.\n\nsource(\"data_cleaning_CN.R\")\n\ndata cleaning complete!.\n\nSIMP001 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\n\n#Present the eample of the result\nhead(SIMP001)\n\n      videoId                  commentId parentId        author\n1 o5TfkwlthWU UgyekRC230MDXREkdeN4AaABAg     &lt;NA&gt;  @DanjonMeshi\n2 o5TfkwlthWU UgxangSP0zjJm6_gHfV4AaABAg     &lt;NA&gt;  @paullee4451\n3 o5TfkwlthWU UgwZdLtl6Eb2wgDWaDV4AaABAg     &lt;NA&gt;      @urikora\n4 o5TfkwlthWU UgwSmeqUyHYUXGD6l3l4AaABAg     &lt;NA&gt; @fayechen1928\n5 o5TfkwlthWU UgwpotCAmJmn2wWU7u54AaABAg     &lt;NA&gt; @running_goat\n6 o5TfkwlthWU UgxQSyQbnLT9r7I1faB4AaABAg     &lt;NA&gt;  @Jack2006103\n                                                                                                                       text\n1                                                             å•†å®¶é›†é«”çµ¦ç©ºè¢‹çœŸçš„ç¬‘æ­»ï¼Œä¸æ­¢ç”Ÿæ´»åœ¨ä¸­åœ‹ï¼Œé€£æ­»åœ¨ä¸­åœ‹éƒ½è¦å·è‘—æ¨‚ğŸ˜†\n2                                                                                                                      é ­é¦™\n3                       æ›´æ…˜çš„æ˜¯ï¼Œäººéƒ½èµ°äº†ä¸€å€‹æœˆ çµæœå°±åœ¨é€™æ™‚æ©Ÿé»è¢«æŠ“ä¾†æ“‹æ”¿åºœåšçš„é†œäº‹ï¼ˆè·¯å´©è¯ç‚ºè»Šè¡é€²å»å‘æ´ç„¶å¾Œå¿«é€Ÿç‡ƒèµ·ä¾†ï¼‰\n4                                                                                      æ¯æ¬¡çœ‹åˆ°ä¸­åœ‹é€™ç¨®æ‚²åŠ‡éƒ½è¦ºå¾—ä¸å¯æ€è­°ğŸ˜¨ğŸ˜­\n5 æˆ‘çœŸçš„å¿…é ˆå¾—èªªå°å²¸ç”·å¥³æˆ°çˆ­çœŸçš„è¶Šä¾†è¶Šåš´é‡== å•éå¥½å¹¾å€‹å°å²¸çš„å¥³ç”Ÿéƒ½èªç‚ºç”·ç”Ÿå°±æ˜¯æ‡‰è©²è¦çµ¦å½©ç¦® åƒé£¯å°±æ˜¯è¦å¹«å¥³ç”Ÿä»˜éŒ¢ç­‰ç­‰ å¾ˆå¯æ€•\n6                                                                                                é€™äº›åº—å®¶åƒäººè¡€é¥…é ­ï¼Œè¶…å™çˆ›\n  likeCount          publishedAt            updatedAt viewerRating canRate\n1        58 2024-05-09T16:01:52Z 2024-05-09T16:01:52Z         none    TRUE\n2         0 2024-05-09T16:02:23Z 2024-05-09T16:02:23Z         none    TRUE\n3       345 2024-05-09T16:05:05Z 2024-05-09T16:05:05Z         none    TRUE\n4         1 2024-05-09T16:05:57Z 2024-05-09T16:05:57Z         none    TRUE\n5       139 2024-05-09T16:06:47Z 2024-05-09T16:06:47Z         none    TRUE\n6         4 2024-05-09T16:07:29Z 2024-05-09T16:07:29Z         none    TRUE\n  reply\n1 FALSE\n2 FALSE\n3 FALSE\n4 FALSE\n5 FALSE\n6 FALSE\n\ndata1_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_cleaned.csv\")\ndata2_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments_cleaned.csv\")\ndata3_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments_cleaned.csv\")\ndata4_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments_cleaned.csv\")\ndata5_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments_cleaned.csv\")\ndata6_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments_cleaned.csv\")"
  },
  {
    "objectID": "758/Final_check-in_2.html#preprocess-the-data",
    "href": "758/Final_check-in_2.html#preprocess-the-data",
    "title": "DACSS785_Final_Project",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nFor visualizing the dominant linguistic patterns within the comment data, I employed two complementary approaches. First, a Word Cloud visualization (generated using the Word_cloud_visualization.R script) provided an intuitive, qualitative representation of high-frequency words, instantly highlighting the most common terms associated with discussions of â€œSIMPâ€ behavior.\nSecond, I conducted a quantitative rank-frequency analysis by applying Zipfâ€™s Law to the word corpus. After arranging all unique words by descending frequency and assigning a rank, I plotted the resulting distribution using the ggplot2 package. The resulting visualization confirmed that the comment discourse adheres to a Zipfian distribution, where a few words account for a disproportionate share of the total vocabulary.\nThe key terms driving the discourse were clearly identifiable:\nThese visualizations collectively offer both quantitative validation (Zipfâ€™s Law distribution) and qualitative insight (Word Cloud/Top Terms) into how the audience discusses and perceives the central event and the related concept of â€œSIMPâ€ behavior in this context. The high frequency of questioning and uncertainty (ç‚ºä»€éº¼, ä¸çŸ¥é“, æ˜¯ä¸æ˜¯) coupled with terms of exploitation (pua) and suffering (å—å®³è€…) reveals a key focus on moral judgment and accountability in the discussion.\n\nsource(\"TOKENIZATION.R\")\n\nTokenization complete!\n\nSIMP001_comments_tokens &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_tokens.csv\")\n#Present the eample of the result\nhead(SIMP001_comments_tokens)\n\n  likeCount reply     word\n1       345 FALSE   è¡é€²å»\n2         1 FALSE ä¸å¯æ€è­°\n3       139 FALSE   è¶Šä¾†è¶Š\n4       139 FALSE   å¥½å¹¾å€‹\n5       141 FALSE   éº¥ç•¶å‹\n6         3 FALSE   æœ‰äººèªª\n\n\n\nsource(\"Word_frequency.R\")\n\nWord Frequency Calculation Complete!\n\nSIMP001_wordfreq &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_wordfreq.csv\")\n\n#Present the eample of the result\nhead(SIMP001_wordfreq)\n\n    word  n rank\n1 å—å®³è€… 75    1\n2 ç‚ºä»€éº¼ 67    2\n3 ä¸çŸ¥é“ 53    3\n4    pua 42    4\n5 é€™ä»¶äº‹ 34    5\n6 ä¸€å€‹äºº 32    6\n\n\n\nsource(\"Same_Word.R\")\n\nCommon words saved to: Final_project_data/common_words_across_files.csv \n\ncommon_words &lt;- read.csv(\"Final_project_data/common_words_across_files.csv\")\n\n#Present the eample of the result\nhead(common_words)\n\n    word total_count\n1 ä¸çŸ¥é“         172\n2 ä¸ºä»€ä¹ˆ         154\n3 å—å®³è€…         118\n4    pua         115\n5 æ˜¯ä¸æ˜¯         101\n6 ç‚ºä»€éº¼         100\n\n\n\nsource(\"Change_to_Traditional_Chinese.R\")\n\nWarning: package 'textstem' was built under R version 4.4.3\n\n\nLoading required package: koRpus.lang.en\n\n\nWarning: package 'koRpus.lang.en' was built under R version 4.4.3\n\n\nLoading required package: koRpus\n\n\nWarning: package 'koRpus' was built under R version 4.4.3\n\n\nLoading required package: sylly\n\n\nWarning: package 'sylly' was built under R version 4.4.3\n\n\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\n\nAttaching package: 'koRpus'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    tokens, types\n\n\nThe following object is masked from 'package:tm':\n\n    readTagged\n\n\nThe following object is masked from 'package:readr':\n\n    tokenize\n\n\nWarning: package 'tmcn' was built under R version 4.4.3\n\n\n# tmcn Version: 0.2-13\n\n\n[1] \"girl\"    \"woman\"   \"simping\" \"lover\"  \nTranslation complete! Output saved to 'your_output_file.csv'\n\nTraditional_Chinese_data_cleaned &lt;- read.csv(\"Final_project_data/traditional_common_words_combined.csv\")\n\n#Present the data after cleaning\nhead(Traditional_Chinese_data_cleaned)\n\n  traditional_text total_count\n1           ç‚ºä»€éº¼         254\n2           ä¸çŸ¥é“         172\n3           å—å®³è€…         118\n4              pua         116\n5           æ˜¯ä¸æ˜¯         101\n6           å¥³æœ‹å‹          95"
  },
  {
    "objectID": "758/Final_check-in_2.html#visualization",
    "href": "758/Final_check-in_2.html#visualization",
    "title": "DACSS785_Final_Project",
    "section": "Visualization",
    "text": "Visualization\nFor visualizing patterns in the comments, I used two approaches. First, the Word_cloud_visualization.R script generated word clouds to highlight high-frequency words, providing a clear and intuitive view of the most common terms associated with discussions of â€œSIMPâ€ behavior. Second, I applied Zipfâ€™s Law to examine the relationship between word rank and frequency. After arranging words by descending frequency and assigning ranks, I plotted all words using ggplot2, labeling only the top five most frequent words to emphasize the key terms in the discourse. The resulting visualizations offer both quantitative and qualitative insight into how people discuss and perceive â€œSIMPâ€ behavior in YouTube comments.\n\nsource(\"Word_cloud_visualization.R\")\n\n\n\n\n\n\n\n\nWord Cloud generated for: traditional_common_words_combined.csv\n\n\n\n# Sort by frequency and assign ranks\nzipf_data_ranked &lt;- Traditional_Chinese_data_cleaned %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  mutate(rank = row_number())\n\n# Print the top 5 ranked words to confirm the data structure\nprint(head(zipf_data_ranked, 5))\n\n  traditional_text total_count rank\n1           ç‚ºä»€éº¼         254    1\n2           ä¸çŸ¥é“         172    2\n3           å—å®³è€…         118    3\n4              pua         116    4\n5           æ˜¯ä¸æ˜¯         101    5\n\n# --- Linear Scale (As Requested) ---\n\nggplot(zipf_data_ranked, aes(x = rank, y = total_count)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"darkorange\", size = 1.5) +\n  geom_text(\n    # Label the top 8 words\n    aes(label = ifelse(rank &lt;= 6, traditional_text, \"\")),\n    vjust = -0.8,\n    size = 3.5,\n    check_overlap = TRUE # Prevents overlapping labels\n  ) +\n  labs(\n    title = \"Zipfâ€™s Law: Word Rank vs Frequency\",\n    x = \"Rank of Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nTranslation\n\n\n\nRank\nWord\nTranslate\n\n\n1\nç‚ºä»€éº¼\nâ€œWhy / Why is it thatâ€¦â€\n\n\n2\nä¸çŸ¥é“\nâ€œDonâ€™t knowâ€\n\n\n3\nå—å®³è€…\nâ€œVictimâ€\n\n\n4\npua\nâ€œPUAâ€\n\n\n5\næ˜¯ä¸æ˜¯\nâ€œIs it / Is it not?â€\n\n\n6\nå¥³æœ‹å‹\nâ€œGirlfriendâ€"
  },
  {
    "objectID": "758/Final_check-in_2.html#word-embedding",
    "href": "758/Final_check-in_2.html#word-embedding",
    "title": "DACSS785_Final_Project",
    "section": "Word Embedding",
    "text": "Word Embedding\nFor semantic analysis, I applied Word2Vec using a pseudo-document approach to capture relationships between words in the comments. Each word was repeated according to its frequency (total_count) to create co-occurrence information, which is essential for small datasets where natural co-occurrences are limited. The repeated words were then combined into a single space-separated pseudo-document and used to train a skip-gram Word2Vec model with a vector dimension of 50, window size of 5, and 50 iterations, setting min_count = 1 to include all words.\nThe resulting word vectors allow calculation of cosine similarity to examine semantic relationships between words, as well as clustering and downstream supervised learning tasks. For example, the vector for a keyword such as â€œç‚ºä»€éº¼â€ can be compared with all other word vectors to identify the top semantically similar words, revealing patterns in how concepts related to â€œSIMPâ€ behavior are discussed in YouTube comments. This approach provides a robust representation of word meaning in the context of the dataset while accommodating the limited co-occurrence information inherent in smaller comment datasets.\n\n# Word2Vec can be the best option for the word embeding.\n\nsource(\"Word Embeddings.R\")\n\nWarning: package 'word2vec' was built under R version 4.4.3\n\n\nWarning: package 'text2vec' was built under R version 4.4.3"
  },
  {
    "objectID": "758/Final_check-in_2.html#sentiment-analysis",
    "href": "758/Final_check-in_2.html#sentiment-analysis",
    "title": "DACSS785_Final_Project",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nFor sentiment analysis, I applied a custom Chinese sentiment dictionary tailored to the context of â€œSIMPâ€ behavior. The dictionary categorizes words into three groups: positive (supportive or relationship-related words such as â€œå¥³æœ‹å‹â€ and â€œé—œå¿ƒâ€), negative (critical or unfairness-related words such as â€œä¸å€¼å¾—â€ and â€œä¸å…¬å¹³â€), and behavior (attention-seeking or â€œsimpâ€ behavior words such as â€œpuaâ€ and â€œè¿½æ±‚â€). Using R, I computed sentiment scores for each word in the dataset by summing occurrences in these categories. A raw polarity score was calculated as the sum of positive and behavior counts minus negative counts, then normalized by the total occurrences of all dictionary words to produce a relative polarity measure.\nThe analysis revealed that the current positive and negative categories do not fully capture the sentiment expressed in the comments. Some words were misclassified or contextually ambiguous, highlighting that the dictionary needs further adjustment and refinement to improve accuracy. Polarity distributions were visualized using a histogram, providing an overview of how positive, negative, and behavior-related language appears in discussions of â€œSIMPâ€ behavior. This approach provides a preliminary sentiment assessment while acknowledging the limitations of the existing dictionary.\n\nsource(\"Sentiment Analysis.R\")\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” forcats   1.0.0     âœ” lubridate 1.9.4\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– NLP::annotate()         masks ggplot2::annotate()\nâœ– httr::content()         masks NLP::content()\nâœ– dplyr::filter()         masks stats::filter()\nâœ– jsonlite::flatten()     masks purrr::flatten()\nâœ– rvest::guess_encoding() masks readr::guess_encoding()\nâœ– dplyr::lag()            masks stats::lag()\nâœ– koRpus::tokenize()      masks readr::tokenize()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'quanteda.textmodels' was built under R version 4.4.3\n\n\nWarning: package 'stopwords' was built under R version 4.4.3\n\n\n\nAttaching package: 'stopwords'\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nWarning: package 'caret' was built under R version 4.4.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:httr':\n\n    progress\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n[1] \"DFM after Custom Simp Dictionary Lookup (Counts per category):\"\n\n### SUPERVISED LEARNING (Naive Bayes Classification)\n[1] \"Confusion Matrix:\"\n\n### LEXICON-BASED NTUSD SENTIMENT RESULTS\n[1] \"Mean Sentiment Score by Simping Label (NTUSD):\"\n# A tibble: 2 Ã— 2\n  contains_simp_factor mean_sentiment\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 FALSE                       -0.0990\n2 TRUE                        -0.477 \n\n\n\n\n\n\n\n\n\nIn this graph:\nX-Axis: Simping Label:\n\nFALSE: Comments that do not contain any of the words from your custom â€œsimpâ€ dictionary (e.g., â€œèˆ”ç‹—,â€ â€œå·¥å…·äºº,â€ â€œä¸€å»‚æƒ…é¡˜,â€ etc.).\nTRUE: Comments that do contain at least one word from your custom â€œsimpâ€ dictionary.\n\nY-Axis: Comment Sentiment Score:\n\nPositive Scores (above 0): Indicate a more positive overall sentiment.\nZero (0): Indicates a neutral or balanced sentiment.\nNegative Scores (below 0): Indicate a more negative overall sentiment."
  },
  {
    "objectID": "758/Final_check-in_2.html#sentiment-analysis-summary",
    "href": "758/Final_check-in_2.html#sentiment-analysis-summary",
    "title": "DACSS785_Final_Project",
    "section": "Sentiment Analysis Summary",
    "text": "Sentiment Analysis Summary\nThe lexicon-based sentiment analysis, utilizing the NTUSD dictionary, reveals a pronounced negative emotional shift in texts discussing the â€œsimpâ€ phenomenon. Specifically, content that contains terms from the custom dictionaryâ€”which targets themes like â€œsimp behaviorâ€ (e.g., èˆ”ç‹—, simp), â€œvictim positionâ€ (e.g., å—å®³è€…, pua), and â€œrelationship imbalanceâ€â€”shows a highly negative mean sentiment score of -0.477. This score is significantly more negative than the average score of -0.0990 found in texts that do not contain these specific terms. This sharp difference (a nearly five-fold increase in negative sentiment magnitude) indicates that conversations about excessive one-sided effort, perceived exploitation, and unequal relationshipsâ€”the core of the â€œsimpâ€ conceptâ€”are strongly associated with negative emotional discourse within the corpus."
  },
  {
    "objectID": "758/Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "href": "758/Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "title": "DACSS785_Final_Project",
    "section": "Supervised Learning Analysis (Naive Bayes Classification)",
    "text": "Supervised Learning Analysis (Naive Bayes Classification)\nThe Naive Bayes model was employed to classify comments based on whether they contained the â€œsimpâ€ factor, using a cleaned feature set that excluded all words from the custom â€œsimpâ€ dictionary to prevent data leakage. The model achieved an overall Accuracy of 81.69%, which is slightly higher than the No Information Rate (NIR) of 80.36%, indicating its performance is marginally better than random guessing based on class prevalence.\nHowever, a closer look at the results reveals a significant class imbalance issue and skewed performance:\n\nHigh Sensitivity (Recall): The model is excellent at correctly identifying comments that do NOT contain the simp factor (the majority class, FALSE), with a high Sensitivity of 95.72%.\nLow Specificity: Conversely, the model is very poor at correctly identifying comments that DO contain the simp factor (the minority class, TRUE), with a low Specificity of 24.29%.\nKappa Value: The Kappa statistic of 0.2565 suggests only a fair level of agreement beyond chance.\n\nIn summary, the high overall accuracy is largely driven by correctly classifying the prevalent negative class (FALSE). The model struggles to reliably identify actual â€œsimpâ€ comments (TRUE), suggesting that the remaining general vocabulary in the comments lacks sufficient predictive power to consistently distinguish between the two categories without the core dictionary terms.\n\nsource(\"Supervised_Learning.R\")\n\n\n--- Solving Data Leakage: Remove the word which exist in Simp dictionary ---\n[1] \"Original (matrix_main): 2703\"\n[1] \"Remove the word in Simp dictionary (X_cleaned): 2674\"\n\n--- 6. Naive Bayes Training ---\n\n--- Naive Bayes Prediction ---\n[1] \"Confusion Matrix:\"\nConfusion Matrix and Statistics\n\n                 y_test\npredicted_cleaned FALSE TRUE\n            FALSE   693  134\n            TRUE     31   43\n                                        \n               Accuracy : 0.8169        \n                 95% CI : (0.79, 0.8416)\n    No Information Rate : 0.8036        \n    P-Value [Acc &gt; NIR] : 0.1676        \n                                        \n                  Kappa : 0.2565        \n                                        \n Mcnemar's Test P-Value : 2.011e-15     \n                                        \n            Sensitivity : 0.9572        \n            Specificity : 0.2429        \n         Pos Pred Value : 0.8380        \n         Neg Pred Value : 0.5811        \n              Precision : 0.8380        \n                 Recall : 0.9572        \n                     F1 : 0.8936        \n             Prevalence : 0.8036        \n         Detection Rate : 0.7691        \n   Detection Prevalence : 0.9179        \n      Balanced Accuracy : 0.6001        \n                                        \n       'Positive' Class : FALSE"
  },
  {
    "objectID": "758/Final_check-in_2.html#topic-modeling-lda",
    "href": "758/Final_check-in_2.html#topic-modeling-lda",
    "title": "DACSS785_Final_Project",
    "section": "Topic Modeling (LDA)",
    "text": "Topic Modeling (LDA)\nThe script follows a standard text mining workflow using the tidyverse and text2vec packages:\n\nData Preparation: It reads the individual tokenized comment files, reconstructs the full comments by assigning and aggregating tokens by a unique document ID (doc_id), and then combines the tokens back into complete text strings.\nFeature Engineering: It creates an iterator from the aggregated text and builds a vocabulary. Crucially, it prunes the vocabulary by removing words that occur less than three times (term_count_min = 3), which helps reduce noise and improves the quality of the derived topics.\nDTM Creation: The processed tokens are converted into a Document-Term Matrix (DTM), which is the input required for LDA.\nModel Training: The script initializes and trains an LDA model with a predefined number of topics (K=8) and 500 iterations.\nOutput: Finally, the code extracts and saves two key results: the Topic-Word distribution (the top 10 most characteristic words for each of the 8 topics) and the Document-Topic distribution (the probability that each comment belongs to each topic), storing both as CSV files for subsequent qualitative analysis.\n\n\nsource(\"Topic_Model.R\")\n\n[1] \"Starting LDA Topic Modeling with K = 8\"\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======================================================================| 100%\n[1] \"LDA Training Complete.\"\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nâ„¹ Using compatibility `.name_repair`.\n\n\n[1] \"Top 10 Words for Each Topic:\"\n# A tibble: 10 Ã— 9\n   Topic_Word_Rank Topic_0 Topic_1 Topic_2 Topic_3  Topic_4  Topic_5 Topic_6\n   &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1 Word_1          pua     ä¸çŸ¥é“  å—å®³è€…  æ˜¯ä¸æ˜¯   ç‚ºä»€éº¼   ä¸çŸ¥é“  è€Œä¸æ˜¯ \n 2 Word_2          ä¸ºä»€ä¹ˆ  ä¸‡ä½™å…ƒ  ä¸ºä»€ä¹ˆ  å¥³æœ‹å‹   åœ¨ä¸€èµ·   è¶Šæ¥è¶Š  é€™ä»¶äº‹ \n 3 Word_3          ä¸­å›½äºº  pua     pua     ä¸ºä»€ä¹ˆ   ä¹Ÿå¯ä»¥   å¤§éƒ¨åˆ†  ä¸çŸ¥é“ \n 4 Word_4          ç”·å­©å­  å¯èƒ½æ˜¯  çœŸçš„æ˜¯  ä¸çŸ¥é“   ä¸å¯èƒ½   éƒ½ä¸æ˜¯  ä¸€å®šæ˜¯ \n 5 Word_5          å¥³å­©å­  ä¸€å€‹äºº  éº¦å½“åŠ³  ä¸éœ€è¦   éƒ½æ²’æœ‰   çœŸçš„æ˜¯  å¥³æœ‹å‹ \n 6 Word_6          ä¹Ÿä¸æ˜¯  é€™ä»¶äº‹  å®¶åº­çš„  ä¹Ÿæ²¡æœ‰   é€™å°±æ˜¯   å—å®³è€…  èƒ½ä¸èƒ½ \n 7 Word_7          æ˜¯ä¸æ˜¯  ä¸­å›½äºº  å…¨ä¸–ç•Œ  å¤§éƒ¨åˆ†   ä¹Ÿæ²’æœ‰   å¥³æœ‹å‹  äººæ°‘å¹£ \n 8 Word_8          æ²¡ä»€ä¹ˆ  å°¤å…¶æ˜¯  å…¶å®æ˜¯  ç”·æœ‹å‹   ç”·å°Šå¥³å‘ æˆ‘çŸ¥é“  åƒåœ¾æ¡¶ \n 9 Word_9          å¯èƒ½æ˜¯  çœŸçš„æ˜¯  å¥½åƒæ˜¯  ç”·å¥³å¹³ç­‰ æ²’ä»€éº¼   æ˜¯ä¸æ˜¯  æœ‰äº›äºº \n10 Word_10         ä¸å€¼å¾—  ç‚ºä»€éº¼  è‚¯å®šæ˜¯  æ‰€è°“çš„   ä¸å­˜åœ¨   ä¹Ÿä¸æ˜¯  ä¸ºä»€ä¹ˆ \n# â„¹ 1 more variable: Topic_7 &lt;chr&gt;\n[1] \"Saved topic words to lda_topic_words.csv\"\n[1] \"Document-Topic Distribution (Head):\"\n# A tibble: 6 Ã— 9\n  doc_id    V1    V2    V3    V4    V5    V6    V7    V8\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1     0  0      0    0      0     0       0     0\n2      2     0  0      0    0      0     0       0     0\n3      3     0  0.05   0    0.15   0.6   0.2     0     0\n4      4     0  0.1    0.2  0      0.4   0.3     0     0\n5      5     0  0      0.8  0      0.2   0       0     0\n6      6     0  0      0    0      0     0       1     0\n[1] \"Saved document-topic distribution to lda_doc_topic_distr.csv\"\n\n\nTranslation for the every words in the topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic_Word_Rank\nTopic_0\nTopic_1\nTopic_2\nTopic_3\nTopic_4\nTopic_5\nTopic_6\nTopic_7\n\n\nWord_1\nPUA\nDonâ€™t know\nVictim\nIs it?\nWhy\nDonâ€™t know\nAnd not\nBuffet (Resource)\n\n\nWord_2\nWhy\nOver 10k yuan\nWhy\nGirlfriend\nBe together\nMore and more\nThis matter\nHighway\n\n\nWord_3\nChinese people\nPUA\nPUA\nWhy\nAlso can\nMajority\nDonâ€™t know\nToo good\n\n\nWord_4\nBoy / Male\nPossibly\nTruly is\nDonâ€™t know\nImpossible\nAre not all\nMust be\nWhy\n\n\nWord_5\nGirl / Female\nA person\nMcDonaldâ€™s\nDonâ€™t need\nDonâ€™t have at all\nTruly is\nGirlfriend\nMainly is\n\n\nWord_6\nAlso is not\nThis matter\nFamilyâ€™s\nAlso donâ€™t have\nThis is\nVictim\nCan or cannot\nSome people\n\n\nWord_7\nIs it?\nChinese people\nWhole world\nMajority\nAlso donâ€™t have\nGirlfriend\nRMB (Money)\nInequality\n\n\nWord_8\nNothing much\nEspecially\nActually is\nBoyfriend\nMale Superiority\nI know\nTrash Can (Worthless)\nShould be\n\n\nWord_9\nPossibly\nTruly is\nSeems like\nGender Equality\nNothing much\nIs it?\nSome people\nA person\n\n\nWord_10\nNot worth it\nWhy\nDefinitely is\nSo-called\nDoes not exist\nAlso is not\nWhy\nGender Equality\n\n\n\nThe interpretation for each topic\n\n\n\n\n\n\n\n\nTopic\nCore Keywords & Interpretation\nSuggested Topic Label\n\n\nTopic 0\nThis topic strongly links the PUA phenomenon with discussions about specific gender roles and identities within the Chinese context. The presence of â€œNot worth itâ€ suggests this cluster is focused on evaluating the value of actions/relationships under the PUA framework.\nPUA & Gender Dynamics in China\n\n\nTopic 1\nThis topic mixes uncertainty and specific financial figures (Over 10k yuan), directly linked to PUA. It suggests discussions about high-stakes financial loss or investment by an individual in a relationship where the outcome or reality is unclear.\nFinancial Dimension of PUA & Uncertainty\n\n\nTopic 2\nThe simultaneous presence of â€œVictim,â€ â€œPUA,â€ and â€œTruly isâ€ indicates a core discussion cluster dedicated to validating the existence and reality of being exploited. â€œMcDonaldâ€™sâ€ implies cheap/casual provision, while â€œFamilyâ€™sâ€ suggests the conversation may touch on the origins or impact of these dynamics within a family unit.\nValidating Victimhood & Low-Cost Exploitation\n\n\nTopic 3\nThis topic is saturated with questioning terms (â€œIs it?â€, â€œWhy?â€, â€œDonâ€™t knowâ€), applied directly to boyfriend/girlfriend roles and the concept of gender equality. It represents a pervasive atmosphere of skepticism and critical discussion about expected behavior in modern relationships.\nSkepticism & Questioning of Relationship Roles\n\n\nTopic 4\nA highly polarized topic that denies (ä¸å¯èƒ½, ä¸å­˜åœ¨) the relevance or existence of Male Superiority (ç”·å°Šå¥³å‘). It focuses on the possibility of being together (åœ¨ä¸€èµ·), suggesting a desire for modern, equal partnerships and a strong rejection of patriarchal norms.\nDenial of Traditional Patriarchy in Relationships\n\n\nTopic 5\nTerms like â€œMore and moreâ€ and â€œMajorityâ€ point to a discussion of social trends and scale. When combined with â€œVictimâ€ and â€œGirlfriend,â€ it indicates a conversation about whether victimhood is becoming increasingly common or if the perception of victimhood is changing within the female partner role.\nDiscussing Shifting Social Norms & Victim Pool\n\n\nTopic 6\nThis is the most explicitly transactional topic. It discusses financial payment (RMB) and the concept of a person being reduced to a â€œTrash Canâ€ (worthless/emotional dumping ground). The use of â€œAnd notâ€ suggests a debate over what a relationship should be versus what it currently is (i.e., not a transaction, but one of money/exploitation).\nMonetary Value vs.Â Emotional Worth (The Price of Simping)\n\n\nTopic 7\nThis topic links resource provision (implied by â€œBuffetâ€ and â€œHighway,â€ often used as metaphors for free/easy access) with discussions of Inequality and Gender Equality. It debates whether resources should be provided freely, who is responsible for providing them, and the resulting fairness in the relationship structure.\nResource Provision & Equality Debate\n\n\n\n\n\n\nDTM\nTopic_Word_Rank\n\n\nV1\nTopic_0\n\n\nV2\nTopic_1\n\n\nV3\nTopic_2\n\n\nV4\nTopic_3\n\n\nV5\nTopic_4\n\n\nV6\nTopic_5\n\n\nV7\nTopic_6\n\n\nV8\nTopic_7"
  },
  {
    "objectID": "758/Final_check-in_2.html#causal-inference",
    "href": "758/Final_check-in_2.html#causal-inference",
    "title": "DACSS785_Final_Project",
    "section": "Causal Inference",
    "text": "Causal Inference\nIn the casual inference part, I present the Ordinary Least Squares (OLS) regression model to analyze how the probability of eight LDA topics influences the number of likes received by a comment (likeCount). Topic V8 (PUA/Victim) was set as the reference group (Reference Topic) in the model. Overall, the modelâ€™s explanatory power is extremely low (\\(\\text{Adjusted R-squared} = 0.00025\\)), suggesting that the variation in likeCount is primarily influenced by factors outside the model, rather than the topics themselves. However, the coefficient tests for individual topics revealed that Topic V2 demonstrated a statistically significant positive influence. After controlling for the effects of other topics, an increase of 1 unit in the probability of Topic V2 (Financial/Money II), relative to the reference group V8, is expected to increase the number of likes by approximately 9.66 (\\(p = 0.038^{*}\\)). This suggests that specific discussion content related to money or finance is more likely to garner attention and agreement within the community. Apart from the intercept, the remaining topics (V1, V3, V4, V5, V6, and V7) did not show a statistically significant relationship with the number of likes.\n\nsource(\"Causal_Inference.R\")\n\nRows: 3006 Columns: 9\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (9): doc_id, V1, V2, V3, V4, V5, V6, V7, V8\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n[1] \"--- OLS Regression Results (Outcome: likeCount) ---\"\n[1] \"Reference Topic: V8 (PUA/Victim)\"\n\nCall:\nlm(formula = likeCount ~ V1 + V2 + V3 + V4 + V5 + V6 + V7, data = merged_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -18.59   -8.93   -6.76   -5.12 1763.41 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.934      2.301   3.882 0.000106 ***\nV1            -2.424      4.144  -0.585 0.558600    \nV2             9.655      4.656   2.074 0.038172 *  \nV3            -2.701      4.312  -0.626 0.531107    \nV4            -2.380      4.266  -0.558 0.576978    \nV5            -2.815      4.399  -0.640 0.522298    \nV6             1.729      4.351   0.398 0.691027    \nV7            -0.953      4.363  -0.218 0.827099    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.08 on 2998 degrees of freedom\nMultiple R-squared:  0.002579,  Adjusted R-squared:  0.00025 \nF-statistic: 1.107 on 7 and 2998 DF,  p-value: 0.3553"
  },
  {
    "objectID": "758/Final_check-in_2.html#conclusion",
    "href": "758/Final_check-in_2.html#conclusion",
    "title": "DACSS785_Final_Project",
    "section": "Conclusion",
    "text": "Conclusion\n(PUT THESE TWO POINT INTO THE FUTURE WORK ALSO NEED TO EXPLAIN ABOUT DIFFICULTY TO COLLECT THE ACADEMIC DATA THERE ** SIMP DOES NOT HAVE A FORMAL DEFINITION**\nâ€œSynthesizing the projectâ€™s findings, the primary discovery is that community engagement (\\(\\text{likeCount}\\)) on YouTube comments is not driven by broad emotional tone or general topics, but rather by a specific, critical narrative focused on â€˜financial exploitation and victimhoodâ€™ (\\(\\text{Topic}\\) \\(\\text{V2}\\)). This specific form of critical discussion related to â€˜SIMPâ€™ behavior is extremely negative in sentiment (\\(\\text{mean}\\) \\(\\text{sentiment} = -0.477\\)) and is so unique in its linguistic pattern that its occurrence can be accurately predicted by the supervised learning model. Consequently, the communityâ€™s response to â€˜SIMPâ€™ behavior is highly concentrated and emotionally charged, with its online visibility predominantly stemming from comments that link the behavior directly to concrete financial inequality and victim scenarios.â€"
  },
  {
    "objectID": "758/Final_check-in_2.html#future",
    "href": "758/Final_check-in_2.html#future",
    "title": "DACSS785_Final_Project",
    "section": "Future",
    "text": "Future\nFuture work should prioritize addressing the observed limitations in both the supervised classification model and the initial data preprocessing pipeline to enhance the robustness and explanatory power of the analysis. Firstly, while the initial Naive Bayes classifier provided baseline insights, its predictive performance should be critically re-evaluated. Improving the accuracy of the automated simp classification label requires exploring more sophisticated machine learning techniques, such as Support Vector Machines (SVMs), Gradient Boosting, or even Transformer-based deep learning models. Concurrently, enhancing the feature set by refining or expanding the custom dictionariesâ€”perhaps incorporating sentiment scores or incorporating word embeddingsâ€”could significantly boost the modelâ€™s ability to discriminate between classes, moving beyond simple bag-of-words approaches. Secondly, a crucial area for improvement lies in the token filtering and data processing stage. Despite standard removal procedures, the presence of numerous contextually irrelevant tokens, such as specific objects (â€œé«˜é€Ÿå…¬è·¯â€) and brands (â€œéº¥ç•¶å‹â€), confirms the necessity of a more rigorous, domain-specific cleanup. Future efforts must focus on constructing an expanded, domain-aware stop word list or implementing Named Entity Recognition (NER) to systematically identify and remove non-topical, low-information tokens, ensuring the remaining features are highly predictive and representative of the core concepts being discussed."
  },
  {
    "objectID": "758/Final_check-in_2.html#reference",
    "href": "758/Final_check-in_2.html#reference",
    "title": "DACSS785_Final_Project",
    "section": "Reference",
    "text": "Reference\nHO, Daniel. The (simp)le truth about excessive & obsessive romantic behaviors in men. (2023). https://ink.library.smu.edu.sg/etd_coll/516\nKrishnamurthy, V., & Duan, Y. (2017). Dependence Structure Analysis Of Meta-level Metrics in YouTube Videos: A Vine Copula Approach. arXiv preprint arXiv:1712.10232. â€œto explain the comment and the view of the video are relatedâ€\nLun-Wei Ku and Hsin-Hsi Chen (2007). Mining Opinions from the Web: Beyond Relevance Retrieval. Journal of American Society for Information Science and Technology, Special Issue on Mining Web Resources for Enhancing Information Retrieval, 58(12), pages 1838-1850.\nPew Research Center. (2020). Many Americans get news on YouTube, where news organizations and independent producers thrive side by side. https://www.pewresearch.org/journalism/2020/09/28/many-americans-get-news-on-youtube-where-news-organizations-and-independent-producers-thrive-side-by-side/\nZhou, W. (2024). é‡åº†è­¦æ–¹å‘å¸ƒâ€œèƒ–çŒ«â€äº‹ä»¶è­¦æƒ…é€šæŠ¥ [Chongqing police issue incident report on the â€œPangmaoâ€ incident]. Xinhua Net. http://www.news.cn/politics/20240519/fb56352660c94810a58e79bc18459a3e/c.html"
  },
  {
    "objectID": "604/604_Final_check-in_2.html",
    "href": "604/604_Final_check-in_2.html",
    "title": "DACSS604_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "604/604_Final_check-in_2.html#research-question-and-hypothesis",
    "href": "604/604_Final_check-in_2.html#research-question-and-hypothesis",
    "title": "DACSS604_Final_Project",
    "section": "",
    "text": "How do the thematic content and emotional framing of YouTube comments about the â€œSuicide of Fat Catâ€ event relate to comment engagement (like count and reply count)?\n\nNull Hypothesis (Hâ‚€):There is no significant linear relationship between the content themes of a comment (as represented by any topic probability from the LDA model) and its community engagement metrics (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count).\nAlternative Hypothesis (Hâ‚):Comment content, specifically themes emphasizing emotional narratives and interpersonal relationships (e.g., Topic 3), will significantly predict higher community engagement (\\(\\text{likeCount}\\) and \\(\\text{reply}\\) count)."
  },
  {
    "objectID": "604/604_Final_check-in_2.html#data-collection",
    "href": "604/604_Final_check-in_2.html#data-collection",
    "title": "DACSS604_Final_Project",
    "section": "Data Collection",
    "text": "Data Collection\nTo explore the concept of â€œsimping,â€ I collected YouTube comments from six relevant videos for textual analysis. I utilized an R scraping script to extract approximately 8,000 comments in total. Following a cleaning and filtering process, a dataset of around 7,000 practical comments was retained for analysis.\nI specifically focused on the case study known as the â€œèƒ–è²“è·³æ±Ÿäº‹ä»¶â€ (Suicide of Fat Cat). This event, which occurred in Mainland China, provides a particularly rich and relevant dataset because it was a well-documented news story officially reported by the Chinese court. This official documentation makes it a real and verifiable event, distinguishing it from mere rumors or social media anecdotes. Furthermore, the use of Chinese-language videos as the reference source is critical, as the Chinese-speaking audience possesses extensive background knowledge and cultural context directly related to the local details of this incident.\n\nSIMP001 - é™ªæ‰“éŠæˆ²è³ºç™¾è¬é¤Šå¥³å‹æ…˜é­åˆ†æ‰‹ï¼ã€Œèƒ–è²“äº‹ä»¶ã€å¼•çˆ†ä¸­åœ‹æ€§åˆ¥æˆ°çˆ­ï¼Ÿã€Œæ’ˆå¥³ã€æ»¿è¡—è·‘çš„èƒŒå¾ŒåŸå› ï¼Ÿã€TODAY çœ‹ä¸–ç•Œã€‘(https://www.youtube.com/watch?v=o5TfkwlthWU&t=13s) 1952 comments from 11/04/2025\nSIMP002 - å½“èƒ–çŒ«é‡åˆ°æå¥³ï¼Œä¸€ä¸ªå¹´è½»äººå¦‚ä½•èµ°ä¸Šä¸å½’è·¯ï¼Ÿï½œå¥³æƒï½œæå¥³ï½œèƒ–çŒ«ï½œç‹è€…è£è€€ï½œç”·å¥³å¹³æƒï½œæ—¥æœ¬ï½œæ¢…å¤§é«˜é€Ÿï½œèˆ†è®ºæ§åˆ¶ï½œç‹å±€æ‹æ¡ˆ20240507 (https://www.youtube.com/watch?v=39Gq_eOPuDY&t=1s) 3731 comments from 11/04/2025\nSIMP003 - è€æ¢ï¼šç»™â€œèƒ–çŒ«â€å¤šæ¡é€‰æ‹© é‡åº†â€œèƒ–çŒ«äº‹ä»¶â€ä¸æ˜¯æ€§åˆ«å¤§æˆ˜ å¦‚ä½•é¿å…æˆä¸ºâ€œèƒ–çŒ«â€(https://www.youtube.com/watch?v=mjcgg0wFpfE) 997 comments from 11/04/2025\nSIMP004 - å°ä¼™ç‚ºæ„›è·³æ±Ÿï¼Œæ‹œé‡‘çš„å¥³å‹ï¼Œå¸è¡€çš„è¦ªå§ï¼Œç„¡è‰¯çš„å•†å®¶ï¼Œç˜‹ç‹‚çš„ç¶²æ°‘ï¼Œèª°æ‰æ˜¯åŠ å®³è€…ï¼Ÿç‚ºä½•è­¦å¯Ÿèªå®šå¥³å‹ç„¡ç½ªï¼Œåè€Œæ˜¯è¦ªå§é•äº†æ³•ï¼Ÿä¸€å£æ°£çœ‹å®Œèƒ–è²“äº‹ä»¶å§‹æœ«ï¼| Wayneèª¿æŸ¥(https://www.youtube.com/watch?v=igs7GoIU4MU) 615 comments from 11/04/2025\nSIMP005 - ç¥ç´šé™ªç©ã€Œèƒ–è²“ã€é­è©ä¹¾227è¬äº¡ å¥³å‹é“æ­‰ï½œ20240506 ETåˆé–“æ–°è (https://www.youtube.com/watch?v=tAE83zZEcOY) 402 comments from 11/04/2025\nSIMP006 - è¢«æ’ˆå¥³é¨™å…‰50è¬ï¼ŒéŠæˆ²å®…ç”·è·³æ±Ÿè‡ªæ®ºï¼Œè½Ÿå‹•å…¨ç¶²ï¼æ’ˆå¥³è­šç«¹æ¦¨ä¹¾èƒ–è²“äº‹ä»¶çœŸç›¸ï¼ã€æ–°é—»æœ€å˜²ç‚¹ å§œå…‰å®‡ã€2024.0508(https://www.youtube.com/watch?v=YYngd2Yt3zk) 271 comments from 11/04/2025\n\n\nlibrary(plyr)\n\nWarning: package 'plyr' was built under R version 4.4.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.4.3\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(purrr)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:plyr':\n\n    compact\n\nlibrary(chromote)\n\nWarning: package 'chromote' was built under R version 4.4.3\n\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tibble)\nlibrary(nnet)\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.4.3\n\n\ncorrplot 0.95 loaded\n\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.4.3\n\n\nLoading required package: NLP\n\n\nWarning: package 'NLP' was built under R version 4.4.2\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.4.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.4.3\n\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.4.3\n\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.4.3\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(\"quanteda.textplots\")\n\nWarning: package 'quanteda.textplots' was built under R version 4.4.3\n\nlibrary(httr)\n\nWarning: package 'httr' was built under R version 4.4.3\n\n\n\nAttaching package: 'httr'\n\n\nThe following object is masked from 'package:NLP':\n\n    content\n\nlibrary(RColorBrewer)\nlibrary(RedditExtractoR)\n\nWarning: package 'RedditExtractoR' was built under R version 4.4.3\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.4.3\n\nlibrary(tidyr)\n\n\ndata1 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\ndata2 &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments.csv\")\ndata3 &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments.csv\")\ndata4 &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments.csv\")\ndata5 &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments.csv\")\ndata6 &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments.csv\")"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "href": "604/604_Final_check-in_2.html#data-cleaning-info-for-the-poster",
    "title": "DACSS604_Final_Project",
    "section": "Data Cleaning (info for the poster",
    "text": "Data Cleaning (info for the poster\nALSO GUIDE IT TO THE PART WHICH I WANT PLUS PRESENT THE CLEAN FORMAT IN THE POSTER\nThe raw dataset, collected as several CSV files, initially contained detailed comment metadata. The original structure included columns such as: videoId, commentId, parentId, author, text, likeCount, publishedAt, updatedAt, viewerRating, canRate, and reply.\nFor data cleaning, all CSV files in the Final_project_data folder were systematically processed using the R environment. The initial step was to streamline the dataset by retaining only the essential variables for textual and engagement analysis: text, likeCount, and reply.\nI utilized the R packages dplyr and stringr to focus on standardizing the text column. This involved a series of cleaning operations: normalization of whitespace (removing line breaks, tabs, and extra spaces, and trimming leading/trailing whitespace) and character filtering. Crucially, I removed non-essential symbols and unusual characters while meticulously preserving all Chinese characters to ensure the comments remained culturally authentic and meaningful for subsequent analysis.\nFinally, each cleaned and standardized dataset was saved as a new CSV file, appended with the suffix _cleaned. UTF-8 encoding was explicitly used to guarantee the accurate representation of the Chinese characters. This systematic workflow ensures the comment data are tidy, standardized, and immediately ready for downstream procedures, such as tokenization and sentiment or frequency analysis.\n\nsource(\"data_cleaning_CN.R\")\n\ndata cleaning complete!.\n\nSIMP001 &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments.csv\")\n\n#Present the eample of the result\nhead(SIMP001)\n\n      videoId                  commentId parentId        author\n1 o5TfkwlthWU UgyekRC230MDXREkdeN4AaABAg     &lt;NA&gt;  @DanjonMeshi\n2 o5TfkwlthWU UgxangSP0zjJm6_gHfV4AaABAg     &lt;NA&gt;  @paullee4451\n3 o5TfkwlthWU UgwZdLtl6Eb2wgDWaDV4AaABAg     &lt;NA&gt;      @urikora\n4 o5TfkwlthWU UgwSmeqUyHYUXGD6l3l4AaABAg     &lt;NA&gt; @fayechen1928\n5 o5TfkwlthWU UgwpotCAmJmn2wWU7u54AaABAg     &lt;NA&gt; @running_goat\n6 o5TfkwlthWU UgxQSyQbnLT9r7I1faB4AaABAg     &lt;NA&gt;  @Jack2006103\n                                                                                                                       text\n1                                                             å•†å®¶é›†é«”çµ¦ç©ºè¢‹çœŸçš„ç¬‘æ­»ï¼Œä¸æ­¢ç”Ÿæ´»åœ¨ä¸­åœ‹ï¼Œé€£æ­»åœ¨ä¸­åœ‹éƒ½è¦å·è‘—æ¨‚ğŸ˜†\n2                                                                                                                      é ­é¦™\n3                       æ›´æ…˜çš„æ˜¯ï¼Œäººéƒ½èµ°äº†ä¸€å€‹æœˆ çµæœå°±åœ¨é€™æ™‚æ©Ÿé»è¢«æŠ“ä¾†æ“‹æ”¿åºœåšçš„é†œäº‹ï¼ˆè·¯å´©è¯ç‚ºè»Šè¡é€²å»å‘æ´ç„¶å¾Œå¿«é€Ÿç‡ƒèµ·ä¾†ï¼‰\n4                                                                                      æ¯æ¬¡çœ‹åˆ°ä¸­åœ‹é€™ç¨®æ‚²åŠ‡éƒ½è¦ºå¾—ä¸å¯æ€è­°ğŸ˜¨ğŸ˜­\n5 æˆ‘çœŸçš„å¿…é ˆå¾—èªªå°å²¸ç”·å¥³æˆ°çˆ­çœŸçš„è¶Šä¾†è¶Šåš´é‡== å•éå¥½å¹¾å€‹å°å²¸çš„å¥³ç”Ÿéƒ½èªç‚ºç”·ç”Ÿå°±æ˜¯æ‡‰è©²è¦çµ¦å½©ç¦® åƒé£¯å°±æ˜¯è¦å¹«å¥³ç”Ÿä»˜éŒ¢ç­‰ç­‰ å¾ˆå¯æ€•\n6                                                                                                é€™äº›åº—å®¶åƒäººè¡€é¥…é ­ï¼Œè¶…å™çˆ›\n  likeCount          publishedAt            updatedAt viewerRating canRate\n1        58 2024-05-09T16:01:52Z 2024-05-09T16:01:52Z         none    TRUE\n2         0 2024-05-09T16:02:23Z 2024-05-09T16:02:23Z         none    TRUE\n3       345 2024-05-09T16:05:05Z 2024-05-09T16:05:05Z         none    TRUE\n4         1 2024-05-09T16:05:57Z 2024-05-09T16:05:57Z         none    TRUE\n5       139 2024-05-09T16:06:47Z 2024-05-09T16:06:47Z         none    TRUE\n6         4 2024-05-09T16:07:29Z 2024-05-09T16:07:29Z         none    TRUE\n  reply\n1 FALSE\n2 FALSE\n3 FALSE\n4 FALSE\n5 FALSE\n6 FALSE\n\ndata1_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_cleaned.csv\")\ndata2_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP002_comments_cleaned.csv\")\ndata3_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP003_comments_cleaned.csv\")\ndata4_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP004_comments_cleaned.csv\")\ndata5_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP005_comments_cleaned.csv\")\ndata6_cleaned &lt;- read.csv(\"Final_project_data/CN_SIMP006_comments_cleaned.csv\")"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#preprocess-the-data",
    "href": "604/604_Final_check-in_2.html#preprocess-the-data",
    "title": "DACSS604_Final_Project",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nFor visualizing the dominant linguistic patterns within the comment data, I employed two complementary approaches. First, a Word Cloud visualization (generated using the Word_cloud_visualization.R script) provided an intuitive, qualitative representation of high-frequency words, instantly highlighting the most common terms associated with discussions of â€œSIMPâ€ behavior.\nSecond, I conducted a quantitative rank-frequency analysis by applying Zipfâ€™s Law to the word corpus. After arranging all unique words by descending frequency and assigning a rank, I plotted the resulting distribution using the ggplot2 package. The resulting visualization confirmed that the comment discourse adheres to a Zipfian distribution, where a few words account for a disproportionate share of the total vocabulary.\nThe key terms driving the discourse were clearly identifiable:\nThese visualizations collectively offer both quantitative validation (Zipfâ€™s Law distribution) and qualitative insight (Word Cloud/Top Terms) into how the audience discusses and perceives the central event and the related concept of â€œSIMPâ€ behavior in this context. The high frequency of questioning and uncertainty (ç‚ºä»€éº¼, ä¸çŸ¥é“, æ˜¯ä¸æ˜¯) coupled with terms of exploitation (pua) and suffering (å—å®³è€…) reveals a key focus on moral judgment and accountability in the discussion.\n\nsource(\"TOKENIZATION.R\")\n\nTokenization complete!\n\nSIMP001_comments_tokens &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_tokens.csv\")\n#Present the eample of the result\nhead(SIMP001_comments_tokens)\n\n  likeCount reply     word\n1       345 FALSE   è¡é€²å»\n2         1 FALSE ä¸å¯æ€è­°\n3       139 FALSE   è¶Šä¾†è¶Š\n4       139 FALSE   å¥½å¹¾å€‹\n5       141 FALSE   éº¥ç•¶å‹\n6         3 FALSE   æœ‰äººèªª\n\n\n\nsource(\"Word_frequency.R\")\n\nWord Frequency Calculation Complete!\n\nSIMP001_wordfreq &lt;- read.csv(\"Final_project_data/CN_SIMP001_comments_wordfreq.csv\")\n\n#Present the eample of the result\nhead(SIMP001_wordfreq)\n\n    word  n rank\n1 å—å®³è€… 75    1\n2 ç‚ºä»€éº¼ 67    2\n3 ä¸çŸ¥é“ 53    3\n4    pua 42    4\n5 é€™ä»¶äº‹ 34    5\n6 ä¸€å€‹äºº 32    6\n\n\n\nsource(\"Same_Word.R\")\n\nCommon words saved to: Final_project_data/common_words_across_files.csv \n\ncommon_words &lt;- read.csv(\"Final_project_data/common_words_across_files.csv\")\n\n#Present the eample of the result\nhead(common_words)\n\n    word total_count\n1 ä¸çŸ¥é“         172\n2 ä¸ºä»€ä¹ˆ         154\n3 å—å®³è€…         118\n4    pua         115\n5 æ˜¯ä¸æ˜¯         101\n6 ç‚ºä»€éº¼         100\n\n\n\nsource(\"Change_to_Traditional_Chinese.R\")\n\nWarning: package 'textstem' was built under R version 4.4.3\n\n\nLoading required package: koRpus.lang.en\n\n\nWarning: package 'koRpus.lang.en' was built under R version 4.4.3\n\n\nLoading required package: koRpus\n\n\nWarning: package 'koRpus' was built under R version 4.4.3\n\n\nLoading required package: sylly\n\n\nWarning: package 'sylly' was built under R version 4.4.3\n\n\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\n\nAttaching package: 'koRpus'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    tokens, types\n\n\nThe following object is masked from 'package:tm':\n\n    readTagged\n\n\nThe following object is masked from 'package:readr':\n\n    tokenize\n\n\nWarning: package 'tmcn' was built under R version 4.4.3\n\n\n# tmcn Version: 0.2-13\n\n\n[1] \"girl\"    \"woman\"   \"simping\" \"lover\"  \nTranslation complete! Output saved to 'your_output_file.csv'\n\nTraditional_Chinese_data_cleaned &lt;- read.csv(\"Final_project_data/traditional_common_words_combined.csv\")\n\n#Present the data after cleaning\nhead(Traditional_Chinese_data_cleaned)\n\n  traditional_text total_count\n1           ç‚ºä»€éº¼         254\n2           ä¸çŸ¥é“         172\n3           å—å®³è€…         118\n4              pua         116\n5           æ˜¯ä¸æ˜¯         101\n6           å¥³æœ‹å‹          95"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#visualization",
    "href": "604/604_Final_check-in_2.html#visualization",
    "title": "DACSS604_Final_Project",
    "section": "Visualization",
    "text": "Visualization\nFor visualizing patterns in the comments, I used two approaches. First, the Word_cloud_visualization.R script generated word clouds to highlight high-frequency words, providing a clear and intuitive view of the most common terms associated with discussions of â€œSIMPâ€ behavior. Second, I applied Zipfâ€™s Law to examine the relationship between word rank and frequency. After arranging words by descending frequency and assigning ranks, I plotted all words using ggplot2, labeling only the top five most frequent words to emphasize the key terms in the discourse. The resulting visualizations offer both quantitative and qualitative insight into how people discuss and perceive â€œSIMPâ€ behavior in YouTube comments.\n\nsource(\"Word_cloud_visualization.R\")\n\n\n\n\n\n\n\n\nWord Cloud generated for: traditional_common_words_combined.csv\n\n\n\n# Sort by frequency and assign ranks\nzipf_data_ranked &lt;- Traditional_Chinese_data_cleaned %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  mutate(rank = row_number())\n\n# Print the top 5 ranked words to confirm the data structure\nprint(head(zipf_data_ranked, 5))\n\n  traditional_text total_count rank\n1           ç‚ºä»€éº¼         254    1\n2           ä¸çŸ¥é“         172    2\n3           å—å®³è€…         118    3\n4              pua         116    4\n5           æ˜¯ä¸æ˜¯         101    5\n\n# --- Linear Scale (As Requested) ---\n\nggplot(zipf_data_ranked, aes(x = rank, y = total_count)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"darkorange\", size = 1.5) +\n  geom_text(\n    # Label the top 8 words\n    aes(label = ifelse(rank &lt;= 6, traditional_text, \"\")),\n    vjust = -0.8,\n    size = 3.5,\n    check_overlap = TRUE # Prevents overlapping labels\n  ) +\n  labs(\n    title = \"Zipfâ€™s Law: Word Rank vs Frequency\",\n    x = \"Rank of Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nTranslation\n\n\n\nRank\nWord\nTranslate\n\n\n1\nç‚ºä»€éº¼\nâ€œWhy / Why is it thatâ€¦â€\n\n\n2\nä¸çŸ¥é“\nâ€œDonâ€™t knowâ€\n\n\n3\nå—å®³è€…\nâ€œVictimâ€\n\n\n4\npua\nâ€œPUAâ€\n\n\n5\næ˜¯ä¸æ˜¯\nâ€œIs it / Is it not?â€\n\n\n6\nå¥³æœ‹å‹\nâ€œGirlfriendâ€"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#word-embedding",
    "href": "604/604_Final_check-in_2.html#word-embedding",
    "title": "DACSS604_Final_Project",
    "section": "Word Embedding",
    "text": "Word Embedding\nFor semantic analysis, I applied Word2Vec using a pseudo-document approach to capture relationships between words in the comments. Each word was repeated according to its frequency (total_count) to create co-occurrence information, which is essential for small datasets where natural co-occurrences are limited. The repeated words were then combined into a single space-separated pseudo-document and used to train a skip-gram Word2Vec model with a vector dimension of 50, window size of 5, and 50 iterations, setting min_count = 1 to include all words.\nThe resulting word vectors allow calculation of cosine similarity to examine semantic relationships between words, as well as clustering and downstream supervised learning tasks. For example, the vector for a keyword such as â€œç‚ºä»€éº¼â€ can be compared with all other word vectors to identify the top semantically similar words, revealing patterns in how concepts related to â€œSIMPâ€ behavior are discussed in YouTube comments. This approach provides a robust representation of word meaning in the context of the dataset while accommodating the limited co-occurrence information inherent in smaller comment datasets.\n\n# Word2Vec can be the best option for the word embeding.\n\nsource(\"Word Embeddings.R\")\n\nWarning: package 'word2vec' was built under R version 4.4.3\n\n\nWarning: package 'text2vec' was built under R version 4.4.3"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#sentiment-analysis",
    "href": "604/604_Final_check-in_2.html#sentiment-analysis",
    "title": "DACSS604_Final_Project",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nFor sentiment analysis, I applied a custom Chinese sentiment dictionary tailored to the context of â€œSIMPâ€ behavior. The dictionary categorizes words into three groups: positive (supportive or relationship-related words such as â€œå¥³æœ‹å‹â€ and â€œé—œå¿ƒâ€), negative (critical or unfairness-related words such as â€œä¸å€¼å¾—â€ and â€œä¸å…¬å¹³â€), and behavior (attention-seeking or â€œsimpâ€ behavior words such as â€œpuaâ€ and â€œè¿½æ±‚â€). Using R, I computed sentiment scores for each word in the dataset by summing occurrences in these categories. A raw polarity score was calculated as the sum of positive and behavior counts minus negative counts, then normalized by the total occurrences of all dictionary words to produce a relative polarity measure.\nThe analysis revealed that the current positive and negative categories do not fully capture the sentiment expressed in the comments. Some words were misclassified or contextually ambiguous, highlighting that the dictionary needs further adjustment and refinement to improve accuracy. Polarity distributions were visualized using a histogram, providing an overview of how positive, negative, and behavior-related language appears in discussions of â€œSIMPâ€ behavior. This approach provides a preliminary sentiment assessment while acknowledging the limitations of the existing dictionary.\n\nsource(\"Sentiment Analysis.R\")\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” forcats   1.0.0     âœ” lubridate 1.9.4\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– NLP::annotate()         masks ggplot2::annotate()\nâœ– httr::content()         masks NLP::content()\nâœ– dplyr::filter()         masks stats::filter()\nâœ– jsonlite::flatten()     masks purrr::flatten()\nâœ– rvest::guess_encoding() masks readr::guess_encoding()\nâœ– dplyr::lag()            masks stats::lag()\nâœ– koRpus::tokenize()      masks readr::tokenize()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'quanteda.textmodels' was built under R version 4.4.3\n\n\nWarning: package 'stopwords' was built under R version 4.4.3\n\n\n\nAttaching package: 'stopwords'\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nWarning: package 'caret' was built under R version 4.4.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:httr':\n\n    progress\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n[1] \"DFM after Custom Simp Dictionary Lookup (Counts per category):\"\n\n### SUPERVISED LEARNING (Naive Bayes Classification)\n[1] \"Confusion Matrix:\"\n\n### LEXICON-BASED NTUSD SENTIMENT RESULTS\n[1] \"Mean Sentiment Score by Simping Label (NTUSD):\"\n# A tibble: 2 Ã— 2\n  contains_simp_factor mean_sentiment\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 FALSE                       -0.0990\n2 TRUE                        -0.477 \n\n\n\n\n\n\n\n\n\nIn this graph:\nX-Axis: Simping Label:\n\nFALSE: Comments that do not contain any of the words from your custom â€œsimpâ€ dictionary (e.g., â€œèˆ”ç‹—,â€ â€œå·¥å…·äºº,â€ â€œä¸€å»‚æƒ…é¡˜,â€ etc.).\nTRUE: Comments that do contain at least one word from your custom â€œsimpâ€ dictionary.\n\nY-Axis: Comment Sentiment Score:\n\nPositive Scores (above 0): Indicate a more positive overall sentiment.\nZero (0): Indicates a neutral or balanced sentiment.\nNegative Scores (below 0): Indicate a more negative overall sentiment."
  },
  {
    "objectID": "604/604_Final_check-in_2.html#sentiment-analysis-summary",
    "href": "604/604_Final_check-in_2.html#sentiment-analysis-summary",
    "title": "DACSS604_Final_Project",
    "section": "Sentiment Analysis Summary",
    "text": "Sentiment Analysis Summary\nThe lexicon-based sentiment analysis, utilizing the NTUSD dictionary, reveals a pronounced negative emotional shift in texts discussing the â€œsimpâ€ phenomenon. Specifically, content that contains terms from the custom dictionaryâ€”which targets themes like â€œsimp behaviorâ€ (e.g., èˆ”ç‹—, simp), â€œvictim positionâ€ (e.g., å—å®³è€…, pua), and â€œrelationship imbalanceâ€â€”shows a highly negative mean sentiment score of -0.477. This score is significantly more negative than the average score of -0.0990 found in texts that do not contain these specific terms. This sharp difference (a nearly five-fold increase in negative sentiment magnitude) indicates that conversations about excessive one-sided effort, perceived exploitation, and unequal relationshipsâ€”the core of the â€œsimpâ€ conceptâ€”are strongly associated with negative emotional discourse within the corpus."
  },
  {
    "objectID": "604/604_Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "href": "604/604_Final_check-in_2.html#supervised-learning-analysis-naive-bayes-classification",
    "title": "DACSS604_Final_Project",
    "section": "Supervised Learning Analysis (Naive Bayes Classification)",
    "text": "Supervised Learning Analysis (Naive Bayes Classification)\nThe Naive Bayes model was employed to classify comments based on whether they contained the â€œsimpâ€ factor, using a cleaned feature set that excluded all words from the custom â€œsimpâ€ dictionary to prevent data leakage. The model achieved an overall Accuracy of 81.69%, which is slightly higher than the No Information Rate (NIR) of 80.36%, indicating its performance is marginally better than random guessing based on class prevalence.\nHowever, a closer look at the results reveals a significant class imbalance issue and skewed performance:\n\nHigh Sensitivity (Recall): The model is excellent at correctly identifying comments that do NOT contain the simp factor (the majority class, FALSE), with a high Sensitivity of 95.72%.\nLow Specificity: Conversely, the model is very poor at correctly identifying comments that DO contain the simp factor (the minority class, TRUE), with a low Specificity of 24.29%.\nKappa Value: The Kappa statistic of 0.2565 suggests only a fair level of agreement beyond chance.\n\nIn summary, the high overall accuracy is largely driven by correctly classifying the prevalent negative class (FALSE). The model struggles to reliably identify actual â€œsimpâ€ comments (TRUE), suggesting that the remaining general vocabulary in the comments lacks sufficient predictive power to consistently distinguish between the two categories without the core dictionary terms.\n\nsource(\"Supervised_Learning.R\")\n\n\n--- Solving Data Leakage: Remove the word which exist in Simp dictionary ---\n[1] \"Original (matrix_main): 2703\"\n[1] \"Remove the word in Simp dictionary (X_cleaned): 2674\"\n\n--- 6. Naive Bayes Training ---\n\n--- Naive Bayes Prediction ---\n[1] \"Confusion Matrix:\"\nConfusion Matrix and Statistics\n\n                 y_test\npredicted_cleaned FALSE TRUE\n            FALSE   693  134\n            TRUE     31   43\n                                        \n               Accuracy : 0.8169        \n                 95% CI : (0.79, 0.8416)\n    No Information Rate : 0.8036        \n    P-Value [Acc &gt; NIR] : 0.1676        \n                                        \n                  Kappa : 0.2565        \n                                        \n Mcnemar's Test P-Value : 2.011e-15     \n                                        \n            Sensitivity : 0.9572        \n            Specificity : 0.2429        \n         Pos Pred Value : 0.8380        \n         Neg Pred Value : 0.5811        \n              Precision : 0.8380        \n                 Recall : 0.9572        \n                     F1 : 0.8936        \n             Prevalence : 0.8036        \n         Detection Rate : 0.7691        \n   Detection Prevalence : 0.9179        \n      Balanced Accuracy : 0.6001        \n                                        \n       'Positive' Class : FALSE"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#topic-modeling-lda",
    "href": "604/604_Final_check-in_2.html#topic-modeling-lda",
    "title": "DACSS604_Final_Project",
    "section": "Topic Modeling (LDA)",
    "text": "Topic Modeling (LDA)\nThe script follows a standard text mining workflow using the tidyverse and text2vec packages:\n\nData Preparation: It reads the individual tokenized comment files, reconstructs the full comments by assigning and aggregating tokens by a unique document ID (doc_id), and then combines the tokens back into complete text strings.\nFeature Engineering: It creates an iterator from the aggregated text and builds a vocabulary. Crucially, it prunes the vocabulary by removing words that occur less than three times (term_count_min = 3), which helps reduce noise and improves the quality of the derived topics.\nDTM Creation: The processed tokens are converted into a Document-Term Matrix (DTM), which is the input required for LDA.\nModel Training: The script initializes and trains an LDA model with a predefined number of topics (K=8) and 500 iterations.\nOutput: Finally, the code extracts and saves two key results: the Topic-Word distribution (the top 10 most characteristic words for each of the 8 topics) and the Document-Topic distribution (the probability that each comment belongs to each topic), storing both as CSV files for subsequent qualitative analysis.\n\n\nsource(\"Topic_Model.R\")\n\n[1] \"Starting LDA Topic Modeling with K = 8\"\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======================================================================| 100%\n[1] \"LDA Training Complete.\"\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nâ„¹ Using compatibility `.name_repair`.\n\n\n[1] \"Top 10 Words for Each Topic:\"\n# A tibble: 10 Ã— 9\n   Topic_Word_Rank Topic_0 Topic_1 Topic_2 Topic_3  Topic_4  Topic_5 Topic_6\n   &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1 Word_1          pua     ä¸çŸ¥é“  å—å®³è€…  æ˜¯ä¸æ˜¯   ç‚ºä»€éº¼   ä¸çŸ¥é“  è€Œä¸æ˜¯ \n 2 Word_2          ä¸ºä»€ä¹ˆ  ä¸‡ä½™å…ƒ  ä¸ºä»€ä¹ˆ  å¥³æœ‹å‹   åœ¨ä¸€èµ·   è¶Šæ¥è¶Š  é€™ä»¶äº‹ \n 3 Word_3          ä¸­å›½äºº  pua     pua     ä¸ºä»€ä¹ˆ   ä¹Ÿå¯ä»¥   å¤§éƒ¨åˆ†  ä¸çŸ¥é“ \n 4 Word_4          ç”·å­©å­  å¯èƒ½æ˜¯  çœŸçš„æ˜¯  ä¸çŸ¥é“   ä¸å¯èƒ½   éƒ½ä¸æ˜¯  ä¸€å®šæ˜¯ \n 5 Word_5          å¥³å­©å­  ä¸€å€‹äºº  éº¦å½“åŠ³  ä¸éœ€è¦   éƒ½æ²’æœ‰   çœŸçš„æ˜¯  å¥³æœ‹å‹ \n 6 Word_6          ä¹Ÿä¸æ˜¯  é€™ä»¶äº‹  å®¶åº­çš„  ä¹Ÿæ²¡æœ‰   é€™å°±æ˜¯   å—å®³è€…  èƒ½ä¸èƒ½ \n 7 Word_7          æ˜¯ä¸æ˜¯  ä¸­å›½äºº  å…¨ä¸–ç•Œ  å¤§éƒ¨åˆ†   ä¹Ÿæ²’æœ‰   å¥³æœ‹å‹  äººæ°‘å¹£ \n 8 Word_8          æ²¡ä»€ä¹ˆ  å°¤å…¶æ˜¯  å…¶å®æ˜¯  ç”·æœ‹å‹   ç”·å°Šå¥³å‘ æˆ‘çŸ¥é“  åƒåœ¾æ¡¶ \n 9 Word_9          å¯èƒ½æ˜¯  çœŸçš„æ˜¯  å¥½åƒæ˜¯  ç”·å¥³å¹³ç­‰ æ²’ä»€éº¼   æ˜¯ä¸æ˜¯  æœ‰äº›äºº \n10 Word_10         ä¸å€¼å¾—  ç‚ºä»€éº¼  è‚¯å®šæ˜¯  æ‰€è°“çš„   ä¸å­˜åœ¨   ä¹Ÿä¸æ˜¯  ä¸ºä»€ä¹ˆ \n# â„¹ 1 more variable: Topic_7 &lt;chr&gt;\n[1] \"Saved topic words to lda_topic_words.csv\"\n[1] \"Document-Topic Distribution (Head):\"\n# A tibble: 6 Ã— 9\n  doc_id    V1    V2    V3    V4    V5    V6    V7    V8\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1     0  0      0    0      0     0       0     0\n2      2     0  0      0    0      0     0       0     0\n3      3     0  0.05   0    0.15   0.6   0.2     0     0\n4      4     0  0.1    0.2  0      0.4   0.3     0     0\n5      5     0  0      0.8  0      0.2   0       0     0\n6      6     0  0      0    0      0     0       1     0\n[1] \"Saved document-topic distribution to lda_doc_topic_distr.csv\"\n\n\nTranslation for the every words in the topics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic_Word_Rank\nTopic_0\nTopic_1\nTopic_2\nTopic_3\nTopic_4\nTopic_5\nTopic_6\nTopic_7\n\n\nWord_1\nPUA\nDonâ€™t know\nVictim\nIs it?\nWhy\nDonâ€™t know\nAnd not\nBuffet (Resource)\n\n\nWord_2\nWhy\nOver 10k yuan\nWhy\nGirlfriend\nBe together\nMore and more\nThis matter\nHighway\n\n\nWord_3\nChinese people\nPUA\nPUA\nWhy\nAlso can\nMajority\nDonâ€™t know\nToo good\n\n\nWord_4\nBoy / Male\nPossibly\nTruly is\nDonâ€™t know\nImpossible\nAre not all\nMust be\nWhy\n\n\nWord_5\nGirl / Female\nA person\nMcDonaldâ€™s\nDonâ€™t need\nDonâ€™t have at all\nTruly is\nGirlfriend\nMainly is\n\n\nWord_6\nAlso is not\nThis matter\nFamilyâ€™s\nAlso donâ€™t have\nThis is\nVictim\nCan or cannot\nSome people\n\n\nWord_7\nIs it?\nChinese people\nWhole world\nMajority\nAlso donâ€™t have\nGirlfriend\nRMB (Money)\nInequality\n\n\nWord_8\nNothing much\nEspecially\nActually is\nBoyfriend\nMale Superiority\nI know\nTrash Can (Worthless)\nShould be\n\n\nWord_9\nPossibly\nTruly is\nSeems like\nGender Equality\nNothing much\nIs it?\nSome people\nA person\n\n\nWord_10\nNot worth it\nWhy\nDefinitely is\nSo-called\nDoes not exist\nAlso is not\nWhy\nGender Equality\n\n\n\nThe interpretation for each topic\n\n\n\n\n\n\n\n\nTopic\nCore Keywords & Interpretation\nSuggested Topic Label\n\n\nTopic 0\nThis topic strongly links the PUA phenomenon with discussions about specific gender roles and identities within the Chinese context. The presence of â€œNot worth itâ€ suggests this cluster is focused on evaluating the value of actions/relationships under the PUA framework.\nPUA & Gender Dynamics in China\n\n\nTopic 1\nThis topic mixes uncertainty and specific financial figures (Over 10k yuan), directly linked to PUA. It suggests discussions about high-stakes financial loss or investment by an individual in a relationship where the outcome or reality is unclear.\nFinancial Dimension of PUA & Uncertainty\n\n\nTopic 2\nThe simultaneous presence of â€œVictim,â€ â€œPUA,â€ and â€œTruly isâ€ indicates a core discussion cluster dedicated to validating the existence and reality of being exploited. â€œMcDonaldâ€™sâ€ implies cheap/casual provision, while â€œFamilyâ€™sâ€ suggests the conversation may touch on the origins or impact of these dynamics within a family unit.\nValidating Victimhood & Low-Cost Exploitation\n\n\nTopic 3\nThis topic is saturated with questioning terms (â€œIs it?â€, â€œWhy?â€, â€œDonâ€™t knowâ€), applied directly to boyfriend/girlfriend roles and the concept of gender equality. It represents a pervasive atmosphere of skepticism and critical discussion about expected behavior in modern relationships.\nSkepticism & Questioning of Relationship Roles\n\n\nTopic 4\nA highly polarized topic that denies (ä¸å¯èƒ½, ä¸å­˜åœ¨) the relevance or existence of Male Superiority (ç”·å°Šå¥³å‘). It focuses on the possibility of being together (åœ¨ä¸€èµ·), suggesting a desire for modern, equal partnerships and a strong rejection of patriarchal norms.\nDenial of Traditional Patriarchy in Relationships\n\n\nTopic 5\nTerms like â€œMore and moreâ€ and â€œMajorityâ€ point to a discussion of social trends and scale. When combined with â€œVictimâ€ and â€œGirlfriend,â€ it indicates a conversation about whether victimhood is becoming increasingly common or if the perception of victimhood is changing within the female partner role.\nDiscussing Shifting Social Norms & Victim Pool\n\n\nTopic 6\nThis is the most explicitly transactional topic. It discusses financial payment (RMB) and the concept of a person being reduced to a â€œTrash Canâ€ (worthless/emotional dumping ground). The use of â€œAnd notâ€ suggests a debate over what a relationship should be versus what it currently is (i.e., not a transaction, but one of money/exploitation).\nMonetary Value vs.Â Emotional Worth (The Price of Simping)\n\n\nTopic 7\nThis topic links resource provision (implied by â€œBuffetâ€ and â€œHighway,â€ often used as metaphors for free/easy access) with discussions of Inequality and Gender Equality. It debates whether resources should be provided freely, who is responsible for providing them, and the resulting fairness in the relationship structure.\nResource Provision & Equality Debate\n\n\n\n\n\n\nDTM\nTopic_Word_Rank\n\n\nV1\nTopic_0\n\n\nV2\nTopic_1\n\n\nV3\nTopic_2\n\n\nV4\nTopic_3\n\n\nV5\nTopic_4\n\n\nV6\nTopic_5\n\n\nV7\nTopic_6\n\n\nV8\nTopic_7"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#causal-inference",
    "href": "604/604_Final_check-in_2.html#causal-inference",
    "title": "DACSS604_Final_Project",
    "section": "Causal Inference",
    "text": "Causal Inference\nIn the casual inference part, I present the Ordinary Least Squares (OLS) regression model to analyze how the probability of eight LDA topics influences the number of likes received by a comment (likeCount). Topic V8 (PUA/Victim) was set as the reference group (Reference Topic) in the model. Overall, the modelâ€™s explanatory power is extremely low (\\(\\text{Adjusted R-squared} = 0.00025\\)), suggesting that the variation in likeCount is primarily influenced by factors outside the model, rather than the topics themselves. However, the coefficient tests for individual topics revealed that Topic V2 demonstrated a statistically significant positive influence. After controlling for the effects of other topics, an increase of 1 unit in the probability of Topic V2 (Financial/Money II), relative to the reference group V8, is expected to increase the number of likes by approximately 9.66 (\\(p = 0.038^{*}\\)). This suggests that specific discussion content related to money or finance is more likely to garner attention and agreement within the community. Apart from the intercept, the remaining topics (V1, V3, V4, V5, V6, and V7) did not show a statistically significant relationship with the number of likes.\n\nsource(\"Causal_Inference.R\")\n\nRows: 3006 Columns: 9\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (9): doc_id, V1, V2, V3, V4, V5, V6, V7, V8\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n[1] \"--- OLS Regression Results (Outcome: likeCount) ---\"\n[1] \"Reference Topic: V8 (PUA/Victim)\"\n\nCall:\nlm(formula = likeCount ~ V1 + V2 + V3 + V4 + V5 + V6 + V7, data = merged_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -18.59   -8.93   -6.76   -5.12 1763.41 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.934      2.301   3.882 0.000106 ***\nV1            -2.424      4.144  -0.585 0.558600    \nV2             9.655      4.656   2.074 0.038172 *  \nV3            -2.701      4.312  -0.626 0.531107    \nV4            -2.380      4.266  -0.558 0.576978    \nV5            -2.815      4.399  -0.640 0.522298    \nV6             1.729      4.351   0.398 0.691027    \nV7            -0.953      4.363  -0.218 0.827099    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.08 on 2998 degrees of freedom\nMultiple R-squared:  0.002579,  Adjusted R-squared:  0.00025 \nF-statistic: 1.107 on 7 and 2998 DF,  p-value: 0.3553"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#conclusion",
    "href": "604/604_Final_check-in_2.html#conclusion",
    "title": "DACSS604_Final_Project",
    "section": "Conclusion",
    "text": "Conclusion\n(PUT THESE TWO POINT INTO THE FUTURE WORK ALSO NEED TO EXPLAIN ABOUT DIFFICULTY TO COLLECT THE ACADEMIC DATA THERE ** SIMP DOES NOT HAVE A FORMAL DEFINITION**\nâ€œSynthesizing the projectâ€™s findings, the primary discovery is that community engagement (\\(\\text{likeCount}\\)) on YouTube comments is not driven by broad emotional tone or general topics, but rather by a specific, critical narrative focused on â€˜financial exploitation and victimhoodâ€™ (\\(\\text{Topic}\\) \\(\\text{V2}\\)). This specific form of critical discussion related to â€˜SIMPâ€™ behavior is extremely negative in sentiment (\\(\\text{mean}\\) \\(\\text{sentiment} = -0.477\\)) and is so unique in its linguistic pattern that its occurrence can be accurately predicted by the supervised learning model. Consequently, the communityâ€™s response to â€˜SIMPâ€™ behavior is highly concentrated and emotionally charged, with its online visibility predominantly stemming from comments that link the behavior directly to concrete financial inequality and victim scenarios.â€"
  },
  {
    "objectID": "604/604_Final_check-in_2.html#future",
    "href": "604/604_Final_check-in_2.html#future",
    "title": "DACSS604_Final_Project",
    "section": "Future",
    "text": "Future\nFuture work should prioritize addressing the observed limitations in both the supervised classification model and the initial data preprocessing pipeline to enhance the robustness and explanatory power of the analysis. Firstly, while the initial Naive Bayes classifier provided baseline insights, its predictive performance should be critically re-evaluated. Improving the accuracy of the automated simp classification label requires exploring more sophisticated machine learning techniques, such as Support Vector Machines (SVMs), Gradient Boosting, or even Transformer-based deep learning models. Concurrently, enhancing the feature set by refining or expanding the custom dictionariesâ€”perhaps incorporating sentiment scores or incorporating word embeddingsâ€”could significantly boost the modelâ€™s ability to discriminate between classes, moving beyond simple bag-of-words approaches. Secondly, a crucial area for improvement lies in the token filtering and data processing stage. Despite standard removal procedures, the presence of numerous contextually irrelevant tokens, such as specific objects (â€œé«˜é€Ÿå…¬è·¯â€) and brands (â€œéº¥ç•¶å‹â€), confirms the necessity of a more rigorous, domain-specific cleanup. Future efforts must focus on constructing an expanded, domain-aware stop word list or implementing Named Entity Recognition (NER) to systematically identify and remove non-topical, low-information tokens, ensuring the remaining features are highly predictive and representative of the core concepts being discussed."
  },
  {
    "objectID": "604/604_Final_check-in_2.html#reference",
    "href": "604/604_Final_check-in_2.html#reference",
    "title": "DACSS604_Final_Project",
    "section": "Reference",
    "text": "Reference\nHO, Daniel. The (simp)le truth about excessive & obsessive romantic behaviors in men. (2023). https://ink.library.smu.edu.sg/etd_coll/516\nKrishnamurthy, V., & Duan, Y. (2017). Dependence Structure Analysis Of Meta-level Metrics in YouTube Videos: A Vine Copula Approach. arXiv preprint arXiv:1712.10232. â€œto explain the comment and the view of the video are relatedâ€\nLun-Wei Ku and Hsin-Hsi Chen (2007). Mining Opinions from the Web: Beyond Relevance Retrieval. Journal of American Society for Information Science and Technology, Special Issue on Mining Web Resources for Enhancing Information Retrieval, 58(12), pages 1838-1850.\nPew Research Center. (2020). Many Americans get news on YouTube, where news organizations and independent producers thrive side by side. https://www.pewresearch.org/journalism/2020/09/28/many-americans-get-news-on-youtube-where-news-organizations-and-independent-producers-thrive-side-by-side/\nZhou, W. (2024). é‡åº†è­¦æ–¹å‘å¸ƒâ€œèƒ–çŒ«â€äº‹ä»¶è­¦æƒ…é€šæŠ¥ [Chongqing police issue incident report on the â€œPangmaoâ€ incident]. Xinhua Net. http://www.news.cn/politics/20240519/fb56352660c94810a58e79bc18459a3e/c.html"
  }
]