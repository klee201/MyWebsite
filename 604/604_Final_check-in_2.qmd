---
title: "DACSS604_Final_Project"
author: "Kuan-Cheng Lee"
editor: visual
format: 
  html: 
    embed-resources: true
    self-contained-math: true
---

## 300-Word Summary Describing

For my final project in DACSS604. A **data-story page** is the best way to present my project. There are two reasons. Firstly, of all, it can present the entire information for the webpage. After I complete the code part, I render my project files into an **HTML** file. In this case, I can present my results completely on different devices. The last reason is that html has low occupy fewer storage.

The audiences are expected to have basic knowledge of the substantive knowledge in **artificial intelligence** and a moderate level of **data science** background. It is very important that they have background knowledge of artificial intelligence. There are two reasons. First of all, they have a more accurate judgement for AI because they understand how the AI works. The final is that they can better to understand in the YouTube comments, especially when they conversation between abstract technical critiques and discussions about industry leaders. In the project, It present many data visualizations, such as a **word cloud** and **zipf’s law graph**. The other is that the method of the data requires basic knowledge from the data science. In the project, the **LDA model** and **OLS regression** are the two methods used to present the results for the project. In this case, it is necessary to expect the audiences have the background knowledge for data science.

The research question is that **“To what extent does the thematic focus of YouTube comments, specifically the distinction between abstract AI trust critiques versus discourse entered on key industry figures and events, influence audience acceptance (log-transformed likeCounts), when controlling for the level of engagement (reply counts)? “.**

## Research Question and Hypothesis

**YouTube** is one of the most famous social media platforms in the world. Many people present their opinions on the media platform. In this case, it become one of the best platform to collect the comments from the people about the AI trust.

**Research Question (RQ)**:

> To what extent does the thematic focus of a YouTube comment—specifically the distinction between abstract AI trust critiques versus discourse centered on key industry figures and events—influence audience acceptance (log-transformed like counts), when controlling for the level of engagement (reply counts)?

#### Hypotheses:

-   Null Hypothesis ($H_0$): After controlling for the number of replies, the specific thematic composition of a comment regarding AI Trust (Topics $V_1$ through $V_7$) has no significant influence on audience acceptance compared to the reference theme of key figures and interviews ($V_8$).

-   Alternative Hypothesis ($H_1$): The thematic focus of a comment is a significant predictor of audience acceptance. Specifically, discourse centered on key industry figures and specific events ($V_8$) will result in significantly higher log-transformed like counts than comments focused on abstract themes, media critiques, or "fake" video concerns (Topics $V_1$ through $V_7$).

**Independent Variables (IVs)**

|  |  |  |  |
|----|----|----|----|
| **Variable** | **Type** | **Operational Definition (Measurement)** | **Relation to Trust** |
| **Topic Tendency** (Primary IV) | Categorical/Continuous | The **probability distribution** of a comment belonging to a specific topic $V_i$ (where $i=1$ to $7$), derived from the LDA model (e.g., $V_4$: News / Fake / Video). | Different themes signal distinct perspectives on the content, potentially mediating trust attitudes. |
| **Reference Topic (**$V_8$) | Categorical | The reference group (or baseline) for the regression model, representing comments highly focused on **Sam Altman / Interview**. | Used as the benchmark to measure if other themes (like $V_4$) exhibit significantly higher or lower audience acceptance. |
| **Reply Count** (Control IV) | Count/Continuous | The raw number of replies a comment receives. This is essential to control for the *interactivity* of a comment, which is independent of its content theme. | Controls for engagement bias; ensures theme effects are not simply due to high controversy/activity. |

**Dependent Variable (DV)**

|  |  |  |  |
|----|----|----|----|
| **Variable** | **Type** | **Operational Definition (Measurement)** | **Justification** |
| **Audience Acceptance (Proxy for Trust)** | Continuous | **Log-Transformed Like Count:** $\ln(\text{likeCount} + 1)$ for each comment. | **Log Transformation** handles the extreme skewness of like counts. **Justification:** In social media comments, higher like counts signal greater audience agreement, approval, or acceptance of the sentiment/attitude expressed in the comment. This acceptance is used as a measurable proxy for alignment with (or trust in) the AI/Human stance implicitly or explicitly discussed in that comment's topic. |

## Data Collection

To explore the concept of **“AI trust,”** I collected **YouTube comments** from six relevant videos for textual analysis. I used the Google API to collect over **12,000 comments** in total.

The core data for this analysis presents the comments from the audience sourced from YouTube videos. To ensure relevance to the research theme of **AI Trust**, a targeted collection strategy was employed. When I did the data collection, I used the keyword "**AI trust**" in the YouTube search box. After that, I set the filter to the top viewing number. To make sure I can get enough quantity for the comments. After that, I selected the video from **TED** or **other news media** to make sure accuracy of the videos

1.  AI001 - **OpenAI’s Sam Altman Talks ChatGPT, AI Agents and Superintelligence — Live at TED2025** (<https://www.youtube.com/watch?v=5MWT_doo68k>) 3965 comments from 12/13/2025

2.  AI002 - **AI Is Dangerous, but Not for the Reasons You Think \| Sasha Luccioni \| TED** (<https://www.youtube.com/watch?v=eXdVDhOGqoE&t=1s>) 2705 comments from 12/13/2025

3.  AI003 - **In the Age of AI, Trust is Key \| Dominique Shelton Leipzig \| TEDxSantaBarbaraSalon** (<https://www.youtube.com/watch?v=p7BCRVD_F30>) 473 comments from 12/13/2025

4.  AI004 - **AI news videos blur line between real and fake reports** (<https://www.youtube.com/watch?v=SRA4brHXBBQ>) 4086 comments from 12/13/2025

5.  AI005 - **Agentic AI - how bots came for our workflows and drudgery \| FT Working It** (<https://www.youtube.com/watch?v=e85AxYW0Qyk>) 341 comments from 12/13/2025

6.  AI006 - **Should you trust marketing AI? \| Ramkumar Sethuramalingam \| TEDxSchlossplatz** (<https://www.youtube.com/watch?v=qIopJqbzXl0>) 176 comments from 12/13/2025

```{r library}
library(plyr)
library(dplyr)
library(stringr)
library(tidytext)
library(readr)
library(purrr)
library(chromote)
library(stargazer)
library(readxl)
library(ggplot2)
library(tibble)
library(nnet)
library(corrplot)
library(tm)
library(wordcloud)
library(quanteda)
library(rvest)
library(jsonlite)
library("quanteda.textplots")
library(httr)
library(RColorBrewer)
library(RedditExtractoR)
library(httr2)
library(tidyr)

```

```{r scrape and collect yt command}

#source("Scrape_youtube-AI001.R")
#source("Scrape_youtube-AI002.R")
#source("Scrape_youtube-AI003.R")
#source("Scrape_youtube-AI004.R")
#source("Scrape_youtube-AI005.R")
#source("Scrape_youtube-AI006.R")


data1 <- read.csv("Final_project_data/AI001_comments.csv")
data2 <- read.csv("Final_project_data/AI002_comments.csv")
data3 <- read.csv("Final_project_data/AI003_comments.csv")
data4 <- read.csv("Final_project_data/AI004_comments.csv")
data5 <- read.csv("Final_project_data/AI005_comments.csv")
data6 <- read.csv("Final_project_data/AI006_comments.csv")

```

## Data Cleaning

The raw dataset, collected as six CSV files, initially contained detailed comment metadata. `videoId`, `commentId`, `parentId`, `author`, `text`, `likeCount`, `publishedAt`, `updatedAt`, `viewerRating`, `canRate`, and `reply` are the 11 columns in the raw dataset.

For data cleaning, all CSV files are processed using the `data_cleaning.R` file for the first process of the data cleaning. The initial step was to remove the space, speical characters, and the emojis and get rid of the 8 columns expcet **text**, **likeCount**, and **reply**.

Finally, each cleaned and standardized data set was saved as a new CSV file, appended with the suffix `_cleaned`. In this case, the audiences can find the cleaned data easily and also check that the program works properly.

```{r data cleaning}

source("data_cleaning.R")

AI001 <- read.csv("Final_project_data/AI001_comments.csv")

#Present the eample of the result
head(AI001)

data1_cleaned <- read.csv("Final_project_data/AI001_comments_cleaned.csv")
data2_cleaned <- read.csv("Final_project_data/AI002_comments_cleaned.csv")
data3_cleaned <- read.csv("Final_project_data/AI003_comments_cleaned.csv")
data4_cleaned <- read.csv("Final_project_data/AI004_comments_cleaned.csv")
data5_cleaned <- read.csv("Final_project_data/AI005_comments_cleaned.csv")
data6_cleaned <- read.csv("Final_project_data/AI006_comments_cleaned.csv")


```

## Preprocess the data

After completing the data cleaning, tokenization is the necessary process to make the text into data. For tokenization, the program makes the sentences into words. In this case, we can present the text about what people think in their minds. After this processing, the output is the CSV files with `_token`

The second process is `Word Frequency.R`. In this program, it combines all the `_token` files together and counts the frequency of the words. The output is the CVS files with `_wordfreq`

The last process is the `Same_Word.R`. After the word frequency process, the data does not present correctly. There are still some words that are repeated. In this case, I created another program to solve this issue.

```{r TOKENIZATION}


source("TOKENIZATION.R")


AI001_comments_tokens <- read.csv("Final_project_data/AI001_comments_tokens.csv")
#Present the eample of the result
head(AI001_comments_tokens)

```

```{r Word_frequency}



source("Word_frequency.R")

AI001_wordfreq <- read.csv("Final_project_data/AI001_comments_wordfreq.csv")

#Present the eample of the result
head(AI001_wordfreq)
```

```{r Same_Word}

source("Same_Word.R")


common_words <- read.csv("Final_project_data/common_words_across_files.csv")

#Present the eample of the result
head(common_words)


```

## Visualization

For visualizing patterns in the comments, I used two approaches. First, the `Word_cloud_visualization.R` script generated word clouds to highlight high-frequency words, providing a clear view of the most common terms associated with discussions of “AI Trust”. Second, I applied Zipf’s Law to present the relationship between word rank and frequency.

```{r Word_cloud_visualization}


source("Word_cloud_visualization.R")


```

```{r Zipf}

# Sort by frequency and assign ranks
zipf_data_ranked <- common_words %>%
  arrange(desc(total_count)) %>%
  mutate(rank = row_number())

# Print the top 5 ranked words to confirm the data structure
print(head(zipf_data_ranked, 5))

# --- Linear Scale (As Requested) ---

ggplot(zipf_data_ranked, aes(x = rank, y = total_count)) +
  geom_line(color = "steelblue") +
  geom_point(color = "darkorange", size = 1.5) +
  geom_text(
    # Label the top 8 words
    aes(label = ifelse(rank <= 6, word, "")),
    vjust = -0.8,
    size = 3.5,
    check_overlap = TRUE # Prevents overlapping labels
  ) +
  labs(
    title = "Zipf’s Law: Word Rank vs Frequency",
    x = "Rank of Word",
    y = "Frequency"
  ) +
  theme_minimal(base_size = 13)





```

## Word Embedding

To analyze the relationships within the audience comments, a **Word2Vec** model was trained using the skip-gram architecture. The program set 50 dimensions, 5 windows, and 50 iterations. The model was trained using a custom corpus constructed by repeating each word based on its total frequency across all documents, effectively weighting the model by word importance. The final result will be presented as a **Semantic Network Graph**.

```{r Word Embeddings}

# Word2Vec can be the best option for the word embeding also include the network graph.

source("Word Embeddings.R")

# ==================================
# 9. Calculate Node Size and Plot the Network
# ==================================

# 9.1. Calculate Node Size 
# Use 'strength' (sum of connected similarity scores) to size the nodes.
V(net)$size <- strength(net, weights = E(net)$weight) * 0.1 

# 9.2. Plot the Network
library(ggraph)
library(ggplot2)

ggraph(net, layout = "fr") +
  # Draw edges (connections)
  geom_edge_fan(aes(alpha = weight), 
                show.legend = FALSE, 
                edge_width = 0.5, 
                color = "gray60") +
  # Draw nodes (words)
  geom_node_point(aes(size = size), color = "steelblue", alpha = 0.8) +
  # Label nodes (repel ensures labels don't overlap much)
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +
  
  labs(
    title = paste("Semantic Network of Words Similar to 'people'"),
    subtitle = paste("Connections are Cosine Similarity >", threshold)
  ) +
  theme_graph()

```

## Sentiment Analysis

The analysis of audience comments employed a hybrid approach combining dictionary-based sentiment scoring with supervised machine learning for classification, centered on the theme of **AI Trust**. In this analysis, Custom dictionary is necessary for AI trust. There are four categories in this dictionary. There are `trust`, `distrust`, `ai_capability`, and `human_value`. After that The document-feature matrix (DFM) was filtered using this dictionary to calculate feature counts for each category. This binary variable served as the dependent variable for the subsequent supervised model. After that is the Supervised Classification. The **Naive Bayes** **classifier** was trained to predict the derived `contains_trust_factor` label using the complete set of comment features. The data was partitioned into **70% training** and **30% testing** sets. The model's performance was evaluated on the test set using a **Confusion Matrix**, providing key metrics such as Accuracy, Sensitivity, and Specificity, to assess its ability to correctly classify comments that express AI Trust. For comparative analysis, a traditional lexicon-based sentiment score was calculated using an external, general-purpose English sentiment lexicon. This allowed for the subsequent visualization and comparison of the general sentiment distribution (via box plots) between comments that were algorithmically identified as containing custom AI Trust words versus those that were not.

```{r Sentiment Analysis}



source("Sentiment Analysis.R")



```

## Sentiment Analysis Summary

The sentiment analysis successfully operationalized the concept of AI Trust within the comment data, demonstrating that specific thematic vocabulary and general emotional tone are significantly intertwined.

## Supervised Learning Analysis (Naive Bayes Classification)

The second major component of the text analysis was the application of **Supervised Machine Learning** using the **Naive Bayes (NB)** algorithm to classify comments based on the derived **AI Trust** factor. Unlike **Dictionary-Based Sentiment Analysis** (e.g., using an external lexicon), which assigns scores purely based on pre-defined positive/negative word lists, **Supervised Classification** learns patterns from the data itself

```{r supervised Learning}

source("Supervised_Learning.R")

```

## Topic Modeling (LDA)

In the Topic Modeling, the program sorted the words based on the frequency. There are eight topics for the topic modeling. The model was trained using the`text2vec` package. We set the number of topics (K) to 8 and ran the model for 500 iterations, using a for reproducibility.

```{r Topic Model}

source("Topic_Model.R")

```

## Causal Inference

The final stage of the project involved Causal Inference to quantify the relationship between the discovered comment themes and audience acceptance. We utilized **Ordinary Least Squares (OLS) Regression** to model the impact of each comment's thematic tendency on its popularity.

#### Model Design and Variables

-   **Dependent Variable (Outcome):** $\text{Like Count}$ but in the **log-transformed** ($\ln(\text{likeCount} + 1)$)

-   **Independent Variables (IVs):** The thematic compositions, $\text{V}_1$ through $\text{V}_7$, derived from the LDA **Document-Topic Distribution** matrix.

-   **Reference Topic:** The eighth topic, $\text{V}_8$ (**Sam Altman / Interview**)

-   **Control Variable:** **Reply Count**

The complete model is specified as:

$$\ln(\text{likeCount} + 1) \sim \beta_0 + \beta_1 V1 + ... + \beta_7 V7 + \beta_8 \text{reply} + \epsilon$$

```{r Casual Inference}

source("Causal_Inference.R")

```

## Conclusion

The **OLS Causal Inference** provided the central and most significant result: **all thematic categories (**$\text{V}_1$ to $\text{V}_7$) present highly significant lower audience acceptance (log-transformed like count) compared to the reference theme $\text{V}_8$ (Sam Altman / Interview). Specifically, the theme $\text{V}_4$ (News / Fake / Video) exhibited the most substantial negative coefficient. According to this result, it reject the null hypothesis and have strong evidence to the alternative hypothesis.

For **Topic Modeling,** The **88.31%** accuracy of the Supervised Naive Bayes classifier acts as another validation. It proves that the themes you categorized are not just statistically different in terms of "likes" but are also linguistically distinct.

## **Limitation**

The **OLS model** successfully presents the result for the alternative hypothesis. However, it still has some limitations for **sarcasm or fear-mongering** in the AI trust debates. Although the Naive Bayes classifier is highly accurate, it might ignore the complete semantic relationships that drive high engagement.

The tokenization part still contains **"noisy" tokens**, such as brand names and irrelevant objects. In this case, these tokens might affect the accuracy of the result.

## Future

In the future, a**dvanced analytical techniques** and semantic is the direction of improving the accuracy of the result. Also, **advance the stop words** to remove unrelated words to the AI or trust.

## Reference

mkulakowski2. (2023). *\[R Code for Text Analysis and Causal Inference\]* \[Computer software/Code\]. GitHub Gist. **https://gist.github.com/mkulakowski2/**

Krishnamurthy, V., & Duan, Y. (2017). Dependence Structure Analysis Of Meta-level Metrics in YouTube Videos: A Vine Copula Approach. arXiv preprint arXiv:1712.10232. “to explain the comment and the view of the video are related”

Pew Research Center. (2020). Many Americans get news on YouTube, where news organizations and independent producers thrive side by side. https://www.pewresearch.org/journalism/2020/09/28/many-americans-get-news-on-youtube-where-news-organizations-and-independent-producers-thrive-side-by-side/
